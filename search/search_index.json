{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Cloud-Native Learning Journey"},{"location":"developer-advanced-1/code-engine/","text":"title: Code Engine IBM Code Engine Introduction \u00b6 IBM Code Engine is a fully managed, serverless platform that runs the containerized workloads. These workloads includes web apps, microservices, event-driven functions, or batch jobs. Code Engine can also help to build the container images from the source code. Code Engine is designed to focus on the application to solve the business problem with no barrier on the infrastructure. Setting up the project in the Code Engine \u00b6 Project is the namespace under which the Code Engine entities such as applications, jobs, and builds are grouped together.Projects are used to manage resources and provide access to its entities. Follow the below steps to create a project in the Code Engine. As part of the IBM Cloud Catalog, provision the Code Engine From the Code Engine console project menu, select Create Project Select the Location, the project to be deployed Enter the name of the project. The name must be unique for all your Projects Choose the resource group under which the project to be deployed. Also specify the tags under which the project be identified The click on the Create Button to create the project After few minutes you will notice the project is created and listed as part of the Projects tab in the Code Engine Configuring and Deploying Applications in the Code Engine \u00b6 Access the Project that is created from the projects listed as part Projects tab. The navigation will land on the Overview section of the project. This will display the details of the project such as Summary Section, Resource Allocation and Recently updated details. Summary section will include details of number of applications deployed, number of jobs deployed, number of builds configured and submitted, number of registry access Current Resource Allocation section provides details as total instances, total cpu utilization and total memory utilization by all the applications Recently updated section provides information about the recently updated applications and jobs The section will get refreshed as the applications, jobs get deployed to the project Configuring the Registry Access \u00b6 As a first step to deploy the an application configure the Image Registry access. Registry access is important as it the Repo used by the Code Engine both for creating the Application Image from the Source Repository and Deploying the Application Image on the Containers. By default, Code engine register server is pointed to the Docker repo server. We can use the IBM Registry Server as the Image Registry as well using the APIKEYS. The below is the steps to configure the Registry access to the Code Engine. Navigate to the Registry access Section in the left nav. Click on the Create button on top of the registry access list table Now navigate to the create registry access page. There are two options for registry source. First is from Dockerhub and second is from Custom. Provide a Registry name. This should be unique in the project. Ex: ibm-registry-us Next provide the Registry server details, in case you have selected the Registry source as custom. In case if you want to use the IBM Registry Server, provide the ibm registry server details. For instance, if IBM Registry US is it can be \"us.icr.io/\" If Docker registry is used, the username and password details to be provide. In case of the ibm registry, it can be accessed through the APIKEYS. The apikey is provided as part of the password section, while username is automatically configured as \"iamapikey\" Provide the Email details as part of the E-mail Now click on the Add button to create the registry access Now the registry access is created and listed as part of the Registry access tab in the Code Engine Deploying Applications in the Code Engine \u00b6 Deploying on an application in the Code Engine can be done multiple choices. It can deployed using the 1. Container images from the Image Repository 1. Source from the public git repo 1. Source from the private/enterprise git repo. The first and second option stated above are simply straight forward, but the deployment from private or enterprise repo will need SSH Keys and Code Engine CLI for Secured Deployment. Deploying Application from the Container images \u00b6 Navigate to the Applications Section in the left nav. Click on the Create application button on top of the applications list table Now Navigate to the Create Application Page Provide Name to you application. This is a unique name within the project. An application name must consist of lower case alphanumeric characters, '-' and must start and end with an alphanumeric character select the 'choose the code to run' option either as Container image Now Select the configure Image option to deploy your application directly from the Image Registry Configure the image registry by providing the registry server, registry access, namespace, repository and tag for the image registry from the Application to Deployed. Ensure the registry has already configured as part of the Registry access discussed earlier. The above can be configured by clicking the button Specify image reference . Here select the Image Registry name already configured as part of the Registry access . Alternatively Registry Access can be added here as well by clicking the button Add registry Select the namespace where the application image repo exists. Select the repository name if already exist or type a new namespace if to be created one. But ensure the Application image is already available in the repository Select the image tag if a specified image to be deployed. Alternatively, the latest version of the image will be deployed. Click Done to specify the Image for the deployed. The same now will appear as part of the Image Reference If the Image reference needs to be changed, Click on the Edit image details link Now configure the Runtime settings, by expanding the Runtime settings Provide Memory configuration for the application as part of Memory (MiB) in the Instance section. By default 1024MB is provided Provide CPU cores for the application as part of CPU (vCPU) in the Instance section. By default 0.1vCPU core is provided Provide the timeout for request in seconds as part of Timeout (seconds) in the Requests section. By default 300 seconds is considered. Provide the maximum number of concurrent requests per instance as part of the Concurrency in the Requests section. By default 100 is considered. Provide the minimum and maximum number of instances for the purpose of the autoscaling in the Scaling section. By default minimum number of instance is considered as 0 and minimum number of instance as 10. Environment Variables can be configured in the Environment Variables section. Basically it is a name, value pair that is used in the application. Now click on the Create button on the right side to create the application After few minutes the application will be deployed and be listed as part of the Application List. Deploying Application from the Source Code (public git repository) \u00b6 Navigate to the Applications Section in the left nav. Click on the Create application button on top of the applications list table Now Navigate to the Create Application Page Provide Name to you application. This is a unique name within the project. An application name must consist of lower case alphanumeric characters, '-' and must start and end with an alphanumeric character Now provide the source code repository (public git repo) of the application. Ex: https://github.com/{user}/repo. Alternatively this can be provided on the Specify build details Now click the Specify build details . There will be a three step process to configure the source build configuration. The first step is to configure the Source Configuration. Provide the Source Repository of the Application Provide the Branch name of the application repository Provide the context directory (optional), if the source directory is different from the root directory. Then click on the Next button to provide the Strategy configuration The second step is to configure the Strategy Configuration. If Dockerfile is the strategy, provide the Dockerfile name to be used for the build. Default is 'Dockerfile' Configure the Timeout Setting, in case the build run fails. Default is 10mins Select one of the Build Resources to be used for the build execution Then click on the Next button to proceed for the output section The final step is to Configure the image registry Here select the Image Registry name already configured as part of the Registry access . Alternatively Registry Access can be added here as well by clicking the button Add registry Select the namespace where the application image repo exists. Select the repository name if already exist or type a new namespace if to be created one. But ensure the Application image is already available in the repository Select the image tag if a specified image to be deployed. Alternatively, the latest version of the image will be deployed. Click Done to specify the Image for the deployed. The same now will appear as part of the Image Reference If the Image reference needs to be changed, Click on the Edit image details link Now configure the Runtime settings, by expanding the Runtime settings Provide Memory configuration for the application as part of Memory (MiB) in the Instance section. By default 1024MB is provided Provide CPU cores for the application as part of CPU (vCPU) in the Instance section. By default 0.1vCPU core is provided Provide the timeout for request in seconds as part of Timeout (seconds) in the Requests section. By default 300 seconds is considered. Provide the maximum number of concurrent requests per instance as part of the Concurrency in the Requests section. By default 100 is considered. Provide the minimum and maximum number of instances for the purpose of the autoscaling in the Scaling section. By default minimum number of instance is considered as 0 and minimum number of instance as 10. Environment Variables can be configured in the Environment Variables section. Basically it is a name, value pair that is used in the application. Now click on the Create button on the right side to create the application After few minutes the application will be deployed and be listed as part of the Application List. Deploying Application from the Source Code (private/Enterprise git repository) \u00b6 Deploying Applications from a private git or enterprise git is not as straight forward as one from the public git repo's. This would require to set the CodeEngine CLI's to create git repo secret access. As a pre-requisite, the git account has to be associated with an SSH Key. An SSH key associated with a user, for example, your own user account or a functional ID that is available in your organization. This SSH key has the repository permissions from the user account. Code Engine requires read access to download the source code. For more information about setting up this type of SSH key refer https://docs.github.com/en/github/authenticating-to-github/adding-a-new-ssh-key-to-your-github-account. Note : Do not create your SSH key file with a secure passphrase as this action causes your build command to fail. Deploying Application from a Private/Enterprise git repo is a four step activities Creating a Git repository access secret with the CLI Assuming the SSH key is generated without any passphrase and this SSH key is configured as part of the git account Install the IBM Cloud Code Engine CLI using the command ibmcloud plugin install code-engine Now login to the ibmcloud account using the CLI and then set up the account, region, resource group where the Code Engine is deployed. To create a Git repository access secret with the CLI, use the command \"repo create\". This command requires a name, a key path, and the address of the Git repository host and also allows other optional arguments ibmcloud ce repo create --name maps to the name of the Git repository access secret. Use a name that is unique within the project. This value is required. --key-path maps to the local path to the unencrypted private SSH key. If you use your personal private SSH key, then this file is usually at $HOME /.ssh/id_rsa. This value is required. --host maps to the Git repository hostname ; for example, github.com. This value is required. --known-hosts-path maps to the path to your known hosts file. This file is usually located at $HOME /.ssh/known_hosts MacOs or Linux, the command looks similar as below ibmcloud ce repo create --name myrepo --key-path $HOME /.ssh/id_rsa --host github.com --known-hosts-path $HOME /.ssh/known_hosts Creating Git repository access secret ... OK once created, you can list the repo using the below cli command. The created secret access should be listed ibmcloud ce repo list Create images from the private repo The build configuration for the Code Engine can be created using code engine (ce) cli. The create the build image from the private repo using the secret access, the below cli to be used - ```bash ibmcloud ce bd create --name, -n value Required. The name of the build. --registry-secret, --rs value Required. The name of the image registry access secret. The image registry access secret is used to authenticate with a private registry when you create the container image. Run 'ibmcloud ce registry create' to create a registry secret. --source, --src value Required. The URL of the Git repository. --git-repo-secret, --grs, --repo, -r value The name of the Git repository access secret to access the private repository containing the source code to build your container -- sample reference as below ```bash ibmcloud ce bd create --name <<Your build name>> --image <<Image Repo Path>> --source <<Private Git Repository>> --registry-secret <<Registry access name created>> --git-repo-secret <<git repo secret created in previous step>> Submit Build images from the private Source repo Once the build is created, the next step is to submit build to create the application image from the source repo. This can be done both in User interface oo CLI. Let see, how this can be done from the User interface. Navigate / Access the project created in the Code Engine Navigate to the Image builds left nav, in the code engine project. The Build configuration created using the CLI in the previous step, will be listed as part of the Image builds table. Access the image builds that is created. Now you will be able to view the configuration as part of the Source , Strategy , Output tabs configured in the previous step. The build is configured from the previous step. Now Click on the Submit build button to build the application image from the repo. It will take few mins to complete the build and generate-push the application image to the image repository. The status and history of the build will be shown part of the build runs table. Once the build is completed, the status will be shown as \"succeeded\" Deploying the Application from the Image Registry Once the build submit is succeeded in the previous step, the application will be available in the Image registry configured earlier. Now the application has be deployed from the Image registry following Deploying Application from the Container images Navigate to the Applications Section in the left nav. Click on the Create application button on top of the applications list table Now Navigate to the Create Application Page Provide Name to you application. This is a unique name within the project. An application name must consist of lower case alphanumeric characters, '-' and must start and end with an alphanumeric character Now Select the Container Image option to deploy your application directly from the Image Registry Provide the image registry from the Application to Deployed. The format is \"registry/namespace/repository:tag\". The tag is optional, if not specified, the latest would be applied. ex: us.icr.io/appdev-cloud-native/ce-springboot. Ensure the registry has already configured as part of the Registry access discussed earlier. The above can be configured by clicking the button Specify image reference . Here select the Image Registry name already configured as part of the Registry access . Alternatively Registry Access can be added here as well by clicking the button Add registry Select the namespace where the application image repo exists. Select the repository name if already exist or type a new namespace if to be created one. But ensure the Application image is already available in the repository Select the image tag if a specified image to be deployed. Alternatively, the latest version of the image will be deployed. Click Done to specify the Image for the deployed. The same now will appear as part of the Image Reference If the Image reference needs to be changed, Click on the Edit image details link Now configure the Runtime settings, by expanding the Runtime settings Provide Memory configuration for the application as part of Memory (MiB) in the Instance section. By default 1024MB is provided Provide CPU cores for the application as part of CPU (vCPU) in the Instance section. By default 0.1vCPU core is provided Provide the timeout for request in seconds as part of Timeout (seconds) in the Requests section. By default 300 seconds is considered. Provide the maximum number of concurrent requests per instance as part of the Concurrency in the Requests section. By default 100 is considered. Provide the minimum and maximum number of instances for the purpose of the autoscaling in the Scaling section. By default minimum number of instance is considered as 0 and minimum number of instance as 10. Environment Variables can be configured in the Environment Variables section. Basically it is a name, value pair that is used in the application. Now click on the Create button on the right side to create the application After few minutes the application will be deployed and be listed as part of the Application List. Thus by following above four step activities, the application source from the private or enterprise repo can be deployed in the code engine. Accessing Code Engine Applications \u00b6 Now that the application is build and deployed, the below steps to be followed to navigate the application Navigate to the Application List Page of your project All the deployed application will be listed as in rows, in the table list identify the application and click on the application link under \"URL link\" or popout link. Now the application would be opened in a new tab of the browser","title":"Code Engine"},{"location":"developer-advanced-1/code-engine/#introduction","text":"IBM Code Engine is a fully managed, serverless platform that runs the containerized workloads. These workloads includes web apps, microservices, event-driven functions, or batch jobs. Code Engine can also help to build the container images from the source code. Code Engine is designed to focus on the application to solve the business problem with no barrier on the infrastructure.","title":"Introduction"},{"location":"developer-advanced-1/code-engine/#setting-up-the-project-in-the-code-engine","text":"Project is the namespace under which the Code Engine entities such as applications, jobs, and builds are grouped together.Projects are used to manage resources and provide access to its entities. Follow the below steps to create a project in the Code Engine. As part of the IBM Cloud Catalog, provision the Code Engine From the Code Engine console project menu, select Create Project Select the Location, the project to be deployed Enter the name of the project. The name must be unique for all your Projects Choose the resource group under which the project to be deployed. Also specify the tags under which the project be identified The click on the Create Button to create the project After few minutes you will notice the project is created and listed as part of the Projects tab in the Code Engine","title":"Setting up the project in the Code Engine"},{"location":"developer-advanced-1/code-engine/#configuring-and-deploying-applications-in-the-code-engine","text":"Access the Project that is created from the projects listed as part Projects tab. The navigation will land on the Overview section of the project. This will display the details of the project such as Summary Section, Resource Allocation and Recently updated details. Summary section will include details of number of applications deployed, number of jobs deployed, number of builds configured and submitted, number of registry access Current Resource Allocation section provides details as total instances, total cpu utilization and total memory utilization by all the applications Recently updated section provides information about the recently updated applications and jobs The section will get refreshed as the applications, jobs get deployed to the project","title":"Configuring and Deploying Applications in the Code Engine"},{"location":"developer-advanced-1/code-engine/#configuring-the-registry-access","text":"As a first step to deploy the an application configure the Image Registry access. Registry access is important as it the Repo used by the Code Engine both for creating the Application Image from the Source Repository and Deploying the Application Image on the Containers. By default, Code engine register server is pointed to the Docker repo server. We can use the IBM Registry Server as the Image Registry as well using the APIKEYS. The below is the steps to configure the Registry access to the Code Engine. Navigate to the Registry access Section in the left nav. Click on the Create button on top of the registry access list table Now navigate to the create registry access page. There are two options for registry source. First is from Dockerhub and second is from Custom. Provide a Registry name. This should be unique in the project. Ex: ibm-registry-us Next provide the Registry server details, in case you have selected the Registry source as custom. In case if you want to use the IBM Registry Server, provide the ibm registry server details. For instance, if IBM Registry US is it can be \"us.icr.io/\" If Docker registry is used, the username and password details to be provide. In case of the ibm registry, it can be accessed through the APIKEYS. The apikey is provided as part of the password section, while username is automatically configured as \"iamapikey\" Provide the Email details as part of the E-mail Now click on the Add button to create the registry access Now the registry access is created and listed as part of the Registry access tab in the Code Engine","title":"Configuring the Registry Access"},{"location":"developer-advanced-1/code-engine/#deploying-applications-in-the-code-engine","text":"Deploying on an application in the Code Engine can be done multiple choices. It can deployed using the 1. Container images from the Image Repository 1. Source from the public git repo 1. Source from the private/enterprise git repo. The first and second option stated above are simply straight forward, but the deployment from private or enterprise repo will need SSH Keys and Code Engine CLI for Secured Deployment.","title":"Deploying Applications in the Code Engine"},{"location":"developer-advanced-1/code-engine/#accessing-code-engine-applications","text":"Now that the application is build and deployed, the below steps to be followed to navigate the application Navigate to the Application List Page of your project All the deployed application will be listed as in rows, in the table list identify the application and click on the application link under \"URL link\" or popout link. Now the application would be opened in a new tab of the browser","title":"Accessing Code Engine Applications"},{"location":"developer-advanced-1/database-with-cloudant/","text":"Add a Cloudant integration to your backend service \u00b6 While logged into the IBM Cloud account use the resource list to find your pre installed Cloudant database instance name after your development cluster. Open the database instance dashboard. Click on the Service Credentials on the left-hand menu. You will see the credentials for the database which will be something like this: Open a terminal window folder/directory called data mkdir data To help create test JSON data we are going to supply a template to the JSON Generator tool, this helps when creating dummy data for testing. Navigate to the following link https://www.json-generator.com/ Replace the default template with the following template (using cut and paste). This will enable a 100 records of test data to be created to represent a products database. Click on the Generate button. { docs: [ '{{repeat(100)}}' , { _id: '{{objectId()}}' , manufacturer: '{{company().toUpperCase()}}' , name: '{{lorem(3, \"words\")}}' , price: '{{floating(10, 1000, 2, \"0.00\")}}' , stock: '{{integer(1, 100)}}' } ] } Copy the generated contents on the right hand side into a file called inventory.json and save it into the same folder. Download the dataload.sh script from the Iteration Zero repository using the following command: curl https://raw.githubusercontent.com/ibm-garage-cloud/ibm-garage-iteration-zero/master/terraform/scripts/dataload.sh >> dataload.sh Add the username and apikey to CLOUDANT_USERNAME and CLOUDANT_API_KEY variables in the dataload.sh script. You can get the credentials from the Cloudant credentials view in the IBM Cloud console. Add DATABASE value to be inventory-<replace with namespace> using the dev namespace/project name you have been using. Save the script, make it executable, and then run it by passing in the filename Before running this script,make sure you have jq installed. chmod +x ./dataload.sh ./dataload.sh inventory.json The data from the inventory.json file will then be used to populate the database, to confirm this on the Dashboard click on Manage menu on the left and then Launch button to see the Cloudant dashboard. Click on the Left icon that looks like a Database and you will see the inventory-<namespace> database created. Click on the inventory database, then click Table view. You can see the rows of data If you click on a row of data, you will see the raw NoSQL form of the data record. This completes the setup of the database and populating it with data. Enable database in the solution \u00b6 If you are starting from the solution, use the following steps to enable the Cloudant database Set up local development \u00b6 Open the mappings.json file under src/main/resources and add a DATABASE_NAME value with the value inventory-{namespace} where namespace is the namespace where the pipeline is running (e.g. dev-{initials}) json title=\"src/main/resources/mappings.json\" { \"DATABASE_NAME\": \"inventory-{namespace}\" } Log into cloud.ibm.com and open the Cloudant service from the resource list Click on service credentials Tab. In service credentials, click on \"New credential\" button. Select the role \"Manager\" Navigate to your namespace where you are running the inventory solution pipeline and create the binding secret for the Cloudant instance on the cloud account oc project [ proj_name ] ibmcloud oc cluster service bind --cluster workshop-team-one --namespace [ YOUR-NAMESPACE ] --service workshop-cldn-one The application needs a user with \"manager\" iam_role whereas the binding secret creates a user with \"writer\" iam_role resulting in \"access denied\" issues. Hence, the \"binding\" key in binding secret you just created needs to be replaced with the values of \"manager\" role user you created in Service Credentials on cloud.ibm.com. Copy the json contents from the credentials into mappings.json under the CLOUDANT_CONFIG object (note that CLOUDANT_CONFIG value must be a string type not a json type, so you must use escaping characters for this value). You could a JSON to String converting tool. json title=\"src/main/resources/mappings.json\" { \"DATABASE_NAME\": \"inventory-{namespace}\", \"CLOUDANT_CONFIG\": \"{paste json here}\" } Activate the Cloudant service implementation \u00b6 Create a new file src/main/java/com/ibm/inventory_management/services/StockItemMockService.java . Copy the contents of src/main/java/com/ibm/inventory_management/services/StockItemService.java in this newly created StockItemMockService.java file. Do the following changes after copying the contents of StockItemService.java to this new file: Remove the @Primary annotation. Change the classname from StockItemService to StockItemMockService Open src/main/java/com/ibm/inventory_management/services/StockItemApi.java and add the following code to the file. The file should look like the following ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems() throws Exception; void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception; void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception; void deleteStockItem(String id) throws Exception; } ``` Update the StockItemService implementation \u00b6 ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import java.io.IOException; import java.util.List; import javax.annotation.PostConstruct; import java.net.URL; import java.net.MalformedURLException; import java.util.UUID; import com.cloudant.client.api.CloudantClient; import com.cloudant.client.api.ClientBuilder; import com.cloudant.client.api.Database; import org.springframework.context.annotation.Primary; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Lazy; import org.springframework.stereotype.Service; import com.ibm.inventory_management.config.CloudantConfig; import com.ibm.inventory_management.models.StockItem; @Service @Primary public class StockItemService implements StockItemApi { @Bean public static CloudantClient buildCloudant(CloudantConfig config) throws CloudServicesException { System.out.println(\"Config: \" + config); URL url = null; try { url = new URL(config.getUrl()); } catch (MalformedURLException e) { throw new CloudServicesException(\"Invalid service URL specified\", e); } return ClientBuilder .url(url) .iamApiKey(config.getApiKey()) // .username(config.getUsername()) // .iamApiKey(config.getPassword()) .build(); } private CloudantConfig config; private CloudantClient client; private Database db = null; public StockItemService(CloudantConfig config, @Lazy CloudantClient client) { this.config = config; this.client = client; } @PostConstruct public void init() { db = client.database(config.getDatabaseName(), true); } @Override public List<StockItem> listStockItems() throws Exception { try { return db.getAllDocsRequestBuilder() .includeDocs(true) .build() .getResponse() .getDocsAs(StockItem.class); } catch (IOException e) { throw new Exception(\"\", e); } } @Override public void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception { try { db.save(new StockItem(UUID.randomUUID().toString()) .withName(name) .withPrice(price) .withStock(stock) .withManufacturer(manufacturer) ); } catch (Exception e) { throw new Exception(\"\",e); } } @Override public void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception { try { StockItem itemToUpdate = db.find(StockItem.class,id); itemToUpdate.setName(name !=null ? name : itemToUpdate.getName()); itemToUpdate.setManufacturer(manufacturer != null ? manufacturer : itemToUpdate.getManufacturer()); itemToUpdate.setPrice(price != null ? price : itemToUpdate.getPrice()); itemToUpdate.setStock(stock != null ? stock : itemToUpdate.getStock()); db.update(itemToUpdate); } catch (Exception e ){ throw new Exception(\"\", e); } } @Override public void deleteStockItem(String id) throws Exception { try { db.remove(db.find(StockItem.class,id)); } catch (Exception e){ throw new Exception(\"\",e); } } } ``` Update the StockItem model \u00b6 ```java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import java.io.Serializable; public class StockItem implements Serializable { private String name; private String _id = null; private String _rev = null; private int stock = 0; private double price = 0.0; private String manufacturer = \"\"; public StockItem() { super(); } public StockItem(String id) { this._id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } public String getId() { return _id; } public void setId(String id) { this._id = id; } public StockItem withId(String id) { this.setId(id); return this; } public int getStock() { return stock; } public void setStock(int stock) { this.stock = stock; } public StockItem withStock(int stock) { this.setStock(stock); return this; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } public StockItem withPrice(double price) { this.setPrice(price); return this; } public String getManufacturer() { return manufacturer; } public void setManufacturer(String manufacturer) { this.manufacturer = manufacturer; } public StockItem withManufacturer(String manufacturer) { this.setManufacturer(manufacturer); return this; } } ``` Update the StockItem controller \u00b6 ```java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.List; import org.springframework.web.bind.annotation.*; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @RestController public class StockItemController { private final StockItemApi service; public StockItemController(StockItemApi service) { this.service = service; } @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List<StockItem> listStockItems() throws Exception{ return this.service.listStockItems(); } @PostMapping(path = \"/stock-item\") public void addStockItem(@RequestParam String name, @RequestParam String manufacturer, @RequestParam Double price, @RequestParam Integer stock) throws Exception{ this.service.addStockItem(name,price,stock,manufacturer); } @PutMapping(path = \"/stock-item/{id}\") public void updateStockItem(@PathVariable(\"id\") String id, @RequestParam(required=false) String name, @RequestParam(required=false) String manufacturer, @RequestParam(required=false) Double price, @RequestParam(required=false) Integer stock) throws Exception{ this.service.updateStockItem(id,name, price, stock, manufacturer); } @DeleteMapping(path = \"/stock-item/{id}\") public void deleteStockItem(@PathVariable(\"id\") String id) throws Exception{ this.service.deleteStockItem(id); } } ``` Update the configuration values in the values.yaml helm chart \u00b6 Open the values.yaml file and add the values cloudantBinding and databaseName yaml title=\"chart/base/values.yaml\" cloudantBinding=\"{binding name}\" databaseName=\"inventory-{namespace}\" Note The cloudantBinding value should match the name of the cloudant binding secret Add a Cloudant integration to your backend service \u00b6 If you are following the instructions from MicroApp part 1 and want to enable the Cloudant database yourself, use the following directions. Update the gradle config to include cloudant dependencies \u00b6 Add build-cloudant.gradle to the gradle folder dependencies { compile group: 'com.cloudant' , name: 'cloudant-client' , version: '2.17.0' compile group: 'com.jayway.jsonpath' , name: 'json-path' , version: '2.4.0' compile group: 'javax.xml.bind' , name: 'jaxb-api' , version: '2.1' compile group: 'joda-time' , name: 'joda-time' , version: '2.10.3' } Enable the cloudant libraries by applying the build-cloudant.gradle to the end of the build.gradle file groovy title=\"build.gradle\" apply from: 'gradle/build-cloudant.gradle' Run ./gradlew init to validate the changes and load the libraries Configuration values \u00b6 Add CloudantConfig to hold the url , username , password , apikey and databaseName values: java title=\"src/main/java/com/ibm/inventory_management/config/CloudantConfig.java\" package com.ibm.inventory_management.config; import com.fasterxml.jackson.annotation.JsonIgnoreProperties; @JsonIgnoreProperties(ignoreUnknown = true) public class CloudantConfig { private String url; private String username; private String password; private String databaseName; private String apikey; public String getUrl() { return url; } public String getApikey() { return apikey; } public void setApikey(String url) { this.apikey = apikey; } public void setUrl(String url) { this.url = url; } public CloudantConfig withUrl(String url) { this.setUrl(url); return this; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public CloudantConfig withUsername(String username) { this.setUsername(username); return this; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public CloudantConfig withPassword(String password) { this.setPassword(password); return this; } public String getDatabaseName() { return databaseName; } public void setDatabaseName(String databaseName) { this.databaseName = databaseName; } public CloudantConfig withDatabaseName(String databaseName) { this.setDatabaseName(databaseName); return this; } public String toString() { return \"[CloudantConfig: url=\" + this.url + \", username=\" + this.username + \", name=\" + this.databaseName + \"]\"; } } Implement logic to load the configuration from the secret binding or local file Add CloudantMapping and CloudantConfigFactory files to the config directory: ```java title=\"src/main/java/com/ibm/inventory_management/config/CloudantMapping.java\" package com.ibm.inventory_management.config; import java.io.Serializable; import com.fasterxml.jackson.annotation.JsonProperty; public class CloudantMapping implements Serializable { @JsonProperty(value = \"CLOUDANT_CONFIG\") private String cloudantConfig; @JsonProperty(value = \"DATABASE_NAME\") private String databaseName; public String getCloudantConfig() { return cloudantConfig; } public void setCloudantConfig(String cloudantConfig) { this.cloudantConfig = cloudantConfig; } public String getDatabaseName() { return databaseName; } public void setDatabaseName(String databaseName) { this.databaseName = databaseName; } } ```java title=\"src/main/java/com/ibm/inventory_management/config/CloudantConfigFactory.java\" package com.ibm.inventory_management.config; import java.io.IOException; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.context.annotation.Bean; import org.springframework.stereotype.Component; @Component public class CloudantConfigFactory { @Bean public CloudantConfig buildCloudantConfig() throws IOException { return buildConfigFromBinding( loadCloudantConfig(), loadDatabaseName() ); } protected String loadCloudantConfig() throws IOException { return System.getProperty(\"CLOUDANT_CONFIG\") != null ? System.getProperty(\"CLOUDANT_CONFIG\") : loadCloudantMappingFromLocalDev().getCloudantConfig(); } protected CloudantMapping loadCloudantMappingFromLocalDev() throws IOException { final ObjectMapper mapper = new ObjectMapper(); return mapper.readValue( this.getClass().getClassLoader().getResourceAsStream(\"mappings.json\"), CloudantMapping.class ); } protected String loadDatabaseName() throws IOException { return System.getProperty(\"DATABASE_NAME\") != null ? System.getProperty(\"DATABASE_NAME\") : loadCloudantMappingFromLocalDev().getDatabaseName(); } protected CloudantConfig buildConfigFromBinding(String binding, String databaseName) throws IOException { final ObjectMapper mapper = new ObjectMapper(); if (binding == null) { return new CloudantConfig(); } final CloudantConfig baseConfig = mapper.readValue(binding, CloudantConfig.class); if (baseConfig == null) { return new CloudantConfig(); } return baseConfig.withDatabaseName(databaseName); } } Service Implementation \u00b6 Add a CloudantApi component to create the CloudantClient instance from the configuration. java title=\"src/main/java/com/ibm/inventory_management/services/CloudServicesException.java\" package com.ibm.inventory_management.services; public class CloudServicesException extends Exception { public CloudServicesException() { } public CloudServicesException(String message) { super(message); } public CloudServicesException(String message, Throwable cause) { super(message, cause); } public CloudServicesException(Throwable cause) { super(cause); } public CloudServicesException( String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace ) { super(message, cause, enableSuppression, writableStackTrace); } } ```java title=\"src/main/java/com/ibm/inventory_management/services/CloudantApi.java\" package com.ibm.inventory_management.services; import java.net.MalformedURLException; import java.net.URL; import com.cloudant.client.api.ClientBuilder; import com.cloudant.client.api.CloudantClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Profile; import org.springframework.stereotype.Component; import com.ibm.inventory_management.config.CloudantConfig; @Component @Profile(\"!mock\") public class CloudantApi { @Bean public CloudantClient buildCloudant(CloudantConfig config) throws CloudServicesException { System.out.println(\"Config: \" + config); URL url = null; try { url = new URL(config.getUrl()); } catch (MalformedURLException e) { throw new CloudServicesException(\"Invalid service URL specified\", e); } return ClientBuilder .url(url) .iamApiKey(config.getApikey()) //.username(config.getUsername()) //.password(config.getPassword()) .build(); } } ``` Open the deployment.yaml file and add environment variables that use those values to the top of the existing env block yaml title=\"chart/template-java-spring/templates/deployment.yaml\" env: - name: CLOUDANT_CONFIG valueFrom: secretKeyRef: name: {{ .Values.cloudantBinding | quote }} key: binding - name: DATABASE_NAME value: {{ .Values.databaseName | quote }} Running the application locally \u00b6 Start the application. ./gradlew build ./gradlew bootrun Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html Once the application is up, go the Swagger UI and execute get stock-items . You should be able to see all the data you pushed to the database through ./dataload.sh Prebuilt solution for this can be found here: Inventory Management Service Cloudant solution template Note You will need to setup your own Cloudant database, create your mappings.json file and update your values.yaml Helm file as mentioned before","title":"Database with Cloudant"},{"location":"developer-advanced-1/database-with-cloudant/#add-a-cloudant-integration-to-your-backend-service","text":"While logged into the IBM Cloud account use the resource list to find your pre installed Cloudant database instance name after your development cluster. Open the database instance dashboard. Click on the Service Credentials on the left-hand menu. You will see the credentials for the database which will be something like this: Open a terminal window folder/directory called data mkdir data To help create test JSON data we are going to supply a template to the JSON Generator tool, this helps when creating dummy data for testing. Navigate to the following link https://www.json-generator.com/ Replace the default template with the following template (using cut and paste). This will enable a 100 records of test data to be created to represent a products database. Click on the Generate button. { docs: [ '{{repeat(100)}}' , { _id: '{{objectId()}}' , manufacturer: '{{company().toUpperCase()}}' , name: '{{lorem(3, \"words\")}}' , price: '{{floating(10, 1000, 2, \"0.00\")}}' , stock: '{{integer(1, 100)}}' } ] } Copy the generated contents on the right hand side into a file called inventory.json and save it into the same folder. Download the dataload.sh script from the Iteration Zero repository using the following command: curl https://raw.githubusercontent.com/ibm-garage-cloud/ibm-garage-iteration-zero/master/terraform/scripts/dataload.sh >> dataload.sh Add the username and apikey to CLOUDANT_USERNAME and CLOUDANT_API_KEY variables in the dataload.sh script. You can get the credentials from the Cloudant credentials view in the IBM Cloud console. Add DATABASE value to be inventory-<replace with namespace> using the dev namespace/project name you have been using. Save the script, make it executable, and then run it by passing in the filename Before running this script,make sure you have jq installed. chmod +x ./dataload.sh ./dataload.sh inventory.json The data from the inventory.json file will then be used to populate the database, to confirm this on the Dashboard click on Manage menu on the left and then Launch button to see the Cloudant dashboard. Click on the Left icon that looks like a Database and you will see the inventory-<namespace> database created. Click on the inventory database, then click Table view. You can see the rows of data If you click on a row of data, you will see the raw NoSQL form of the data record. This completes the setup of the database and populating it with data.","title":"Add a Cloudant integration to your backend service"},{"location":"developer-advanced-1/database-with-cloudant/#enable-database-in-the-solution","text":"If you are starting from the solution, use the following steps to enable the Cloudant database","title":"Enable database in the solution"},{"location":"developer-advanced-1/database-with-cloudant/#set-up-local-development","text":"Open the mappings.json file under src/main/resources and add a DATABASE_NAME value with the value inventory-{namespace} where namespace is the namespace where the pipeline is running (e.g. dev-{initials}) json title=\"src/main/resources/mappings.json\" { \"DATABASE_NAME\": \"inventory-{namespace}\" } Log into cloud.ibm.com and open the Cloudant service from the resource list Click on service credentials Tab. In service credentials, click on \"New credential\" button. Select the role \"Manager\" Navigate to your namespace where you are running the inventory solution pipeline and create the binding secret for the Cloudant instance on the cloud account oc project [ proj_name ] ibmcloud oc cluster service bind --cluster workshop-team-one --namespace [ YOUR-NAMESPACE ] --service workshop-cldn-one The application needs a user with \"manager\" iam_role whereas the binding secret creates a user with \"writer\" iam_role resulting in \"access denied\" issues. Hence, the \"binding\" key in binding secret you just created needs to be replaced with the values of \"manager\" role user you created in Service Credentials on cloud.ibm.com. Copy the json contents from the credentials into mappings.json under the CLOUDANT_CONFIG object (note that CLOUDANT_CONFIG value must be a string type not a json type, so you must use escaping characters for this value). You could a JSON to String converting tool. json title=\"src/main/resources/mappings.json\" { \"DATABASE_NAME\": \"inventory-{namespace}\", \"CLOUDANT_CONFIG\": \"{paste json here}\" }","title":"Set up local development"},{"location":"developer-advanced-1/database-with-cloudant/#activate-the-cloudant-service-implementation","text":"Create a new file src/main/java/com/ibm/inventory_management/services/StockItemMockService.java . Copy the contents of src/main/java/com/ibm/inventory_management/services/StockItemService.java in this newly created StockItemMockService.java file. Do the following changes after copying the contents of StockItemService.java to this new file: Remove the @Primary annotation. Change the classname from StockItemService to StockItemMockService Open src/main/java/com/ibm/inventory_management/services/StockItemApi.java and add the following code to the file. The file should look like the following ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems() throws Exception; void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception; void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception; void deleteStockItem(String id) throws Exception; } ```","title":"Activate the Cloudant service implementation"},{"location":"developer-advanced-1/database-with-cloudant/#update-the-stockitemservice-implementation","text":"```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import java.io.IOException; import java.util.List; import javax.annotation.PostConstruct; import java.net.URL; import java.net.MalformedURLException; import java.util.UUID; import com.cloudant.client.api.CloudantClient; import com.cloudant.client.api.ClientBuilder; import com.cloudant.client.api.Database; import org.springframework.context.annotation.Primary; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Lazy; import org.springframework.stereotype.Service; import com.ibm.inventory_management.config.CloudantConfig; import com.ibm.inventory_management.models.StockItem; @Service @Primary public class StockItemService implements StockItemApi { @Bean public static CloudantClient buildCloudant(CloudantConfig config) throws CloudServicesException { System.out.println(\"Config: \" + config); URL url = null; try { url = new URL(config.getUrl()); } catch (MalformedURLException e) { throw new CloudServicesException(\"Invalid service URL specified\", e); } return ClientBuilder .url(url) .iamApiKey(config.getApiKey()) // .username(config.getUsername()) // .iamApiKey(config.getPassword()) .build(); } private CloudantConfig config; private CloudantClient client; private Database db = null; public StockItemService(CloudantConfig config, @Lazy CloudantClient client) { this.config = config; this.client = client; } @PostConstruct public void init() { db = client.database(config.getDatabaseName(), true); } @Override public List<StockItem> listStockItems() throws Exception { try { return db.getAllDocsRequestBuilder() .includeDocs(true) .build() .getResponse() .getDocsAs(StockItem.class); } catch (IOException e) { throw new Exception(\"\", e); } } @Override public void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception { try { db.save(new StockItem(UUID.randomUUID().toString()) .withName(name) .withPrice(price) .withStock(stock) .withManufacturer(manufacturer) ); } catch (Exception e) { throw new Exception(\"\",e); } } @Override public void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception { try { StockItem itemToUpdate = db.find(StockItem.class,id); itemToUpdate.setName(name !=null ? name : itemToUpdate.getName()); itemToUpdate.setManufacturer(manufacturer != null ? manufacturer : itemToUpdate.getManufacturer()); itemToUpdate.setPrice(price != null ? price : itemToUpdate.getPrice()); itemToUpdate.setStock(stock != null ? stock : itemToUpdate.getStock()); db.update(itemToUpdate); } catch (Exception e ){ throw new Exception(\"\", e); } } @Override public void deleteStockItem(String id) throws Exception { try { db.remove(db.find(StockItem.class,id)); } catch (Exception e){ throw new Exception(\"\",e); } } } ```","title":"Update the StockItemService implementation"},{"location":"developer-advanced-1/database-with-cloudant/#update-the-stockitem-model","text":"```java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import java.io.Serializable; public class StockItem implements Serializable { private String name; private String _id = null; private String _rev = null; private int stock = 0; private double price = 0.0; private String manufacturer = \"\"; public StockItem() { super(); } public StockItem(String id) { this._id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } public String getId() { return _id; } public void setId(String id) { this._id = id; } public StockItem withId(String id) { this.setId(id); return this; } public int getStock() { return stock; } public void setStock(int stock) { this.stock = stock; } public StockItem withStock(int stock) { this.setStock(stock); return this; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } public StockItem withPrice(double price) { this.setPrice(price); return this; } public String getManufacturer() { return manufacturer; } public void setManufacturer(String manufacturer) { this.manufacturer = manufacturer; } public StockItem withManufacturer(String manufacturer) { this.setManufacturer(manufacturer); return this; } } ```","title":"Update the StockItem model"},{"location":"developer-advanced-1/database-with-cloudant/#update-the-stockitem-controller","text":"```java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.List; import org.springframework.web.bind.annotation.*; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @RestController public class StockItemController { private final StockItemApi service; public StockItemController(StockItemApi service) { this.service = service; } @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List<StockItem> listStockItems() throws Exception{ return this.service.listStockItems(); } @PostMapping(path = \"/stock-item\") public void addStockItem(@RequestParam String name, @RequestParam String manufacturer, @RequestParam Double price, @RequestParam Integer stock) throws Exception{ this.service.addStockItem(name,price,stock,manufacturer); } @PutMapping(path = \"/stock-item/{id}\") public void updateStockItem(@PathVariable(\"id\") String id, @RequestParam(required=false) String name, @RequestParam(required=false) String manufacturer, @RequestParam(required=false) Double price, @RequestParam(required=false) Integer stock) throws Exception{ this.service.updateStockItem(id,name, price, stock, manufacturer); } @DeleteMapping(path = \"/stock-item/{id}\") public void deleteStockItem(@PathVariable(\"id\") String id) throws Exception{ this.service.deleteStockItem(id); } } ```","title":"Update the StockItem controller"},{"location":"developer-advanced-1/database-with-cloudant/#update-the-configuration-values-in-the-valuesyaml-helm-chart","text":"Open the values.yaml file and add the values cloudantBinding and databaseName yaml title=\"chart/base/values.yaml\" cloudantBinding=\"{binding name}\" databaseName=\"inventory-{namespace}\" Note The cloudantBinding value should match the name of the cloudant binding secret","title":"Update the configuration values in the values.yaml helm chart"},{"location":"developer-advanced-1/database-with-cloudant/#add-a-cloudant-integration-to-your-backend-service_1","text":"If you are following the instructions from MicroApp part 1 and want to enable the Cloudant database yourself, use the following directions.","title":"Add a Cloudant integration to your backend service"},{"location":"developer-advanced-1/database-with-cloudant/#update-the-gradle-config-to-include-cloudant-dependencies","text":"Add build-cloudant.gradle to the gradle folder dependencies { compile group: 'com.cloudant' , name: 'cloudant-client' , version: '2.17.0' compile group: 'com.jayway.jsonpath' , name: 'json-path' , version: '2.4.0' compile group: 'javax.xml.bind' , name: 'jaxb-api' , version: '2.1' compile group: 'joda-time' , name: 'joda-time' , version: '2.10.3' } Enable the cloudant libraries by applying the build-cloudant.gradle to the end of the build.gradle file groovy title=\"build.gradle\" apply from: 'gradle/build-cloudant.gradle' Run ./gradlew init to validate the changes and load the libraries","title":"Update the gradle config to include cloudant dependencies"},{"location":"developer-advanced-1/database-with-cloudant/#configuration-values","text":"Add CloudantConfig to hold the url , username , password , apikey and databaseName values: java title=\"src/main/java/com/ibm/inventory_management/config/CloudantConfig.java\" package com.ibm.inventory_management.config; import com.fasterxml.jackson.annotation.JsonIgnoreProperties; @JsonIgnoreProperties(ignoreUnknown = true) public class CloudantConfig { private String url; private String username; private String password; private String databaseName; private String apikey; public String getUrl() { return url; } public String getApikey() { return apikey; } public void setApikey(String url) { this.apikey = apikey; } public void setUrl(String url) { this.url = url; } public CloudantConfig withUrl(String url) { this.setUrl(url); return this; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public CloudantConfig withUsername(String username) { this.setUsername(username); return this; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public CloudantConfig withPassword(String password) { this.setPassword(password); return this; } public String getDatabaseName() { return databaseName; } public void setDatabaseName(String databaseName) { this.databaseName = databaseName; } public CloudantConfig withDatabaseName(String databaseName) { this.setDatabaseName(databaseName); return this; } public String toString() { return \"[CloudantConfig: url=\" + this.url + \", username=\" + this.username + \", name=\" + this.databaseName + \"]\"; } } Implement logic to load the configuration from the secret binding or local file Add CloudantMapping and CloudantConfigFactory files to the config directory: ```java title=\"src/main/java/com/ibm/inventory_management/config/CloudantMapping.java\" package com.ibm.inventory_management.config; import java.io.Serializable; import com.fasterxml.jackson.annotation.JsonProperty; public class CloudantMapping implements Serializable { @JsonProperty(value = \"CLOUDANT_CONFIG\") private String cloudantConfig; @JsonProperty(value = \"DATABASE_NAME\") private String databaseName; public String getCloudantConfig() { return cloudantConfig; } public void setCloudantConfig(String cloudantConfig) { this.cloudantConfig = cloudantConfig; } public String getDatabaseName() { return databaseName; } public void setDatabaseName(String databaseName) { this.databaseName = databaseName; } } ```java title=\"src/main/java/com/ibm/inventory_management/config/CloudantConfigFactory.java\" package com.ibm.inventory_management.config; import java.io.IOException; import com.fasterxml.jackson.databind.ObjectMapper; import org.springframework.context.annotation.Bean; import org.springframework.stereotype.Component; @Component public class CloudantConfigFactory { @Bean public CloudantConfig buildCloudantConfig() throws IOException { return buildConfigFromBinding( loadCloudantConfig(), loadDatabaseName() ); } protected String loadCloudantConfig() throws IOException { return System.getProperty(\"CLOUDANT_CONFIG\") != null ? System.getProperty(\"CLOUDANT_CONFIG\") : loadCloudantMappingFromLocalDev().getCloudantConfig(); } protected CloudantMapping loadCloudantMappingFromLocalDev() throws IOException { final ObjectMapper mapper = new ObjectMapper(); return mapper.readValue( this.getClass().getClassLoader().getResourceAsStream(\"mappings.json\"), CloudantMapping.class ); } protected String loadDatabaseName() throws IOException { return System.getProperty(\"DATABASE_NAME\") != null ? System.getProperty(\"DATABASE_NAME\") : loadCloudantMappingFromLocalDev().getDatabaseName(); } protected CloudantConfig buildConfigFromBinding(String binding, String databaseName) throws IOException { final ObjectMapper mapper = new ObjectMapper(); if (binding == null) { return new CloudantConfig(); } final CloudantConfig baseConfig = mapper.readValue(binding, CloudantConfig.class); if (baseConfig == null) { return new CloudantConfig(); } return baseConfig.withDatabaseName(databaseName); } }","title":"Configuration values"},{"location":"developer-advanced-1/database-with-cloudant/#service-implementation","text":"Add a CloudantApi component to create the CloudantClient instance from the configuration. java title=\"src/main/java/com/ibm/inventory_management/services/CloudServicesException.java\" package com.ibm.inventory_management.services; public class CloudServicesException extends Exception { public CloudServicesException() { } public CloudServicesException(String message) { super(message); } public CloudServicesException(String message, Throwable cause) { super(message, cause); } public CloudServicesException(Throwable cause) { super(cause); } public CloudServicesException( String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace ) { super(message, cause, enableSuppression, writableStackTrace); } } ```java title=\"src/main/java/com/ibm/inventory_management/services/CloudantApi.java\" package com.ibm.inventory_management.services; import java.net.MalformedURLException; import java.net.URL; import com.cloudant.client.api.ClientBuilder; import com.cloudant.client.api.CloudantClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Profile; import org.springframework.stereotype.Component; import com.ibm.inventory_management.config.CloudantConfig; @Component @Profile(\"!mock\") public class CloudantApi { @Bean public CloudantClient buildCloudant(CloudantConfig config) throws CloudServicesException { System.out.println(\"Config: \" + config); URL url = null; try { url = new URL(config.getUrl()); } catch (MalformedURLException e) { throw new CloudServicesException(\"Invalid service URL specified\", e); } return ClientBuilder .url(url) .iamApiKey(config.getApikey()) //.username(config.getUsername()) //.password(config.getPassword()) .build(); } } ``` Open the deployment.yaml file and add environment variables that use those values to the top of the existing env block yaml title=\"chart/template-java-spring/templates/deployment.yaml\" env: - name: CLOUDANT_CONFIG valueFrom: secretKeyRef: name: {{ .Values.cloudantBinding | quote }} key: binding - name: DATABASE_NAME value: {{ .Values.databaseName | quote }}","title":"Service Implementation"},{"location":"developer-advanced-1/database-with-cloudant/#running-the-application-locally","text":"Start the application. ./gradlew build ./gradlew bootrun Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html Once the application is up, go the Swagger UI and execute get stock-items . You should be able to see all the data you pushed to the database through ./dataload.sh Prebuilt solution for this can be found here: Inventory Management Service Cloudant solution template Note You will need to setup your own Cloudant database, create your mappings.json file and update your values.yaml Helm file as mentioned before","title":"Running the application locally"},{"location":"developer-advanced-1/database-with-mongodb/","text":"Add a MongoDB integration to your backend service \u00b6 Create a MongoDB instance \u00b6 On OpenShift Install your self managed MongoDB instance using Helm Charts . Login to your OpenShift cluster on a terminal and choose your development project. Create a mongodb.values.yaml with the following content ```yaml title=\"mongodb.values.yaml\" podSecurityContext: enabled: false containerSecurityContext: enabled: false auth: usernames: myuser passwords: mypassword databases: inventory-db You can change the username, password and database name to your liking. - Run the following commands, it will install a Helm chart on your project, make sure to have `helm` installed locally. ```bash $ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm install mongodb bitnami/mongodb -f mongodb.values.yaml Head over to your OpenShift console on the Topology perspective, you should the database created on your project. The Helm chart creates a secret holding MongoDB passwords, for consistency of this tutorial we will be creating another secret holding the same information, formatted in another way. Run the following command : $ oc create secret generic --from-literal = MONGODB_HOST = mongodb --from-literal = MONGODB_PORT = 27017 --from-literal = MONGODB_USER = myuser --from-literal = MONGODB_PASSWORD = mypassword --from-literal = MONGODB_DATABASE = inventory-db mongodb-access You should see the created secret mongodb-access on your OpenShift console : On IBM Cloud Warning The IBM Cloud service enforces SSL certificates usage to access the instance, however this is not covered in this tutorial, you can find the documentation for it here . Provision a MongoDB instance using the Databases for MongoDB service on IBM Cloud. Log into the IBM Cloud console and look for the Databases for MongoDB service. Configure it to your needs and create the instance. After your instance has provisioned, create a Service Credential to have access to your MongoDB instance. The service credential should contain information to login to the instance. We will look for the database, the hostname, the username, the password and a base64 encoded certificate. Decode the certificate, and create an OpenShift secret holding these values : oc create secret generic --from-literal = MONGODB_HOST = <HOST> --from-literal = MONGODB_PORT = <PORT> --from-literal = MONGODB_USER = <USER> --from-literal = MONGODB_DATABASE = <DATABASE> --from-literal = MONGODB_PASSWORD = <PASSWORD> --from-literal = MONGODB_CERT = <DECODED_CERT> mongodb-access You can also do this from the OpenShift console : BYO MongoDB Create your own MongoDB instance on any other cloud platform and bring the login credentials. Login to your OpenShift cluster and create a secret using the following command: $ oc create secret generic --from-literal = MONGODB_HOST = <HOST> --from-literal = MONGODB_PORT = <PORT> --from-literal = MONGODB_USER = <USER> --from-literal = MONGODB_PASSWORD = <PASSWORD> --from-literal = MONGODB_DATABASE = <DATABASE> mongodb-access Fill out the placeholders <HOST> , <USER> , <PASSWORD> and <DATABASE> with your own MongoDB instance's credentials. Fill in the database with mock data \u00b6 To help create test JSON data we are going to supply a template to the JSON Generator tool, this helps when creating dummy data for testing. Navigate to the following link https://www.json-generator.com/ . Replace the default template with following template. This will enable a 100 records of test data to be created to represent a products database. Click on the Generate button. [ '{{repeat(100)}}' , { _id: '{{objectId()}}' , manufacturer: '{{company().toUpperCase()}}' , name: '{{lorem(3, \"words\")}}' , price: '{{floating(10, 1000, 2, \"0.00\")}}' , stock: '{{integer(1, 100)}}' } ] Connect to your MongoDB instance, create a new collection called stockItem in your database and insert the generated data. Enable database in the solution \u00b6 If you are starting from the solution, use the following steps to enable the Cloudant database Update the gradle config to include mongodb dependencies Head over to the gradle and create the following build-mongodb.gradle file: ```groovy title=\"gradle/build-mongodb.gradle\" dependencies { implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-mongodb', version: '2.7.1' implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa', version: '2.7.1' } - On the `build.gradle` file, add the following line : ```groovy title=\"build.gradle\" apply from: 'gradle/build-mongodb.gradle' Run ./gradlew build --refresh-dependencies to validate the changes and load the libraries. Update application configuration to connect to database On the src/main/resources/application.yml file, update the spring by adding the content as such. yaml title=\"src/main/resources/application.yml\" spring: data: mongodb: username: \"${MONGODB_USERNAME:<USERNAME>}\" password: \"${MONGODB_PASSWORD:<PASSWORD>}\" database: \"${MONGODB_DATABASE:<DATABASE>}\" authentication-database: \"${MONGODB_DATABASE:<DATABASE>}\" port: \"${MONGODB_PORT:<PORT>}\" host: \"${MONGODB_HOST:<HOST>}\" autoconfigure: exclude: org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration Replace the placeholders with your database instance values. Update the configuration values in the Helm chart Open the values.yaml file and add the following value : ```yaml title=\"chart/base/values.yaml\" mongodbAccess=mongodb-access - Open the `deployment.yaml` file and add the following environment variables : ```yaml title=\"chart/base/templates/deployment.yaml\" env: - name: MONGODB_USERNAME valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_USERNAME - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_PASSWORD - name: MONGODB_DATABASE valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_DATABASE - name: MONGODB_PORT valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_PORT - name: MONGODB_HOST valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_HOST Update project files Update the StockItem.java file, add an @Id annotation on the id field. ```java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import org.springframework.data.annotation.Id; import java.io.Serializable; public class StockItem implements Serializable { private String name; @Id private String id = null; private int stock = 0; private double price = 0.0; private String manufacturer = \"\"; public StockItem() { super(); } public StockItem(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } public String getId() { return id; } public void setId(String id) { this.id = id; } public StockItem withId(String id) { this.setId(id); return this; } public int getStock() { return stock; } public void setStock(int stock) { this.stock = stock; } public StockItem withStock(int stock) { this.setStock(stock); return this; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } public StockItem withPrice(double price) { this.setPrice(price); return this; } public String getManufacturer() { return manufacturer; } public void setManufacturer(String manufacturer) { this.manufacturer = manufacturer; } public StockItem withManufacturer(String manufacturer) { this.setManufacturer(manufacturer); return this; } } - Create a `StockItemRepository.java` interface to interact with the database, put it in a `repositories` directory. ```java title=\"src/main/java/com/ibm/inventory_management/repositories/StockItemRepository.java\" package com.ibm.inventory_management.repositories; import com.ibm.inventory_management.models.StockItem; import org.springframework.data.mongodb.repository.MongoRepository; import org.springframework.stereotype.Repository; import java.util.Optional; @Repository public interface StockItemRepository extends MongoRepository<StockItem, String> { Optional<StockItem> findById(String id); } - Update the StockItemApi.java interface to have CRUD operations. ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems() throws Exception; void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception; void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception; void deleteStockItem(String id) throws Exception; } - Update the `StockItemService.java` to implement the interface and use the repository. ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.repositories.StockItemRepository; import org.bson.types.ObjectId; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import com.ibm.inventory_management.models.StockItem; @Service public class StockItemService implements StockItemApi { @Autowired private StockItemRepository stockItemRepository; @Override public List<StockItem> listStockItems() { return stockItemRepository.findAll(); } @Override public void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception { try { stockItemRepository.save( new StockItem(ObjectId.get().toString()) .withName(name) .withManufacturer(manufacturer) .withStock(stock) .withPrice(price) ); } catch (Exception e) { throw new Exception(\"\",e); } } @Override public void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception { try { StockItem itemToUpdate = stockItemRepository.findById(id).get(); itemToUpdate.setName(name !=null ? name : itemToUpdate.getName()); itemToUpdate.setManufacturer(manufacturer != null ? manufacturer : itemToUpdate.getManufacturer()); itemToUpdate.setPrice(price != null ? price : itemToUpdate.getPrice()); itemToUpdate.setStock(stock != null ? stock : itemToUpdate.getStock()); stockItemRepository.save(itemToUpdate); } catch (Exception e) { throw new Exception(\"\",e); } } @Override public void deleteStockItem(String id) throws Exception { try { stockItemRepository.deleteById(id); } catch (Exception e){ throw new Exception(\"\",e); } } } Running the application locally \u00b6 Start the application locally ./gradlew build ./gradlew bootrun Tip If you are using MongoDB on OpenShift , you can use the oc port-forward command to access your database instance through localhost Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html Once the application is up, go the Swagger UI and execute get stock-items . You should be able to see all the data you pushed to the database through ./dataload.sh Prebuilt solution for this can be found here: Inventory Management Service MongoDB solution template Note You will need to setup your own database credentials in the application.yml file.","title":"Database with MongoDB"},{"location":"developer-advanced-1/database-with-mongodb/#add-a-mongodb-integration-to-your-backend-service","text":"","title":"Add a MongoDB integration to your backend service"},{"location":"developer-advanced-1/database-with-mongodb/#create-a-mongodb-instance","text":"On OpenShift Install your self managed MongoDB instance using Helm Charts . Login to your OpenShift cluster on a terminal and choose your development project. Create a mongodb.values.yaml with the following content ```yaml title=\"mongodb.values.yaml\" podSecurityContext: enabled: false containerSecurityContext: enabled: false auth: usernames: myuser passwords: mypassword databases: inventory-db You can change the username, password and database name to your liking. - Run the following commands, it will install a Helm chart on your project, make sure to have `helm` installed locally. ```bash $ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm install mongodb bitnami/mongodb -f mongodb.values.yaml Head over to your OpenShift console on the Topology perspective, you should the database created on your project. The Helm chart creates a secret holding MongoDB passwords, for consistency of this tutorial we will be creating another secret holding the same information, formatted in another way. Run the following command : $ oc create secret generic --from-literal = MONGODB_HOST = mongodb --from-literal = MONGODB_PORT = 27017 --from-literal = MONGODB_USER = myuser --from-literal = MONGODB_PASSWORD = mypassword --from-literal = MONGODB_DATABASE = inventory-db mongodb-access You should see the created secret mongodb-access on your OpenShift console : On IBM Cloud Warning The IBM Cloud service enforces SSL certificates usage to access the instance, however this is not covered in this tutorial, you can find the documentation for it here . Provision a MongoDB instance using the Databases for MongoDB service on IBM Cloud. Log into the IBM Cloud console and look for the Databases for MongoDB service. Configure it to your needs and create the instance. After your instance has provisioned, create a Service Credential to have access to your MongoDB instance. The service credential should contain information to login to the instance. We will look for the database, the hostname, the username, the password and a base64 encoded certificate. Decode the certificate, and create an OpenShift secret holding these values : oc create secret generic --from-literal = MONGODB_HOST = <HOST> --from-literal = MONGODB_PORT = <PORT> --from-literal = MONGODB_USER = <USER> --from-literal = MONGODB_DATABASE = <DATABASE> --from-literal = MONGODB_PASSWORD = <PASSWORD> --from-literal = MONGODB_CERT = <DECODED_CERT> mongodb-access You can also do this from the OpenShift console : BYO MongoDB Create your own MongoDB instance on any other cloud platform and bring the login credentials. Login to your OpenShift cluster and create a secret using the following command: $ oc create secret generic --from-literal = MONGODB_HOST = <HOST> --from-literal = MONGODB_PORT = <PORT> --from-literal = MONGODB_USER = <USER> --from-literal = MONGODB_PASSWORD = <PASSWORD> --from-literal = MONGODB_DATABASE = <DATABASE> mongodb-access Fill out the placeholders <HOST> , <USER> , <PASSWORD> and <DATABASE> with your own MongoDB instance's credentials.","title":"Create a MongoDB instance"},{"location":"developer-advanced-1/database-with-mongodb/#fill-in-the-database-with-mock-data","text":"To help create test JSON data we are going to supply a template to the JSON Generator tool, this helps when creating dummy data for testing. Navigate to the following link https://www.json-generator.com/ . Replace the default template with following template. This will enable a 100 records of test data to be created to represent a products database. Click on the Generate button. [ '{{repeat(100)}}' , { _id: '{{objectId()}}' , manufacturer: '{{company().toUpperCase()}}' , name: '{{lorem(3, \"words\")}}' , price: '{{floating(10, 1000, 2, \"0.00\")}}' , stock: '{{integer(1, 100)}}' } ] Connect to your MongoDB instance, create a new collection called stockItem in your database and insert the generated data.","title":"Fill in the database with mock data"},{"location":"developer-advanced-1/database-with-mongodb/#enable-database-in-the-solution","text":"If you are starting from the solution, use the following steps to enable the Cloudant database Update the gradle config to include mongodb dependencies Head over to the gradle and create the following build-mongodb.gradle file: ```groovy title=\"gradle/build-mongodb.gradle\" dependencies { implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-mongodb', version: '2.7.1' implementation group: 'org.springframework.boot', name: 'spring-boot-starter-data-jpa', version: '2.7.1' } - On the `build.gradle` file, add the following line : ```groovy title=\"build.gradle\" apply from: 'gradle/build-mongodb.gradle' Run ./gradlew build --refresh-dependencies to validate the changes and load the libraries. Update application configuration to connect to database On the src/main/resources/application.yml file, update the spring by adding the content as such. yaml title=\"src/main/resources/application.yml\" spring: data: mongodb: username: \"${MONGODB_USERNAME:<USERNAME>}\" password: \"${MONGODB_PASSWORD:<PASSWORD>}\" database: \"${MONGODB_DATABASE:<DATABASE>}\" authentication-database: \"${MONGODB_DATABASE:<DATABASE>}\" port: \"${MONGODB_PORT:<PORT>}\" host: \"${MONGODB_HOST:<HOST>}\" autoconfigure: exclude: org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration Replace the placeholders with your database instance values. Update the configuration values in the Helm chart Open the values.yaml file and add the following value : ```yaml title=\"chart/base/values.yaml\" mongodbAccess=mongodb-access - Open the `deployment.yaml` file and add the following environment variables : ```yaml title=\"chart/base/templates/deployment.yaml\" env: - name: MONGODB_USERNAME valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_USERNAME - name: MONGODB_PASSWORD valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_PASSWORD - name: MONGODB_DATABASE valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_DATABASE - name: MONGODB_PORT valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_PORT - name: MONGODB_HOST valueFrom: secretKeyRef: name: {{ .Values.mongodbAccess | quote }} key: MONGODB_HOST Update project files Update the StockItem.java file, add an @Id annotation on the id field. ```java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import org.springframework.data.annotation.Id; import java.io.Serializable; public class StockItem implements Serializable { private String name; @Id private String id = null; private int stock = 0; private double price = 0.0; private String manufacturer = \"\"; public StockItem() { super(); } public StockItem(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } public String getId() { return id; } public void setId(String id) { this.id = id; } public StockItem withId(String id) { this.setId(id); return this; } public int getStock() { return stock; } public void setStock(int stock) { this.stock = stock; } public StockItem withStock(int stock) { this.setStock(stock); return this; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } public StockItem withPrice(double price) { this.setPrice(price); return this; } public String getManufacturer() { return manufacturer; } public void setManufacturer(String manufacturer) { this.manufacturer = manufacturer; } public StockItem withManufacturer(String manufacturer) { this.setManufacturer(manufacturer); return this; } } - Create a `StockItemRepository.java` interface to interact with the database, put it in a `repositories` directory. ```java title=\"src/main/java/com/ibm/inventory_management/repositories/StockItemRepository.java\" package com.ibm.inventory_management.repositories; import com.ibm.inventory_management.models.StockItem; import org.springframework.data.mongodb.repository.MongoRepository; import org.springframework.stereotype.Repository; import java.util.Optional; @Repository public interface StockItemRepository extends MongoRepository<StockItem, String> { Optional<StockItem> findById(String id); } - Update the StockItemApi.java interface to have CRUD operations. ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems() throws Exception; void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception; void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception; void deleteStockItem(String id) throws Exception; } - Update the `StockItemService.java` to implement the interface and use the repository. ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.repositories.StockItemRepository; import org.bson.types.ObjectId; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import com.ibm.inventory_management.models.StockItem; @Service public class StockItemService implements StockItemApi { @Autowired private StockItemRepository stockItemRepository; @Override public List<StockItem> listStockItems() { return stockItemRepository.findAll(); } @Override public void addStockItem(String name, Double price, Integer stock, String manufacturer) throws Exception { try { stockItemRepository.save( new StockItem(ObjectId.get().toString()) .withName(name) .withManufacturer(manufacturer) .withStock(stock) .withPrice(price) ); } catch (Exception e) { throw new Exception(\"\",e); } } @Override public void updateStockItem(String id, String name, Double price, Integer stock, String manufacturer) throws Exception { try { StockItem itemToUpdate = stockItemRepository.findById(id).get(); itemToUpdate.setName(name !=null ? name : itemToUpdate.getName()); itemToUpdate.setManufacturer(manufacturer != null ? manufacturer : itemToUpdate.getManufacturer()); itemToUpdate.setPrice(price != null ? price : itemToUpdate.getPrice()); itemToUpdate.setStock(stock != null ? stock : itemToUpdate.getStock()); stockItemRepository.save(itemToUpdate); } catch (Exception e) { throw new Exception(\"\",e); } } @Override public void deleteStockItem(String id) throws Exception { try { stockItemRepository.deleteById(id); } catch (Exception e){ throw new Exception(\"\",e); } } }","title":"Enable database in the solution"},{"location":"developer-advanced-1/database-with-mongodb/#running-the-application-locally","text":"Start the application locally ./gradlew build ./gradlew bootrun Tip If you are using MongoDB on OpenShift , you can use the oc port-forward command to access your database instance through localhost Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html Once the application is up, go the Swagger UI and execute get stock-items . You should be able to see all the data you pushed to the database through ./dataload.sh Prebuilt solution for this can be found here: Inventory Management Service MongoDB solution template Note You will need to setup your own database credentials in the application.yml file.","title":"Running the application locally"},{"location":"developer-advanced-1/inventory-appid/","text":"Securing Inventory App with App ID To secure the application we are using the capabilities available within the IBM Cloud platform to enable integration with AppID. With Openshift 3.11, a simple annotation was used on the ingress to enable Appid. In Openshift 4.x, Red Hat OpenShift on IBM Cloud annotations (ingress.bluemix.net/[annotation]) and NGINX annotations (nginx.ingress.kubernetes.io/[annotation_name]) are not supported for the router or the Ingress resource. With Openshift 4.x, AppID integration is enabled with SDKs. Prerequisites \u00b6 The following prerequisites are required for AppID integration: An instance of the App ID service: In IBM Cloud Dashboard, go to \"Services\" and select the AppID instance. A set of service credentials: In AppID instance, go to \"Application\". Click on \"Add application\". Enter your application name Select the type as Regular web application. Click on Save to create the service credentials for your application. After saving, your application credentials will get created.Click on the down Arrow at the left end of your application name and get the credentials. yarn version 1.22.19 or higher. node version 16.16.0 or higher (it is recommended that you use an LTS version). Note To install yarn run the command npm install -g yarn Enable Appid in the solution \u00b6 Installation \u00b6 By using the command line, change to the directory that contains your Node.js app. Install the AppID service and other dependencies. yarn add ibmcloud-appid passport@0.5.2 express-session Obtain your credentials by navigating to the Applications tab of the AppID dashboard as mentioned in Prerequisites . Obtain the application as mentioned in AppID Redirect Url Config . Create binding secret for appid in your namespace \u00b6 Login to the IBM Cloud cluster where your workshop-team-one ocp cluster resides. ibmcloud login -u [ username ] -p [ password ] Login to your ocp cluster using the oc cli. Click on \"Copy Login Command\". It will redirect to a new page. Click the display token link. Copy the \"Log in with this token\" command & login to oc cli, it should look like this : oc login --token = sha256~bfGcq7l6H3JHd9GwbNRaSsJ7cDAiLK5EPF4tbPQ-WfY --server = https://c108-e.eu-gb.containers.cloud.ibm.com:31718 Navigate to your namespace where you are running the inventory solution pipeline and create the binding secret for He the Appid instance on the cloud account oc project <PROJECT_NAME> ibmcloud oc cluster service bind --cluster workshop-team-one --namespace <PROJECT_NAME> --service workshop-team-one-appid Note Save the binding name, you will use it later Update the configuration values in the configuration files \u00b6 Put these credentials in server/config/mappings.json to be referred by application: json title=\"server/config/mappings.json\" { \"APPID_CONFIG\": \"\"{\\\"tenantId\\\":\\\"<tenantId_value>\\\",\\\"oauthServerUrl\\\":\\\"<oauthServer_URL>\\\",\\\"clientId\\\": \\\"<ClientID_value>\\\", \\\"secret\\\": \\\"<secret_value>\\\"}\", \"application_url\":\"<openshift_appln_route_url>\" } Add the following parameter in values.yaml along with its value: yaml title=\"chart/base/values.yaml\" appidBinding: <BINDING_NAME> Adding the dependencies \u00b6 Add the following require definitions to your server/server.js : bash title=\"server/server.js\" const express = require('express'); const session = require('express-session') const passport = require('passport'); const WebAppStrategy = require(\"ibmcloud-appid\").WebAppStrategy; const CALLBACK_URL = \"/ibm/cloud/appid/callback\"; const appidConfig = require(\"./config/mappings.json\"); Activate the appid integration \u00b6 In server.js , set up your express app to use express-session middleware. ```javascript title=\"server/server.js\" const app = express(); const appidcfg = appidConfig.APPID_CONFIG; app.use( session({ secret: appidcfg.secret, resave: true, saveUninitialized: true }) ); app.use(passport.initialize()); app.use(passport.session()); ``` In the same file, initialize the SDK using the information obtained in the previous steps. ```javascript title=\"server/server.js\" passport.use( new WebAppStrategy({ tenantId: appidcfg.tenantId, clientId: appidcfg.clientId, secret: appidcfg.secret, oauthServerUrl: appidcfg.oAuthServerUrl, redirectUri: appidConfig.application_url + CALLBACK_URL }) ); ``` In the same file, configure passport with serialization and deserialization. This configuration step is required for authenticated session persistence across HTTP requests. For more information, see the passport docs javascript title=\"server/server.js\" passport.serializeUser(function(user, cb) { cb(null, user); }); passport.deserializeUser(function(obj, cb) { cb(null, obj); }); Add the following code to your server.js to issue the service redirects. javascript title=\"server/server.js\" app.get(CALLBACK_URL, passport.authenticate(WebAppStrategy.STRATEGY_NAME)); app.use(passport.authenticate(WebAppStrategy.STRATEGY_NAME )); Adding environment variables to deployment.yaml \u00b6 Open the deployment.yaml file and add environment variables that use those values to the top of the existing env block: ```yaml title=\"chart/base/templates/deployment.yaml\" name: APPID_CONFIG valueFrom: secretKeyRef: name: {{ .Values.appidBinding | quote }} key: binding ``` AppID redirect url config \u00b6 Get the ingress for the UI component by running igc ingress -n dev-{initials} . Open the IBM Cloud resource list - https://cloud.ibm.com/resources Open the AppID instance to the Manage Authentication -> Authentication Settings view Add the redirect url for the application to the web redirect URLs. The redirect url will have the following form: {ingress url}/ibm/cloud/appid/callback e.g. https://inventory-manangement-ui-dev.sms-test-oc-cluster.us-east.containers.appdomain.cloud/ibm/cloud/appid/callback Add users to AppID \u00b6 Open the AppID instance to Cloud Directory -> Users Add yourself as a user with an email address, name, and password# Commit and push the changes \u00b6 Commit your local changes and push them to your remote repository git add . git commit -m \"Added appid\" git push Your previously defined pipeline should be launched and the new app should be deployed afterwards Access the UI \u00b6 Open a browser to the UI Application URL You should be met with the AppID login screen. (This screen can be customized from the AppID service console but for now we are showing the default screen.) Provide the email address and password you configured in the previous steps. You should be granted access to the UI.","title":"Authentication with AppID"},{"location":"developer-advanced-1/inventory-appid/#prerequisites","text":"The following prerequisites are required for AppID integration: An instance of the App ID service: In IBM Cloud Dashboard, go to \"Services\" and select the AppID instance. A set of service credentials: In AppID instance, go to \"Application\". Click on \"Add application\". Enter your application name Select the type as Regular web application. Click on Save to create the service credentials for your application. After saving, your application credentials will get created.Click on the down Arrow at the left end of your application name and get the credentials. yarn version 1.22.19 or higher. node version 16.16.0 or higher (it is recommended that you use an LTS version). Note To install yarn run the command npm install -g yarn","title":"Prerequisites"},{"location":"developer-advanced-1/inventory-appid/#enable-appid-in-the-solution","text":"","title":"Enable Appid in the solution"},{"location":"developer-advanced-1/inventory-appid/#installation","text":"By using the command line, change to the directory that contains your Node.js app. Install the AppID service and other dependencies. yarn add ibmcloud-appid passport@0.5.2 express-session Obtain your credentials by navigating to the Applications tab of the AppID dashboard as mentioned in Prerequisites . Obtain the application as mentioned in AppID Redirect Url Config .","title":"Installation"},{"location":"developer-advanced-1/inventory-appid/#create-binding-secret-for-appid-in-your-namespace","text":"Login to the IBM Cloud cluster where your workshop-team-one ocp cluster resides. ibmcloud login -u [ username ] -p [ password ] Login to your ocp cluster using the oc cli. Click on \"Copy Login Command\". It will redirect to a new page. Click the display token link. Copy the \"Log in with this token\" command & login to oc cli, it should look like this : oc login --token = sha256~bfGcq7l6H3JHd9GwbNRaSsJ7cDAiLK5EPF4tbPQ-WfY --server = https://c108-e.eu-gb.containers.cloud.ibm.com:31718 Navigate to your namespace where you are running the inventory solution pipeline and create the binding secret for He the Appid instance on the cloud account oc project <PROJECT_NAME> ibmcloud oc cluster service bind --cluster workshop-team-one --namespace <PROJECT_NAME> --service workshop-team-one-appid Note Save the binding name, you will use it later","title":"Create binding secret for appid in your namespace"},{"location":"developer-advanced-1/inventory-appid/#update-the-configuration-values-in-the-configuration-files","text":"Put these credentials in server/config/mappings.json to be referred by application: json title=\"server/config/mappings.json\" { \"APPID_CONFIG\": \"\"{\\\"tenantId\\\":\\\"<tenantId_value>\\\",\\\"oauthServerUrl\\\":\\\"<oauthServer_URL>\\\",\\\"clientId\\\": \\\"<ClientID_value>\\\", \\\"secret\\\": \\\"<secret_value>\\\"}\", \"application_url\":\"<openshift_appln_route_url>\" } Add the following parameter in values.yaml along with its value: yaml title=\"chart/base/values.yaml\" appidBinding: <BINDING_NAME>","title":"Update the configuration values in the configuration files"},{"location":"developer-advanced-1/inventory-appid/#adding-the-dependencies","text":"Add the following require definitions to your server/server.js : bash title=\"server/server.js\" const express = require('express'); const session = require('express-session') const passport = require('passport'); const WebAppStrategy = require(\"ibmcloud-appid\").WebAppStrategy; const CALLBACK_URL = \"/ibm/cloud/appid/callback\"; const appidConfig = require(\"./config/mappings.json\");","title":"Adding the dependencies"},{"location":"developer-advanced-1/inventory-appid/#activate-the-appid-integration","text":"In server.js , set up your express app to use express-session middleware. ```javascript title=\"server/server.js\" const app = express(); const appidcfg = appidConfig.APPID_CONFIG; app.use( session({ secret: appidcfg.secret, resave: true, saveUninitialized: true }) ); app.use(passport.initialize()); app.use(passport.session()); ``` In the same file, initialize the SDK using the information obtained in the previous steps. ```javascript title=\"server/server.js\" passport.use( new WebAppStrategy({ tenantId: appidcfg.tenantId, clientId: appidcfg.clientId, secret: appidcfg.secret, oauthServerUrl: appidcfg.oAuthServerUrl, redirectUri: appidConfig.application_url + CALLBACK_URL }) ); ``` In the same file, configure passport with serialization and deserialization. This configuration step is required for authenticated session persistence across HTTP requests. For more information, see the passport docs javascript title=\"server/server.js\" passport.serializeUser(function(user, cb) { cb(null, user); }); passport.deserializeUser(function(obj, cb) { cb(null, obj); }); Add the following code to your server.js to issue the service redirects. javascript title=\"server/server.js\" app.get(CALLBACK_URL, passport.authenticate(WebAppStrategy.STRATEGY_NAME)); app.use(passport.authenticate(WebAppStrategy.STRATEGY_NAME ));","title":"Activate the appid integration"},{"location":"developer-advanced-1/inventory-appid/#adding-environment-variables-to-deploymentyaml","text":"Open the deployment.yaml file and add environment variables that use those values to the top of the existing env block: ```yaml title=\"chart/base/templates/deployment.yaml\" name: APPID_CONFIG valueFrom: secretKeyRef: name: {{ .Values.appidBinding | quote }} key: binding ```","title":"Adding environment variables to deployment.yaml"},{"location":"developer-advanced-1/inventory-appid/#appid-redirect-url-config","text":"Get the ingress for the UI component by running igc ingress -n dev-{initials} . Open the IBM Cloud resource list - https://cloud.ibm.com/resources Open the AppID instance to the Manage Authentication -> Authentication Settings view Add the redirect url for the application to the web redirect URLs. The redirect url will have the following form: {ingress url}/ibm/cloud/appid/callback e.g. https://inventory-manangement-ui-dev.sms-test-oc-cluster.us-east.containers.appdomain.cloud/ibm/cloud/appid/callback","title":"AppID redirect url config"},{"location":"developer-advanced-1/inventory-appid/#add-users-to-appid","text":"Open the AppID instance to Cloud Directory -> Users Add yourself as a user with an email address, name, and password#","title":"Add users to AppID"},{"location":"developer-advanced-1/inventory-appid/#commit-and-push-the-changes","text":"Commit your local changes and push them to your remote repository git add . git commit -m \"Added appid\" git push Your previously defined pipeline should be launched and the new app should be deployed afterwards","title":"Commit and push the changes"},{"location":"developer-advanced-1/inventory-appid/#access-the-ui","text":"Open a browser to the UI Application URL You should be met with the AppID login screen. (This screen can be customized from the AppID service console but for now we are showing the default screen.) Provide the email address and password you configured in the previous steps. You should be granted access to the UI.","title":"Access the UI"},{"location":"developer-advanced-1/inventory-cd/","text":"Extending the Inventory Micro app to include Continuous Delivery to Test. Guide \u00b6 This Micro App guidance continues to build upon the micro-services that were built in the Inventory Micro App guide. Make sure you have complete Inventory Application or deployed the working Inventory Solution . We implemented the three tiers in the Inventory Mico App and deployed the app to the dev namespace/project. We will take that app and make these additions. Deploy the app to the test namespace/project using CD techniques and ArgoCD Using CD to deploy to Test \u00b6 ArgoCD is a tool that provides continuous delivery for projects and applications. If you haven't already, be sure to read through the Continuous Delivery with ArgoCD guide . For this exercise, we are going to use ArgoCD to push the Inventory app from dev to test (and possibly staging as well). If you have already completed the Inventory Micro App , then it can be used for the ArgoCD process (although perhaps with some minor pipeline updates). If you haven't completed the exercise, you can start from the solution repositories to perform the ArgoCD steps. Set up the GitOps repo \u00b6 Let's get started with using Argo CD. Create a new repo from the ArgoCD Code Pattern Clone the project to your machine Create a branch named test git checkout -b test Push the branch to the remote git push -u origin test Create the test namespace with the CLI by running oc sync test-{initials} --dev Register the GitOps repo in ArgoCD \u00b6 Now that the repository has been created, we need to tell ArgoCD where it is. Get the ArgoCD login information from the oc credentials cli command Note You need to be logged into the cluster on the command-line for the CLI to access the cluster information. Log into ArgoCD (use oc credentials to obtain your credentials and login to argo) Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. Create a project in ArgoCD (Optional) \u00b6 In ArgoCD terms, each deployable component is an Application and applications are grouped into Projects . Projects are not required for ArgoCD to be able to deploy applications but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. To create a project, do the following: Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Projects option Click the New Project button at the top of the page. Provide the following values then press Create : name - the name for the project (provide `inventory-management) description - a brief description of the project sources - click add source and pick the Git repository from the list that was added previously destinations Add https://kubernetes.default.svc for the cluster url and test-{initials} for the namespace Add https://kubernetes.default.svc for the cluster url and staging-{initials} for the namespace Note: Initially, the only cluster that is available is the one in which ArgoCD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the test-{initials} and staging-{initials} namespaces within the current cluster. Configure the GitOps repo for Inventory Management service \u00b6 Clone the GitOps repository you created earlier, copy the folder templates/app-helm to the root of the repository and give it a name that matches the Inventory Management service component (e.g. inventory-management-svc-{initials} ) Update inventory-management-svc-{initials}/Chart.yaml and update the name to match the directory name Update inventory-management-svc-{initials}/requirements.yaml with the following values: name - the name of helm chart/image. This should match the folder name version - the version number of the helm chart repository - the url to the helm repository including the folder where helm charts are being stored. here is an example dependencies : - name : inventory-management-svc-mjp version : 1.0.0-1 repository : http://artifactory.mooc-one-rhos-cluster.us-east.containers.appdomain.cloud/artifactory/generic-local/mooc-team-one/ The url of the Artifactory helm repository can be taken from the below step. In the Artifactory Setup screen, in Set Me Up Section, select the tool as \"Generic\" and repository as \"generic-local\". Copy the deploy URL from the Set Me Up dialog box. That is the Artifactory helm repository URL. Run kubectl get configmap/ibmcloud-config -n tools -o yaml to print the configuration information for the cluster In inventory-management-svc-{initials}/values.yaml replace <app-chart-name> with the directory name. Replace ingressSubdomain with the value from the previous step. Update tlsSecretName with the value from the previous step. The result should look something like the following ```yaml title=\"inventory-management-svc-{initials}/values.yaml\" global: ingressSubdomain: sms-test.us-south.containers.appdomain.cloud tlsSecretName: sms-test-cluster inventory-management-svc-{initials}: replicaCount: 1 ingress: enabled: true tlsSecretName: sms-test-cluster ``` Commit and push the changes git add . git commit -m \"Adds inventory-management-svc config\" git push Add an application in ArgoCD for the Inventory Management service \u00b6 The last step in the process is to define the application(s) within ArgoCD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Log into ArgoCD Click New Application and provide the following values: application name - test-inventory-management-svc project - inventory-management sync-policy - Automatic repository url - The url of the GitOps repository you created earlier revision - test path - inventory-management-svc-{initials} destination cluster - The cluster url for the deployment destination namespace - test-{initials} values file - values.yaml Click Create Click on the newly created application. A graph of kubernetes resources should be shown if everything is configured correctly. Make a change in the GitOps repo \u00b6 In order to trigger a (re-)deployment we can make an update to a value in the GitOps repo and watch ArgoCD apply the change. Open a terminal and navigate to your GitOps repo directory Be sure that you are in the test branch git checkout test Update inventory-management-svc-{initials}/values.yaml to increase the replica count ```yaml title=\"inventory-management-svc-{initials}/values.yaml\" global: ingressSubdomain: sms-test.us-south.containers.appdomain.cloud tlsSecretName: sms-test-cluster inventory-management-svc-{initials}: replicaCount: 3 ingress: enabled: true tlsSecretName: sms-test-cluster ``` Commit and push the change git add . git commit -m \"Increases replica count\" Log into the ArgoCD UI and look at the state of the application. It should say Synchronizing . If you don't want to wait you can manually by pressing the Synchronize button. Hook the CI pipeline to the CD pipeline \u00b6 The last stage in the CI pipeline updates the version number in the requirements.yaml to the version of the helm chart that was just built. Through a couple naming conventions the only thing the pipeline needs in order to interact with the CD process is a kubernetes secret named gitops-cd-secret that provides the details needed to connect to the git repo to push updates. The IGC CLI has a command that provides a helper to make the creating of a kubernetes secret with git credentials very easy. Log into the cluster on the command-line and select your dev project. Run igc gitops <GITOPS_REPO_URL> . This command will prompt for the username, personal access token, and the branch to use. What just happened? \u00b6 The igc gitops command creates a secret git-credentials and a configmap named gitops-repo in the OpenShift project. These contain the url, username, password, and branch information for the GitOps repository. You can verify the secret was created by running: oc get configmap/gitops-repo -n dev- { initials } -o yaml oc get secret/git-credentials -n dev- { initials } -o yaml Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running (e.g. dev-{initials} ). The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. test is the recommended value for the branch field. Trigger the pipeline for the Inventory Management service to build by making a change to the Inventory Management Service code and push the changes to Git. Repeat for BFF and UI components \u00b6 Starting from Configure the GitOps repo for Inventory Management service , the steps need to be repeated for each application within the project.","title":"CD for Inventory App"},{"location":"developer-advanced-1/inventory-cd/#guide","text":"This Micro App guidance continues to build upon the micro-services that were built in the Inventory Micro App guide. Make sure you have complete Inventory Application or deployed the working Inventory Solution . We implemented the three tiers in the Inventory Mico App and deployed the app to the dev namespace/project. We will take that app and make these additions. Deploy the app to the test namespace/project using CD techniques and ArgoCD","title":"Guide"},{"location":"developer-advanced-1/inventory-cd/#using-cd-to-deploy-to-test","text":"ArgoCD is a tool that provides continuous delivery for projects and applications. If you haven't already, be sure to read through the Continuous Delivery with ArgoCD guide . For this exercise, we are going to use ArgoCD to push the Inventory app from dev to test (and possibly staging as well). If you have already completed the Inventory Micro App , then it can be used for the ArgoCD process (although perhaps with some minor pipeline updates). If you haven't completed the exercise, you can start from the solution repositories to perform the ArgoCD steps.","title":"Using CD to deploy to Test"},{"location":"developer-advanced-1/inventory-cd/#set-up-the-gitops-repo","text":"Let's get started with using Argo CD. Create a new repo from the ArgoCD Code Pattern Clone the project to your machine Create a branch named test git checkout -b test Push the branch to the remote git push -u origin test Create the test namespace with the CLI by running oc sync test-{initials} --dev","title":"Set up the GitOps repo"},{"location":"developer-advanced-1/inventory-cd/#register-the-gitops-repo-in-argocd","text":"Now that the repository has been created, we need to tell ArgoCD where it is. Get the ArgoCD login information from the oc credentials cli command Note You need to be logged into the cluster on the command-line for the CLI to access the cluster information. Log into ArgoCD (use oc credentials to obtain your credentials and login to argo) Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created.","title":"Register the GitOps repo in ArgoCD"},{"location":"developer-advanced-1/inventory-cd/#create-a-project-in-argocd-optional","text":"In ArgoCD terms, each deployable component is an Application and applications are grouped into Projects . Projects are not required for ArgoCD to be able to deploy applications but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. To create a project, do the following: Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Projects option Click the New Project button at the top of the page. Provide the following values then press Create : name - the name for the project (provide `inventory-management) description - a brief description of the project sources - click add source and pick the Git repository from the list that was added previously destinations Add https://kubernetes.default.svc for the cluster url and test-{initials} for the namespace Add https://kubernetes.default.svc for the cluster url and staging-{initials} for the namespace Note: Initially, the only cluster that is available is the one in which ArgoCD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the test-{initials} and staging-{initials} namespaces within the current cluster.","title":"Create a project in ArgoCD (Optional)"},{"location":"developer-advanced-1/inventory-cd/#configure-the-gitops-repo-for-inventory-management-service","text":"Clone the GitOps repository you created earlier, copy the folder templates/app-helm to the root of the repository and give it a name that matches the Inventory Management service component (e.g. inventory-management-svc-{initials} ) Update inventory-management-svc-{initials}/Chart.yaml and update the name to match the directory name Update inventory-management-svc-{initials}/requirements.yaml with the following values: name - the name of helm chart/image. This should match the folder name version - the version number of the helm chart repository - the url to the helm repository including the folder where helm charts are being stored. here is an example dependencies : - name : inventory-management-svc-mjp version : 1.0.0-1 repository : http://artifactory.mooc-one-rhos-cluster.us-east.containers.appdomain.cloud/artifactory/generic-local/mooc-team-one/ The url of the Artifactory helm repository can be taken from the below step. In the Artifactory Setup screen, in Set Me Up Section, select the tool as \"Generic\" and repository as \"generic-local\". Copy the deploy URL from the Set Me Up dialog box. That is the Artifactory helm repository URL. Run kubectl get configmap/ibmcloud-config -n tools -o yaml to print the configuration information for the cluster In inventory-management-svc-{initials}/values.yaml replace <app-chart-name> with the directory name. Replace ingressSubdomain with the value from the previous step. Update tlsSecretName with the value from the previous step. The result should look something like the following ```yaml title=\"inventory-management-svc-{initials}/values.yaml\" global: ingressSubdomain: sms-test.us-south.containers.appdomain.cloud tlsSecretName: sms-test-cluster inventory-management-svc-{initials}: replicaCount: 1 ingress: enabled: true tlsSecretName: sms-test-cluster ``` Commit and push the changes git add . git commit -m \"Adds inventory-management-svc config\" git push","title":"Configure the GitOps repo for Inventory Management service"},{"location":"developer-advanced-1/inventory-cd/#add-an-application-in-argocd-for-the-inventory-management-service","text":"The last step in the process is to define the application(s) within ArgoCD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Log into ArgoCD Click New Application and provide the following values: application name - test-inventory-management-svc project - inventory-management sync-policy - Automatic repository url - The url of the GitOps repository you created earlier revision - test path - inventory-management-svc-{initials} destination cluster - The cluster url for the deployment destination namespace - test-{initials} values file - values.yaml Click Create Click on the newly created application. A graph of kubernetes resources should be shown if everything is configured correctly.","title":"Add an application in ArgoCD for the Inventory Management service"},{"location":"developer-advanced-1/inventory-cd/#make-a-change-in-the-gitops-repo","text":"In order to trigger a (re-)deployment we can make an update to a value in the GitOps repo and watch ArgoCD apply the change. Open a terminal and navigate to your GitOps repo directory Be sure that you are in the test branch git checkout test Update inventory-management-svc-{initials}/values.yaml to increase the replica count ```yaml title=\"inventory-management-svc-{initials}/values.yaml\" global: ingressSubdomain: sms-test.us-south.containers.appdomain.cloud tlsSecretName: sms-test-cluster inventory-management-svc-{initials}: replicaCount: 3 ingress: enabled: true tlsSecretName: sms-test-cluster ``` Commit and push the change git add . git commit -m \"Increases replica count\" Log into the ArgoCD UI and look at the state of the application. It should say Synchronizing . If you don't want to wait you can manually by pressing the Synchronize button.","title":"Make a change in the GitOps repo"},{"location":"developer-advanced-1/inventory-cd/#hook-the-ci-pipeline-to-the-cd-pipeline","text":"The last stage in the CI pipeline updates the version number in the requirements.yaml to the version of the helm chart that was just built. Through a couple naming conventions the only thing the pipeline needs in order to interact with the CD process is a kubernetes secret named gitops-cd-secret that provides the details needed to connect to the git repo to push updates. The IGC CLI has a command that provides a helper to make the creating of a kubernetes secret with git credentials very easy. Log into the cluster on the command-line and select your dev project. Run igc gitops <GITOPS_REPO_URL> . This command will prompt for the username, personal access token, and the branch to use.","title":"Hook the CI pipeline to the CD pipeline"},{"location":"developer-advanced-1/inventory-cd/#what-just-happened","text":"The igc gitops command creates a secret git-credentials and a configmap named gitops-repo in the OpenShift project. These contain the url, username, password, and branch information for the GitOps repository. You can verify the secret was created by running: oc get configmap/gitops-repo -n dev- { initials } -o yaml oc get secret/git-credentials -n dev- { initials } -o yaml Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running (e.g. dev-{initials} ). The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. test is the recommended value for the branch field. Trigger the pipeline for the Inventory Management service to build by making a change to the Inventory Management Service code and push the changes to Git.","title":"What just happened?"},{"location":"developer-advanced-1/inventory-cd/#repeat-for-bff-and-ui-components","text":"Starting from Configure the GitOps repo for Inventory Management service , the steps need to be repeated for each application within the project.","title":"Repeat for BFF and UI components"},{"location":"developer-foundation/","text":"Before you jump straight into Cloud-Native development using this crafted enablement journey, you must understand the related basic concepts, technologies used and methodologies applied for Cloud-Native development. Developer Foundation is broken into three areas: Principles of Cloud, Cloud-Native development & deployment Basics of Containers, Kubernetes/Openshift IBM Garage Method Concepts \u00b6 Topics Covered \u00b6 Self Paced \u00b6 Topics Description Link What is Cloud-Native? Cloud-Native overview. Cloud-Native Cloud-Native Application Development Basics of Cloud-Native application development. Cloud-Native application. development Continuous Integration CI overview. CI Continuous Delivery CD overview. CD Technologies \u00b6 Topics Covered \u00b6 Self Paced Topics Description Link Core Concepts Covers Kubernetes objects and namespaces. Core Concepts Configuration Covers ConfigMaps, Resource Quotas, Secrets, and Service Accounts. Configuration Multi-Container Pods Use Cases for multi container pods. Multi-Container Pods Observability Covers probes and logging for containers. Observability Pod Design Covers Labels, Selectors, Deployments, and Jobs. Pod Design Services & Networking Covers Services and Ingresses. Services and Networking State Persistence Covering Volumes and Persistence Volumes. State Persistence Troubleshooting Ways to troubleshoot your kubernetes. Troubleshooting Activities \u00b6 Self Paced Topics Description Link Pod Creation Covers the topics in the Core Concepts page. Pod Creation Pod Configuration Covers the topics in the Configuration page. Pod Configuration Multiple Containers Covers the topics in the Multi-Container Pods page. Multiple Containers Probes Covers the topics in the Observability page. Probes Rolling Updates Lab Covers the topics in the Pod Design page. Rolling Updates Cron Jobs Lab Covers the topics in the Pod Design Jobs section. Crons Jobs Creating Services Covers the topics in the Services page. Setting up Services Setting up Persistent Volumes Covers the topics in the State Persistence page. Setting up Persistent Volumes Debugging Covers the topics in the Troubleshooting page. Debugging Solutions \u00b6 Self Paced Topics Link Solutions Solutions for Exercises Optional Extra Credits \u00b6 Kubernetes \u00b6 Deployment Best practices \u00b6 Use the following links to get a deep understanding of Cloud-Native Deployment: Method \u00b6 Topics Covered \u00b6 Self Paced Topics Description Link IBM Garage Method Garage Method overview. Garage Method Resources \u00b6 Kubernetes Basics presentation .","title":"Agenda"},{"location":"developer-foundation/#concepts","text":"","title":"Concepts"},{"location":"developer-foundation/#topics-covered","text":"","title":"Topics Covered"},{"location":"developer-foundation/#self-paced","text":"Topics Description Link What is Cloud-Native? Cloud-Native overview. Cloud-Native Cloud-Native Application Development Basics of Cloud-Native application development. Cloud-Native application. development Continuous Integration CI overview. CI Continuous Delivery CD overview. CD","title":"Self Paced"},{"location":"developer-foundation/#technologies","text":"","title":"Technologies"},{"location":"developer-foundation/#topics-covered_1","text":"Self Paced Topics Description Link Core Concepts Covers Kubernetes objects and namespaces. Core Concepts Configuration Covers ConfigMaps, Resource Quotas, Secrets, and Service Accounts. Configuration Multi-Container Pods Use Cases for multi container pods. Multi-Container Pods Observability Covers probes and logging for containers. Observability Pod Design Covers Labels, Selectors, Deployments, and Jobs. Pod Design Services & Networking Covers Services and Ingresses. Services and Networking State Persistence Covering Volumes and Persistence Volumes. State Persistence Troubleshooting Ways to troubleshoot your kubernetes. Troubleshooting","title":"Topics Covered"},{"location":"developer-foundation/#activities","text":"Self Paced Topics Description Link Pod Creation Covers the topics in the Core Concepts page. Pod Creation Pod Configuration Covers the topics in the Configuration page. Pod Configuration Multiple Containers Covers the topics in the Multi-Container Pods page. Multiple Containers Probes Covers the topics in the Observability page. Probes Rolling Updates Lab Covers the topics in the Pod Design page. Rolling Updates Cron Jobs Lab Covers the topics in the Pod Design Jobs section. Crons Jobs Creating Services Covers the topics in the Services page. Setting up Services Setting up Persistent Volumes Covers the topics in the State Persistence page. Setting up Persistent Volumes Debugging Covers the topics in the Troubleshooting page. Debugging","title":"Activities"},{"location":"developer-foundation/#solutions","text":"Self Paced Topics Link Solutions Solutions for Exercises","title":"Solutions"},{"location":"developer-foundation/#optional-extra-credits","text":"","title":"Optional Extra Credits"},{"location":"developer-foundation/#kubernetes","text":"","title":"Kubernetes"},{"location":"developer-foundation/#deployment-best-practices","text":"Use the following links to get a deep understanding of Cloud-Native Deployment:","title":"Deployment Best practices"},{"location":"developer-foundation/#method","text":"","title":"Method"},{"location":"developer-foundation/#topics-covered_2","text":"Self Paced Topics Description Link IBM Garage Method Garage Method overview. Garage Method","title":"Topics Covered"},{"location":"developer-foundation/#resources","text":"Kubernetes Basics presentation .","title":"Resources"},{"location":"developer-foundation/activities/labs/","text":"Kubernetes \u00b6 These activities give you a chance to walk through basic kubernetes tasks via Katacoda. Then give them a try yourself by solving some problems from scratch. These tasks assume that you have: - Reviewed Kubernetes Concept pages. - Created a Mini-Kube cluster. - Created a Katacoda Account. Task Description Link Time Walk-throughs Creating a Cluster Learn how to launch a Single Node Minikube cluster including DNS and Kube UI. Cluster Creation 30 min Deploy Containers Using Kubectl Learn how to use Kubectl to create and launch Deployments, Replication Controllers and expose them via Services without writing yaml definitions. Using Kubectl 15 mins Deploy Containers Using YAML Learn how to use Kubectl to create and launch Deployments, Replication Controllers and expose them via Services by writing yaml definitions. Using YAML 15 mins Intro to Networking Learn about core Kubernetes Networking capabilities Networking 30 mins Health checks Learn how Kubernetes checks containers health using Readiness and Liveness Probes. Liveness and Readiness Probes 20 min Try It Yourself Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Pod Creation 30 min Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration 30 min Multiple Containers Build a container using legacy container image. Multiple Containers 30 min Probes Create some Health & Startup Probes to find what's causing an issue. Probes 30 min Debugging Find which service is breaking in your cluster and find out why. Debugging 30 min Rolling Updates Roll out a new version of a video game. Rolling Updates 30 min Cron Jobs Create a Cron Job to run some maintenance. Cron Jobs 30 min Creating Services Create two services with certain requirements. Setting up Services 45 min Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes 1 hr Once you have completed these tasks, you should have a basic understanding for Kubernetes.","title":"Kubernetes Activities"},{"location":"developer-foundation/activities/labs/#kubernetes","text":"These activities give you a chance to walk through basic kubernetes tasks via Katacoda. Then give them a try yourself by solving some problems from scratch. These tasks assume that you have: - Reviewed Kubernetes Concept pages. - Created a Mini-Kube cluster. - Created a Katacoda Account. Task Description Link Time Walk-throughs Creating a Cluster Learn how to launch a Single Node Minikube cluster including DNS and Kube UI. Cluster Creation 30 min Deploy Containers Using Kubectl Learn how to use Kubectl to create and launch Deployments, Replication Controllers and expose them via Services without writing yaml definitions. Using Kubectl 15 mins Deploy Containers Using YAML Learn how to use Kubectl to create and launch Deployments, Replication Controllers and expose them via Services by writing yaml definitions. Using YAML 15 mins Intro to Networking Learn about core Kubernetes Networking capabilities Networking 30 mins Health checks Learn how Kubernetes checks containers health using Readiness and Liveness Probes. Liveness and Readiness Probes 20 min Try It Yourself Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Pod Creation 30 min Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration 30 min Multiple Containers Build a container using legacy container image. Multiple Containers 30 min Probes Create some Health & Startup Probes to find what's causing an issue. Probes 30 min Debugging Find which service is breaking in your cluster and find out why. Debugging 30 min Rolling Updates Roll out a new version of a video game. Rolling Updates 30 min Cron Jobs Create a Cron Job to run some maintenance. Cron Jobs 30 min Creating Services Create two services with certain requirements. Setting up Services 45 min Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes 1 hr Once you have completed these tasks, you should have a basic understanding for Kubernetes.","title":"Kubernetes"},{"location":"developer-foundation/activities/labs/lab1/","text":"HANDS ON LAB: Pod Creation <div class=\"bx--row\"> <div class=\"bx--col-sm-2 bx--col-md-2 bx--col-lg-2\"> <div id=\"slideshowNavigator\" style=\"font-size:15px; text-align: center; border-right: 1px solid gray;\"> <div style=\"padding: 10px 0 10px 0px; width:70px\"> <img src=\"images/goal.png\"/> Problem </div> <div style=\"padding: 10px 0 10px 0px; width:80px; \"> <img src=\"images/learn.png\"/> <p> What you'll learn</p> </div> <div style=\"padding: 10px 0 10px 0px; width:55px\"> <img src=\"images/faq.png\"/> Solution </div> </div> </div> <div class=\"bx--col-sm-7 bx--col-md-7 bx--col-lg-7\"> <div class=\"bx--row\" style=\"margin: 25px 10px;\">Create the POD and verify the POD is working or not.</div> <div class=\"bx--row\" style=\"padding-top: 60px; padding-left: 10px;\"> To learn the POD creation with nginx container image. </div> <div class=\"bx--row\" style=\"padding-top: 25px;\"> <ul style=\"padding-left: 10px; !important\"> <li>Create the POD definition YAML file</li> <li>Execute the command with specific namespace for creating the POD in Openshift Cluster.</li> <li>Execute the command to verify whether the POD is created or not.</li> </ul> </div> </div> <div class=\"bx--col-sm-2 bx--col-md-2 bx--col-lg-2\"> <div class=\"bx--row\" style=\"padding-top: 15px;\"> Difficulty Level </div> <div class=\"bx--row\" style=\"padding-top: 6px;\"> Duration </div> </div> <div class=\"bx--col-sm-1 bx--col-md-1 bx--col-lg-1\"> <div class=\"bx--row\" style=\"font-weight:bold; padding-top: 15px;\"> Beginner </div> <div class=\"bx--row\" style=\"font-weight:bold; padding-top: 7px;\"> 10 Min </div> </div> </div> apiVersion: v1 kind: Pod metadata: name: nginx namespace: {DEV_NAMESPACE} spec: containers: - name: nginx image: nginx command: [\"nginx\"] args: [\"-g\", \"daemon off;\", \"-q\"] ports: - containerPort: 80 Create Yaml file using above POD definition. Execute the below command. shell script oc apply -f Downloads/podcreation.yaml -n {DEV_NAMESPACE} Verification \u00b6 When you have completed this lab, use the following commands to validate your solution. The 'get pods' command will shell script kubectl get pods -n {DEV_NAMESPACE} kubectl describe pod nginx -n {DEV_NAMESPACE}","title":"Kubernetes Lab 1 - Pod Creation"},{"location":"developer-foundation/activities/labs/lab1/#verification","text":"When you have completed this lab, use the following commands to validate your solution. The 'get pods' command will shell script kubectl get pods -n {DEV_NAMESPACE} kubectl describe pod nginx -n {DEV_NAMESPACE}","title":"Verification"},{"location":"developer-foundation/activities/labs/lab1/solution/","text":"Solution \u00b6 apiVersion : v1 kind : Pod metadata : name : nginx namespace : web spec : containers : - name : nginx image : bitnami/nginx command : [ \"nginx\" ] args : [ \"-g\" , \"daemon off;\" , \"-q\" ] ports : - containerPort : 80","title":"Kubernetes Lab 1 - Pod Creation"},{"location":"developer-foundation/activities/labs/lab1/solution/#solution","text":"apiVersion : v1 kind : Pod metadata : name : nginx namespace : web spec : containers : - name : nginx image : bitnami/nginx command : [ \"nginx\" ] args : [ \"-g\" , \"daemon off;\" , \"-q\" ] ports : - containerPort : 80","title":"Solution"},{"location":"developer-foundation/activities/labs/lab10/","text":"Problem \u00b6 The death star plans can't be lost no matter what happens so we need to make sure we protect them at all costs. Solution \u00b6 In order to do that you will need to do the following: Create a PersistentVolume : \u00b6 The PersistentVolume should be named postgresql-pv . The volume needs a capacity of 1Gi . Use a storageClassName of localdisk . Use the accessMode ReadWriteOnce . Store the data locally on the node using a hostPath volume at the location /mnt/data . Execute the below commands to creating PersistentVolume using yaml definition. shell script oc apply -f pv.yaml -n {DEV_NAMESPACE} yaml apiVersion: v1 kind: PersistentVolume metadata: name: postgresql-pv spec: storageClassName: localdisk capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" Create a PersistentVolumeClaim : \u00b6 The PersistentVolumeClaim should be named postgresql-pv-claim . Set a resource request on the claim for 500Mi of storage. Use the same storageClassName and accessModes as the PersistentVolume so that this claim can bind to the PersistentVolume. Execute the below commands to creating PersistentVolumeClaim using yaml definition. shell script oc apply -f pvc.yaml -n {DEV_NAMESPACE} yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgresql-pv-claim spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 500Mi Create a Postgresql Pod configured to use the PersistentVolumeClaim : \u00b6 The Pod should be named postgresql-pod . Use the image bitnami/postgresql . Expose the containerPort 5432 . Set an environment variable called MYSQL_ROOT_PASSWORD with the value password . Add the PersistentVolumeClaim as a volume and mount it to the container at the path /bitnami/postgresql/ . Execute the below commands to creating POD using yaml definition. shell script oc apply -f postgresql-pod.yaml -n {DEV_NAMESPACE} yaml apiVersion: v1 kind: Pod metadata: name: postgresql-pod spec: containers: - name: postgresql image: bitnami/postgresql ports: - containerPort: 5432 env: - name: MYSQL_ROOT_PASSWORD value: password volumeMounts: - name: sql-storage mountPath: /bitnami/postgresql/ volumes: - name: sql-storage persistentVolumeClaim: claimName: postgresql-pv-claim","title":"Kubernetes Lab 10 - Persistent Volumes"},{"location":"developer-foundation/activities/labs/lab10/#problem","text":"The death star plans can't be lost no matter what happens so we need to make sure we protect them at all costs.","title":"Problem"},{"location":"developer-foundation/activities/labs/lab10/#solution","text":"In order to do that you will need to do the following:","title":"Solution"},{"location":"developer-foundation/activities/labs/lab10/#create-a-persistentvolume","text":"The PersistentVolume should be named postgresql-pv . The volume needs a capacity of 1Gi . Use a storageClassName of localdisk . Use the accessMode ReadWriteOnce . Store the data locally on the node using a hostPath volume at the location /mnt/data . Execute the below commands to creating PersistentVolume using yaml definition. shell script oc apply -f pv.yaml -n {DEV_NAMESPACE} yaml apiVersion: v1 kind: PersistentVolume metadata: name: postgresql-pv spec: storageClassName: localdisk capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\"","title":"Create a PersistentVolume:"},{"location":"developer-foundation/activities/labs/lab10/#create-a-persistentvolumeclaim","text":"The PersistentVolumeClaim should be named postgresql-pv-claim . Set a resource request on the claim for 500Mi of storage. Use the same storageClassName and accessModes as the PersistentVolume so that this claim can bind to the PersistentVolume. Execute the below commands to creating PersistentVolumeClaim using yaml definition. shell script oc apply -f pvc.yaml -n {DEV_NAMESPACE} yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgresql-pv-claim spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 500Mi","title":"Create a PersistentVolumeClaim:"},{"location":"developer-foundation/activities/labs/lab10/#create-a-postgresql-pod-configured-to-use-the-persistentvolumeclaim","text":"The Pod should be named postgresql-pod . Use the image bitnami/postgresql . Expose the containerPort 5432 . Set an environment variable called MYSQL_ROOT_PASSWORD with the value password . Add the PersistentVolumeClaim as a volume and mount it to the container at the path /bitnami/postgresql/ . Execute the below commands to creating POD using yaml definition. shell script oc apply -f postgresql-pod.yaml -n {DEV_NAMESPACE} yaml apiVersion: v1 kind: Pod metadata: name: postgresql-pod spec: containers: - name: postgresql image: bitnami/postgresql ports: - containerPort: 5432 env: - name: MYSQL_ROOT_PASSWORD value: password volumeMounts: - name: sql-storage mountPath: /bitnami/postgresql/ volumes: - name: sql-storage persistentVolumeClaim: claimName: postgresql-pv-claim","title":"Create a Postgresql Pod configured to use the PersistentVolumeClaim:"},{"location":"developer-foundation/activities/labs/lab10/solution/","text":"Solution \u00b6 apiVersion : v1 kind : PersistentVolume metadata : name : postgresql-pv spec : storageClassName : localdisk capacity : storage : 1Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" apiVersion : v1 kind : PersistentVolumeClaim metadata : name : postgresql-pv-claim spec : storageClassName : localdisk accessModes : - ReadWriteOnce resources : requests : storage : 500Mi apiVersion : v1 kind : Pod metadata : name : postgresql-pod spec : containers : - name : postgresql image : bitnami/postgresql ports : - containerPort : 5432 env : - name : MYSQL_ROOT_PASSWORD value : password volumeMounts : - name : sql-storage mountPath : /bitnami/postgresql/ volumes : - name : sql-storage persistentVolumeClaim : claimName : postgresql-pv-claim verify via ls /mnt/data on node","title":"Kubernetes Lab 10 - Persistent Volumes"},{"location":"developer-foundation/activities/labs/lab10/solution/#solution","text":"apiVersion : v1 kind : PersistentVolume metadata : name : postgresql-pv spec : storageClassName : localdisk capacity : storage : 1Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" apiVersion : v1 kind : PersistentVolumeClaim metadata : name : postgresql-pv-claim spec : storageClassName : localdisk accessModes : - ReadWriteOnce resources : requests : storage : 500Mi apiVersion : v1 kind : Pod metadata : name : postgresql-pod spec : containers : - name : postgresql image : bitnami/postgresql ports : - containerPort : 5432 env : - name : MYSQL_ROOT_PASSWORD value : password volumeMounts : - name : sql-storage mountPath : /bitnami/postgresql/ volumes : - name : sql-storage persistentVolumeClaim : claimName : postgresql-pv-claim verify via ls /mnt/data on node","title":"Solution"},{"location":"developer-foundation/activities/labs/lab2/","text":"HANDS ON LAB: Pod Configuration <div class=\"bx--row\"> <div class=\"bx--col-sm-2 bx--col-md-2 bx--col-lg-2\"> <div id=\"slideshowNavigator\" style=\"font-size:15px; text-align: center; border-right: 1px solid gray;\"> <div style=\"padding: 10px 0 10px 0px; width:70px\"> <img src=\"images/goal.png\"/> Problem </div> <div style=\"padding: 10px 0 10px 0px; width:80px; \"> <img src=\"images/learn.png\"/> <p> What you'll learn</p> </div> <div style=\"padding: 10px 0 10px 0px; width:55px\"> <img src=\"images/faq.png\"/> Exercises </div> </div> </div> <div class=\"bx--col-sm-7 bx--col-md-7 bx--col-lg-7\"> <div class=\"bx--row\" style=\"padding: 22px 10px;\"> Create definition file like <span style=\"padding: 0px 5px 0px 10px;background-color: grey;border-radius: 4px;\"> yoda-service-pod.yml </span>, and then using the definition file create a POD, ConfigMap,Secret and Service Account. </div> <div class=\"bx--row\" style=\"padding: 20px 0px 0px 10px;\"> To learn the POD, ConfigMap, Secret and Service Account creation and configuration. </div> <div class=\"bx--row\" style=\"padding: 50px 0px 0px 0px;\"> <ul style=\"padding-left: 10px; !important\"> <li>Create the definition file for POD, ConfigMap, Secret and Service Account</li> <li>Execute the command and create the respective resources</li> <li>Verify the created resources</li> </ul> </div> </div> <div class=\"bx--col-sm-2 bx--col-md-2 bx--col-lg-2\"> <div class=\"bx--row\" style=\"padding-top: 15px;\"> Difficulty Level </div> <div class=\"bx--row\" style=\"padding-top: 6px;\"> Duration </div> </div> <div class=\"bx--col-sm-1 bx--col-md-1 bx--col-lg-1\"> <div class=\"bx--row\" style=\"font-weight:bold; padding-top: 15px;\"> Beginner </div> <div class=\"bx--row\" style=\"font-weight:bold; padding-top: 7px;\"> 15 Min </div> </div> </div> apiVersion: v1 kind: ConfigMap metadata: name: yoda-service-config data: yoda.cfg: |- yoda.baby.power=100000000 yoda.strength=10 apiVersion: v1 kind: Secret metadata: name: yoda-db-password stringData: password: 0penSh1ftRul3s! apiVersion: v1 kind: Pod metadata: name: yoda-service spec: serviceAccountName: yoda-svc containers: - name: yoda-service image: bitnami/nginx volumeMounts: - name: config-volume mountPath: /etc/yoda-service env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: yoda-db-password key: password resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" volumes: - name: config-volume configMap: name: yoda-service-config key:service-config Create the Service Account executing the following command. shell script oc create sa yoda-svc -n {DEV_NAMESPACE} Once the Service Account created, create the ConfigMap,Secret. Create the Yaml file for POD, ConfigMap, Secret using below POD configuration. Execute the below command for creating ConfigMap,Secret. shell script oc apply -f Downloads/yoda-service-config.yaml -n {DEV_NAMESPACE} oc apply -f Downloads/yoda-db-password.yaml -n {DEV_NAMESPACE} - Execute the below command for creating the POD. shell script oc apply -f Downloads/yoda-service-pod.yml -n {DEV_NAMESPACE} Verification \u00b6 To verify your setup is complete, check /etc/yoda-service for the yoda.cfg file and use the cat command to check it's contents. kubectl exec -it yoda-service /bin/bash cd /etc/yoda-service cat yoda.cfg","title":"Kubernetes Lab 2 - Pod Configuration"},{"location":"developer-foundation/activities/labs/lab2/#verification","text":"To verify your setup is complete, check /etc/yoda-service for the yoda.cfg file and use the cat command to check it's contents. kubectl exec -it yoda-service /bin/bash cd /etc/yoda-service cat yoda.cfg","title":"Verification"},{"location":"developer-foundation/activities/labs/lab2/solution/","text":"Solution \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : yoda-service-config data : yoda.cfg : |- yoda.baby.power=100000000 yoda.strength=10 apiVersion : v1 kind : ServiceAccount metadata : name : yoda-svc apiVersion : v1 kind : Secret metadata : name : yoda-db-password stringData : password : 0penSh1ftRul3s! apiVersion : v1 kind : Pod metadata : name : yoda-service spec : serviceAccountName : yoda-svc containers : - name : yoda-service image : bitnami/nginx volumeMounts : - name : config-volume mountPath : /etc/yoda-service env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : yoda-db-password key : password resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" volumes : - name : config-volume configMap : name : yoda-service-config","title":"Kubernetes Lab 2 - Pod Configuration"},{"location":"developer-foundation/activities/labs/lab2/solution/#solution","text":"apiVersion : v1 kind : ConfigMap metadata : name : yoda-service-config data : yoda.cfg : |- yoda.baby.power=100000000 yoda.strength=10 apiVersion : v1 kind : ServiceAccount metadata : name : yoda-svc apiVersion : v1 kind : Secret metadata : name : yoda-db-password stringData : password : 0penSh1ftRul3s! apiVersion : v1 kind : Pod metadata : name : yoda-service spec : serviceAccountName : yoda-svc containers : - name : yoda-service image : bitnami/nginx volumeMounts : - name : config-volume mountPath : /etc/yoda-service env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : yoda-db-password key : password resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" volumes : - name : config-volume configMap : name : yoda-service-config","title":"Solution"},{"location":"developer-foundation/activities/labs/lab3/","text":"HANDS ON LAB: Manage Multiple Containers <div class=\"bx--row\"> <div class=\"bx--col-sm-2 bx--col-md-2 bx--col-lg-2\"> <div id=\"slideshowNavigator\" style=\"font-size:15px; text-align: center; border-right: 1px solid gray;\"> <div style=\"padding: 10px 0 10px 0px; width:70px\"> <img src=\"images/goal.png\"/> Problem </div> <div style=\"padding: 10px 0 10px 0px; width:80px; \"> <img src=\"images/learn.png\"/> <p> What you'll learn</p> </div> <div style=\"padding: 10px 0 10px 0px; width:55px\"> <img src=\"images/faq.png\"/> Solution </div> <div style=\"padding: 50px 0 0px 0px; width:55px\"> <img src=\"images/explore.png\"/> Explore </div> </div> </div> <div class=\"bx--col-sm-7 bx--col-md-7 bx--col-lg-7\"> <div class=\"bx--row\" style=\"padding: 22px 10px;\"> The legacy app is hard-coded to only serve content on port 8989, but the team wants to be able to access the service using the standard port 80 </div> <div class=\"bx--row\" style=\"padding: 10px 0px 0px;\"> <ul style=\"padding-left: 10px; !important\"> <li> To learn the POD, ConfigMap creation and configuration. </li> <li> HAProxy configuration. </li> </ul> </div> <div class=\"bx--row\"> <ul style=\"padding-left: 10px; !important\"> <li>Build a Kubernetes pod that runs this legacy container and uses the ambassador design pattern to expose access to the service on port 80</li> <li>Create the POD and ConfigMap definition</li> <li>Create the POD with ambassador container thats run on haproxy:1.7</li> <li>The HAProxy configuration should be stored in a ConfigMap</li> </ul> </div> <div class=\"bx--row\"> <ul style=\"padding-left: 10px; !important\"> <li>The Ambassador Container Pattern</li> <li>Sidecar Pattern</li> <li>Adapter Pattern</li> </ul> </div> </div> <div class=\"bx--col-sm-2 bx--col-md-2 bx--col-lg-2\"> <div class=\"bx--row\" style=\"padding-top: 15px;\"> Difficulty Level </div> <div class=\"bx--row\" style=\"padding-top: 6px;\"> Duration </div> </div> <div class=\"bx--col-sm-1 bx--col-md-1 bx--col-lg-1\"> <div class=\"bx--row\" style=\"font-weight:bold; padding-top: 15px;\"> Beginner </div> <div class=\"bx--row\" style=\"font-weight:bold; padding-top: 7px;\"> 25 Min </div> </div> </div> apiVersion: v1 kind: ConfigMap metadata: name: vader-service-ambassador-config data: haproxy.cfg: |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8989 maxconn 32 apiVersion: v1 kind: Pod metadata: name: vader-service spec: containers: - name: millennium-falcon image: ibmcase/millennium-falcon:1 - name: haproxy-ambassador image: haproxy:1.7 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /usr/local/etc/haproxy volumes: - name: config-volume configMap: name: vader-service-ambassador-config key: vader-config-map apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: myapp-container image: radial/busyboxplus:curl command: ['sh', '-c', 'while true; do sleep 3600; done'] Create the ConfigMap. shell script oc apply -f vader-service-ambassador-config.yaml -n {DEV_NAMESPACE} - Create the service container pod. shell script oc apply -f vader-service.yaml -n {DEV_NAMESPACE} - Create the application container pod. shell script oc apply -f busybox.yml -n {DEV_NAMESPACE} Use this command to access vader-service using port 80 from within the busybox pod. oc exec busybox -- curl $(kubectl get pod vader-service -o=custom-columns=IP:.status.podIP --no-headers):80 If the service is working, you should get a message that the hyper drive of the millennium falcon needs repair. Relevant Documentation: - Kubernetes Sidecar Logging Agent - Shared Volumes - Distributed System Toolkit Patterns","title":"Kubernetes Lab 3 - Manage Multiple Containers"},{"location":"developer-foundation/activities/labs/lab3/solution/","text":"Solution \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : vader-service-ambassador-config data : haproxy.cfg : |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8775 maxconn 32 apiVersion : v1 kind : Pod metadata : name : vader-service spec : containers : - name : millennium-falcon image : ibmcase/millennium-falcon:1 - name : haproxy-ambassador image : haproxy:1.7 ports : - containerPort : 80 volumeMounts : - name : config-volume mountPath : /usr/local/etc/haproxy volumes : - name : config-volume configMap : name : vader-service-ambassador-config apiVersion : v1 kind : Pod metadata : name : busybox spec : containers : - name : myapp-container image : radial/busyboxplus:curl command : [ 'sh' , '-c' , 'while true; do sleep 3600; done' ] kubectl exec busybox -- curl $( kubectl get pod vader-service -o = jsonpath = '{.status.podIP}' ) :80","title":"Kubernetes Lab 3 - Manage Multiple Containers"},{"location":"developer-foundation/activities/labs/lab3/solution/#solution","text":"apiVersion : v1 kind : ConfigMap metadata : name : vader-service-ambassador-config data : haproxy.cfg : |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8775 maxconn 32 apiVersion : v1 kind : Pod metadata : name : vader-service spec : containers : - name : millennium-falcon image : ibmcase/millennium-falcon:1 - name : haproxy-ambassador image : haproxy:1.7 ports : - containerPort : 80 volumeMounts : - name : config-volume mountPath : /usr/local/etc/haproxy volumes : - name : config-volume configMap : name : vader-service-ambassador-config apiVersion : v1 kind : Pod metadata : name : busybox spec : containers : - name : myapp-container image : radial/busyboxplus:curl command : [ 'sh' , '-c' , 'while true; do sleep 3600; done' ] kubectl exec busybox -- curl $( kubectl get pod vader-service -o = jsonpath = '{.status.podIP}' ) :80","title":"Solution"},{"location":"developer-foundation/activities/labs/lab4/","text":"Problem Statement I - Container Health Issues \u00b6 The first issue is caused by application instances entering an unhealthy state and responding to user requests with error messages. Unfortunately, this state does not cause the container to stop, so the Kubernetes cluster is not able to detect this state and restart the container. Luckily, the application has an internal endpoint that can be used to detect whether or not it is healthy. This endpoint is /healthz on port 8080 . Solution \u00b6 Your first task will be to create a probe to check this endpoint periodically. If the endpoint returns an error or fails to respond, the probe will detect this and the cluster will restart the container. Problem Statement II - Container Startup Issues \u00b6 Another issue is caused by new pods when they are starting up. The application takes a few seconds after startup before it is ready to service requests. As a result, some users are getting error message during this brief time. Solution \u00b6 To fix this, you will need to create another probe . To detect whether the application is ready , the probe should simply make a request to the root endpoint, /ready , on port 8080 . If this request succeeds, then the application is ready. Also set a initial delay of 5 seconds for the probes. Here is the Pod yaml file, add the probes, then create the pod in the cluster to test it. Create the service container pod. shell script oc apply -f energy-shield-service.yaml -n {DEV_NAMESPACE} apiVersion: v1 kind: Pod metadata: name: energy-shield-service spec: containers: - name: energy-shield image: ibmcase/energy-shield:1 livenessProbe: httpGet: path: /healthz port: 8080 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5","title":"Kubernetes Lab 4 - Probes"},{"location":"developer-foundation/activities/labs/lab4/#problem-statement-i-container-health-issues","text":"The first issue is caused by application instances entering an unhealthy state and responding to user requests with error messages. Unfortunately, this state does not cause the container to stop, so the Kubernetes cluster is not able to detect this state and restart the container. Luckily, the application has an internal endpoint that can be used to detect whether or not it is healthy. This endpoint is /healthz on port 8080 .","title":"Problem Statement I - Container Health Issues"},{"location":"developer-foundation/activities/labs/lab4/#solution","text":"Your first task will be to create a probe to check this endpoint periodically. If the endpoint returns an error or fails to respond, the probe will detect this and the cluster will restart the container.","title":"Solution"},{"location":"developer-foundation/activities/labs/lab4/#problem-statement-ii-container-startup-issues","text":"Another issue is caused by new pods when they are starting up. The application takes a few seconds after startup before it is ready to service requests. As a result, some users are getting error message during this brief time.","title":"Problem Statement II -  Container Startup Issues"},{"location":"developer-foundation/activities/labs/lab4/#solution_1","text":"To fix this, you will need to create another probe . To detect whether the application is ready , the probe should simply make a request to the root endpoint, /ready , on port 8080 . If this request succeeds, then the application is ready. Also set a initial delay of 5 seconds for the probes. Here is the Pod yaml file, add the probes, then create the pod in the cluster to test it. Create the service container pod. shell script oc apply -f energy-shield-service.yaml -n {DEV_NAMESPACE} apiVersion: v1 kind: Pod metadata: name: energy-shield-service spec: containers: - name: energy-shield image: ibmcase/energy-shield:1 livenessProbe: httpGet: path: /healthz port: 8080 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5","title":"Solution"},{"location":"developer-foundation/activities/labs/lab4/solution/","text":"Solution \u00b6 apiVersion : v1 kind : Pod metadata : name : energy-shield-service spec : containers : - name : energy-shield image : ibmcase/energy-shield:1 livenessProbe : httpGet : path : /healthz port : 8080 readinessProbe : httpGet : path : /ready port : 8080 initialDelaySeconds : 5","title":"Kubernetes Lab 4 - Probes"},{"location":"developer-foundation/activities/labs/lab4/solution/#solution","text":"apiVersion : v1 kind : Pod metadata : name : energy-shield-service spec : containers : - name : energy-shield image : ibmcase/energy-shield:1 livenessProbe : httpGet : path : /healthz port : 8080 readinessProbe : httpGet : path : /ready port : 8080 initialDelaySeconds : 5","title":"Solution"},{"location":"developer-foundation/activities/labs/lab5/","text":"Problem \u00b6 The Hyper Drive isn't working and we need to find out why. Let's debug the hyper-drive deployment so that we can reach light speed again. Here are some tips to help you solve the Hyper Drive: Check the description of the deployment . Get and save the logs of one of the broken pods . Are the correct ports assigned. Make sure your labels and selectors are correct. Check to see if the Probes are correctly working. To fix the deployment, save then modify the yaml file for redeployment. Solution \u00b6 Setup environment \u00b6 Execute the command to create the Deployment, Namespace, Service below definition. shell script oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/lab-setup/lab-5-debug-k8s-setup.yaml - Check the status of the POD. oc get pods -n {DEV_NAMESPACE} - Check the description of the deployment. oc describe deployment hyper-drive -n {DEV_NAMESPACE} - Save logs for broken pod. oc logs <pod name> -n {DEV_NAMESPACE} > /home/cloud_user/debug/broken-pod-logs.log In the description you will see the following is wrong: - Selector and Label names do not match. - The Probe is TCP instead of HTTP Get. - The Service Port is 80 instead of 8080. To fix problem, can't do oc edit on deployment, need to delete and recreate the deployment. So export deployment and do the necessary changes and recreate it. oc get deployment <deployment name> -n {DEV_NAMESPACE} -o yaml --export > hyper-drive.yml - Delete the existing deployment. oc delete deployment <deployment name> -n {DEV_NAMESPACE} Edit the exported YAML file and apply. oc apply -f hyper-drive.yml -n {DEV_NAMESPACE} - Verify the deployment. oc get deployment <deployment name> -n {DEV_NAMESPACE} Validate \u00b6 Once you get the Hyper Drive working again. Verify it by checking the endpoints. kubectl get ep hyper-drive","title":"Kubernetes Lab 5 - Debugging"},{"location":"developer-foundation/activities/labs/lab5/#problem","text":"The Hyper Drive isn't working and we need to find out why. Let's debug the hyper-drive deployment so that we can reach light speed again. Here are some tips to help you solve the Hyper Drive: Check the description of the deployment . Get and save the logs of one of the broken pods . Are the correct ports assigned. Make sure your labels and selectors are correct. Check to see if the Probes are correctly working. To fix the deployment, save then modify the yaml file for redeployment.","title":"Problem"},{"location":"developer-foundation/activities/labs/lab5/#solution","text":"","title":"Solution"},{"location":"developer-foundation/activities/labs/lab5/#setup-environment","text":"Execute the command to create the Deployment, Namespace, Service below definition. shell script oc apply -f https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/lab-setup/lab-5-debug-k8s-setup.yaml - Check the status of the POD. oc get pods -n {DEV_NAMESPACE} - Check the description of the deployment. oc describe deployment hyper-drive -n {DEV_NAMESPACE} - Save logs for broken pod. oc logs <pod name> -n {DEV_NAMESPACE} > /home/cloud_user/debug/broken-pod-logs.log In the description you will see the following is wrong: - Selector and Label names do not match. - The Probe is TCP instead of HTTP Get. - The Service Port is 80 instead of 8080. To fix problem, can't do oc edit on deployment, need to delete and recreate the deployment. So export deployment and do the necessary changes and recreate it. oc get deployment <deployment name> -n {DEV_NAMESPACE} -o yaml --export > hyper-drive.yml - Delete the existing deployment. oc delete deployment <deployment name> -n {DEV_NAMESPACE} Edit the exported YAML file and apply. oc apply -f hyper-drive.yml -n {DEV_NAMESPACE} - Verify the deployment. oc get deployment <deployment name> -n {DEV_NAMESPACE}","title":"Setup environment"},{"location":"developer-foundation/activities/labs/lab5/#validate","text":"Once you get the Hyper Drive working again. Verify it by checking the endpoints. kubectl get ep hyper-drive","title":"Validate"},{"location":"developer-foundation/activities/labs/lab5/solution/","text":"Solution \u00b6 Check STATUS column for not Ready kubectl get pods --all-namespaces Check the description of the deployment kubectl describe deployment hyper-drive Save logs for a broken pod kubectl logs <pod name> -n <namespace> > /home/cloud_user/debug/broken-pod-logs.log In the description you will see the following is wrong: - Selector and Label names do not match. - The Probe is TCP instead of HTTP Get. - The Service Port is 80 instead of 8080. To fix probe, can't kubectl edit, need to delete and recreate the deployment kubectl get deployment <deployment name> -n <namespace> -o yaml --export > hyper-drive.yml Delete pod kubectl delete deployment <deployment name> -n <namespace> Can also use kubectl replace Edit yaml, and apply kubectl apply -f hyper-drive.yml -n <namespace> Verify kubectl get deployment <deployment name> -n <namespace>","title":"Kubernetes Lab 5 - Debugging"},{"location":"developer-foundation/activities/labs/lab5/solution/#solution","text":"Check STATUS column for not Ready kubectl get pods --all-namespaces Check the description of the deployment kubectl describe deployment hyper-drive Save logs for a broken pod kubectl logs <pod name> -n <namespace> > /home/cloud_user/debug/broken-pod-logs.log In the description you will see the following is wrong: - Selector and Label names do not match. - The Probe is TCP instead of HTTP Get. - The Service Port is 80 instead of 8080. To fix probe, can't kubectl edit, need to delete and recreate the deployment kubectl get deployment <deployment name> -n <namespace> -o yaml --export > hyper-drive.yml Delete pod kubectl delete deployment <deployment name> -n <namespace> Can also use kubectl replace Edit yaml, and apply kubectl apply -f hyper-drive.yml -n <namespace> Verify kubectl get deployment <deployment name> -n <namespace>","title":"Solution"},{"location":"developer-foundation/activities/labs/lab6/","text":"Problem \u00b6 Your company's developers have just finished developing a new version of their jedi-themed mobile game. They are ready to update the backend services that are running in your Kubernetes cluster. There is a deployment in the cluster managing the replicas for this application. The deployment is called jedi-deployment . You have been asked to update the image for the container named jedi-ws in this deployment template to a new version, bitnami/nginx:1.18.0 . After you have updated the image using a rolling update, check on the status of the update to make sure it is working. If it is not working, perform a rollback to the previous state. Solution \u00b6 Setup environment \u00b6 Execute the command to create the deployment using below definition. shell script oc apply -f jedi-deployment.yaml -n {DEV_NAMESPACE} yaml apiVersion: apps/v1 kind: Deployment metadata: name: jedi-deployment spec: replicas: 2 selector: matchLabels: app: jedi-deployment template: metadata: labels: app: jedi-deployment spec: containers: - image: bitnami/nginx:1.16.1 name: jedi-ws Rolling Updates \u00b6 Update the deployment to the new version like so: oc set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.0 --record - Check the progress of the rolling update. oc rollout status deployment/jedi-deployment Get the Pods details in another terminal window. oc get pods -w Get a list of previous deployment revisions. oc rollout history deployment/jedi-deployment Undo the last revision. oc rollout undo deployment/jedi-deployment Check the status of the rollout. oc rollout status deployment/jedi-deployment","title":"Kubernetes Lab 6 - Rolling Updates"},{"location":"developer-foundation/activities/labs/lab6/#problem","text":"Your company's developers have just finished developing a new version of their jedi-themed mobile game. They are ready to update the backend services that are running in your Kubernetes cluster. There is a deployment in the cluster managing the replicas for this application. The deployment is called jedi-deployment . You have been asked to update the image for the container named jedi-ws in this deployment template to a new version, bitnami/nginx:1.18.0 . After you have updated the image using a rolling update, check on the status of the update to make sure it is working. If it is not working, perform a rollback to the previous state.","title":"Problem"},{"location":"developer-foundation/activities/labs/lab6/#solution","text":"","title":"Solution"},{"location":"developer-foundation/activities/labs/lab6/#setup-environment","text":"Execute the command to create the deployment using below definition. shell script oc apply -f jedi-deployment.yaml -n {DEV_NAMESPACE} yaml apiVersion: apps/v1 kind: Deployment metadata: name: jedi-deployment spec: replicas: 2 selector: matchLabels: app: jedi-deployment template: metadata: labels: app: jedi-deployment spec: containers: - image: bitnami/nginx:1.16.1 name: jedi-ws","title":"Setup environment"},{"location":"developer-foundation/activities/labs/lab6/#rolling-updates","text":"Update the deployment to the new version like so: oc set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.0 --record - Check the progress of the rolling update. oc rollout status deployment/jedi-deployment Get the Pods details in another terminal window. oc get pods -w Get a list of previous deployment revisions. oc rollout history deployment/jedi-deployment Undo the last revision. oc rollout undo deployment/jedi-deployment Check the status of the rollout. oc rollout status deployment/jedi-deployment","title":"Rolling Updates"},{"location":"developer-foundation/activities/labs/lab6/solution/","text":"Solution \u00b6 Update the deployment to the new version like so: kubectl set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.0 --record Check the progress of the rolling update: kubectl rollout status deployment/jedi-deployment In another terminal window kubectl get pods -w Get a list of previous revisions. kubectl rollout history deployment/jedi-deployment Undo the last revision. kubectl rollout undo deployment/jedi-deployment Check the status of the rollout. kubectl rollout status deployment/jedi-deployment","title":"Kubernetes Lab 6 - Rolling Updates"},{"location":"developer-foundation/activities/labs/lab6/solution/#solution","text":"Update the deployment to the new version like so: kubectl set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.0 --record Check the progress of the rolling update: kubectl rollout status deployment/jedi-deployment In another terminal window kubectl get pods -w Get a list of previous revisions. kubectl rollout history deployment/jedi-deployment Undo the last revision. kubectl rollout undo deployment/jedi-deployment Check the status of the rollout. kubectl rollout status deployment/jedi-deployment","title":"Solution"},{"location":"developer-foundation/activities/labs/lab7/","text":"Problem \u00b6 Your commander has a simple data process that is run periodically to check status. They would like to stop doing this manually in order to save time, so you have been asked to implement a cron job in the Kubernetes cluster to run this process. - Create a cron job called xwing-cronjob using the ibmcase/xwing-status:1.0 image. - Have the job run every second minute with the following cron expression: */2 * * * * . - Pass the argument /usr/sbin/xwing-status.sh to the container. Solution \u00b6 Create the cronjob executing the below command and use the below yaml definition. shell script oc apply -f Downloads/FoundationHandsOn/cronjob.yaml -n {DEV_NAMESPACE} yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: xwing-cronjob spec: schedule: \"*/2 * * * *\" jobTemplate: spec: template: spec: containers: - name: xwing-status image: ibmcase/xwing-status:1.0 args: - /usr/sbin/xwing-status.sh restartPolicy: OnFailure To view th Cronjob. shell script oc get cronjob xwing-cronjob -n -n {DEV_NAMESPACE} Verification \u00b6 Run kubectl get cronjobs.batch and LAST-SCHEDULE to see last time it ran From a bash shell, run the following to see the logs for all jobs: jobs=( $(kubectl get jobs --no-headers -o custom-columns=\":metadata.name\") ) echo -e \"Job \\t\\t\\t\\t Pod \\t\\t\\t\\t\\tLog\" for job in \"${jobs[@]}\" do pod=$(kubectl get pods -l job-name=$job --no-headers -o custom-columns=\":metadata.name\") echo -en \"$job \\t $pod \\t\" kubectl logs $pod done","title":"Kubernetes Lab 7 - Cron Jobs"},{"location":"developer-foundation/activities/labs/lab7/#problem","text":"Your commander has a simple data process that is run periodically to check status. They would like to stop doing this manually in order to save time, so you have been asked to implement a cron job in the Kubernetes cluster to run this process. - Create a cron job called xwing-cronjob using the ibmcase/xwing-status:1.0 image. - Have the job run every second minute with the following cron expression: */2 * * * * . - Pass the argument /usr/sbin/xwing-status.sh to the container.","title":"Problem"},{"location":"developer-foundation/activities/labs/lab7/#solution","text":"Create the cronjob executing the below command and use the below yaml definition. shell script oc apply -f Downloads/FoundationHandsOn/cronjob.yaml -n {DEV_NAMESPACE} yaml apiVersion: batch/v1beta1 kind: CronJob metadata: name: xwing-cronjob spec: schedule: \"*/2 * * * *\" jobTemplate: spec: template: spec: containers: - name: xwing-status image: ibmcase/xwing-status:1.0 args: - /usr/sbin/xwing-status.sh restartPolicy: OnFailure To view th Cronjob. shell script oc get cronjob xwing-cronjob -n -n {DEV_NAMESPACE}","title":"Solution"},{"location":"developer-foundation/activities/labs/lab7/#verification","text":"Run kubectl get cronjobs.batch and LAST-SCHEDULE to see last time it ran From a bash shell, run the following to see the logs for all jobs: jobs=( $(kubectl get jobs --no-headers -o custom-columns=\":metadata.name\") ) echo -e \"Job \\t\\t\\t\\t Pod \\t\\t\\t\\t\\tLog\" for job in \"${jobs[@]}\" do pod=$(kubectl get pods -l job-name=$job --no-headers -o custom-columns=\":metadata.name\") echo -en \"$job \\t $pod \\t\" kubectl logs $pod done","title":"Verification"},{"location":"developer-foundation/activities/labs/lab7/solution/","text":"Solution \u00b6 apiVersion: batch/v1beta1 kind: CronJob metadata: name: xwing-cronjob spec: schedule: \"*/2 * * * *\" jobTemplate: spec: template: spec: containers: - name: xwing-status image: ibmcase/xwing-status:1.0 args: - /usr/sbin/xwing-status.sh restartPolicy: OnFailure kubectl get cronjob xwing-cronjob","title":"Kubernetes Lab 7 - Cron Jobs"},{"location":"developer-foundation/activities/labs/lab7/solution/#solution","text":"apiVersion: batch/v1beta1 kind: CronJob metadata: name: xwing-cronjob spec: schedule: \"*/2 * * * *\" jobTemplate: spec: template: spec: containers: - name: xwing-status image: ibmcase/xwing-status:1.0 args: - /usr/sbin/xwing-status.sh restartPolicy: OnFailure kubectl get cronjob xwing-cronjob","title":"Solution"},{"location":"developer-foundation/activities/labs/lab8/","text":"Problem \u00b6 We have a jedi-deployment and yoda-deployment that need to communicate with others. The jedi needs to talk to the world(outside the cluster), while yoda only needs to talk to jedi council(others in the cluster). Solution \u00b6 Examine the two deployments, and create two services that meet the following criteria: jedi-svc - The service name is jedi-svc . - The service exposes the pod replicas managed by the deployment named jedi-deployment . - The service listens on port 80 and its targetPort matches the port exposed by the pods. - The service type is NodePort . yoda-svc - The service name is yoda-svc . - The service exposes the pod replicas managed by the deployment named yoda-deployment . - The service listens on port 80 and its targetPort matches the port exposed by the pods. - The service type is ClusterIP . Setup environment: \u00b6 Execute the command to create the deployments using below definition. shell script kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-8-service-setup.yaml - Create two services jedi-svc & yoda-svc using below yam definition. ```yaml apiVersion: v1 kind: Service metadata: name: jedi-svc spec: type: NodePort selector: app: jedi ports: - protocol: TCP port: 80 targetPort: 8080 ``` ```yaml apiVersion: v1 kind: Service metadata: name: yoda-svc spec: type: ClusterIP selector: app: yoda ports: - protocol: TCP port: 80 targetPort: 8080 ``` Execute the below commands to creating services. shell script oc apply -f jedi-svc.yaml -n {DEV_NAMESPACE} oc apply -f oda-svc.yaml -n {DEV_NAMESPACE}","title":"Kubernetes Lab 8 - Services"},{"location":"developer-foundation/activities/labs/lab8/#problem","text":"We have a jedi-deployment and yoda-deployment that need to communicate with others. The jedi needs to talk to the world(outside the cluster), while yoda only needs to talk to jedi council(others in the cluster).","title":"Problem"},{"location":"developer-foundation/activities/labs/lab8/#solution","text":"Examine the two deployments, and create two services that meet the following criteria: jedi-svc - The service name is jedi-svc . - The service exposes the pod replicas managed by the deployment named jedi-deployment . - The service listens on port 80 and its targetPort matches the port exposed by the pods. - The service type is NodePort . yoda-svc - The service name is yoda-svc . - The service exposes the pod replicas managed by the deployment named yoda-deployment . - The service listens on port 80 and its targetPort matches the port exposed by the pods. - The service type is ClusterIP .","title":"Solution"},{"location":"developer-foundation/activities/labs/lab8/#setup-environment","text":"Execute the command to create the deployments using below definition. shell script kubectl apply -f https://gist.githubusercontent.com/csantanapr/87df4292e94441617707dae5de488cf4/raw/cb515f7bae77a3f0e76fdc7f6aa0f4e89cc5fec7/lab-8-service-setup.yaml - Create two services jedi-svc & yoda-svc using below yam definition. ```yaml apiVersion: v1 kind: Service metadata: name: jedi-svc spec: type: NodePort selector: app: jedi ports: - protocol: TCP port: 80 targetPort: 8080 ``` ```yaml apiVersion: v1 kind: Service metadata: name: yoda-svc spec: type: ClusterIP selector: app: yoda ports: - protocol: TCP port: 80 targetPort: 8080 ``` Execute the below commands to creating services. shell script oc apply -f jedi-svc.yaml -n {DEV_NAMESPACE} oc apply -f oda-svc.yaml -n {DEV_NAMESPACE}","title":"Setup environment:"},{"location":"developer-foundation/activities/labs/lab8/solution/","text":"Solution \u00b6 apiVersion : v1 kind : Service metadata : name : jedi-svc spec : type : NodePort selector : app : jedi ports : - protocol : TCP port : 80 targetPort : 8080 apiVersion : v1 kind : Service metadata : name : yoda-svc spec : type : ClusterIP selector : app : yoda ports : - protocol : TCP port : 80 targetPort : 8080","title":"Kubernetes Lab 8 - Services"},{"location":"developer-foundation/activities/labs/lab8/solution/#solution","text":"apiVersion : v1 kind : Service metadata : name : jedi-svc spec : type : NodePort selector : app : jedi ports : - protocol : TCP port : 80 targetPort : 8080 apiVersion : v1 kind : Service metadata : name : yoda-svc spec : type : ClusterIP selector : app : yoda ports : - protocol : TCP port : 80 targetPort : 8080","title":"Solution"},{"location":"developer-foundation/activities/labs/lab9/","text":"Problem \u00b6 Setup minikube minikube start --network-plugin=cni kubectl apply -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true kubectl -n kube-system get pods | grep calico-node Create secured pod apiVersion : v1 kind : Pod metadata : name : network-policy-secure-pod labels : app : secure-app spec : containers : - name : nginx image : bitnami/nginx ports : - containerPort : 8080 Create client pod apiVersion : v1 kind : Pod metadata : name : network-policy-client-pod spec : containers : - name : busybox image : radial/busyboxplus:curl command : [ \"/bin/sh\" , \"-c\" , \"while true; do sleep 3600; done\" ] Create a policy to allow only client pods with label allow-access: \"true\" to access secure pod","title":"Kubernetes Lab 9 - Network Policies"},{"location":"developer-foundation/activities/labs/lab9/#problem","text":"Setup minikube minikube start --network-plugin=cni kubectl apply -f https://docs.projectcalico.org/v3.9/manifests/calico.yaml kubectl -n kube-system set env daemonset/calico-node FELIX_IGNORELOOSERPF=true kubectl -n kube-system get pods | grep calico-node Create secured pod apiVersion : v1 kind : Pod metadata : name : network-policy-secure-pod labels : app : secure-app spec : containers : - name : nginx image : bitnami/nginx ports : - containerPort : 8080 Create client pod apiVersion : v1 kind : Pod metadata : name : network-policy-client-pod spec : containers : - name : busybox image : radial/busyboxplus:curl command : [ \"/bin/sh\" , \"-c\" , \"while true; do sleep 3600; done\" ] Create a policy to allow only client pods with label allow-access: \"true\" to access secure pod","title":"Problem"},{"location":"developer-foundation/activities/labs/lab9/solution/","text":"Solution \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : my-network-policy spec : podSelector : matchLabels : app : secure-app policyTypes : - Ingress ingress : - from : - podSelector : matchLabels : allow-access : \"true\"","title":"Kubernetes Lab 9 - Network Policies"},{"location":"developer-foundation/activities/labs/lab9/solution/#solution","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : my-network-policy spec : podSelector : matchLabels : app : secure-app policyTypes : - Ingress ingress : - from : - podSelector : matchLabels : allow-access : \"true\"","title":"Solution"},{"location":"developer-foundation/activities/labs/solutions/","text":"Solutions \u00b6 apiVersion: v1 kind: Pod metadata: name: nginx namespace: web spec: containers: - name: nginx image: nginx command: [\"nginx\"] args: [\"-g\", \"daemon off;\", \"-q\"] ports: - containerPort: 80 apiVersion: v1 kind: ConfigMap metadata: name: yoda-service-config data: yoda.cfg: |- yoda.baby.power=100000000 yoda.strength=10 apiVersion: v1 kind: Secret metadata: name: yoda-db-password stringData: password: 0penSh1ftRul3s! apiVersion: v1 kind: Pod metadata: name: yoda-service spec: serviceAccountName: yoda-svc containers: - name: yoda-service image: bitnami/nginx volumeMounts: - name: config-volume mountPath: /etc/yoda-service env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: yoda-db-password key: password resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" volumes: - name: config-volume configMap: name: yoda-service-config apiVersion: v1 kind: ConfigMap metadata: name: vader-service-ambassador-config data: haproxy.cfg: |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8989 maxconn 32 apiVersion: v1 kind: Pod metadata: name: vader-service spec: containers: - name: millennium-falcon image: ibmcase/millennium-falcon:1 - name: haproxy-ambassador image: haproxy:1.7 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /usr/local/etc/haproxy volumes: - name: config-volume configMap: name: vader-service-ambassador-config apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: myapp-container image: radial/busyboxplus:curl command: ['sh', '-c', 'while true; do sleep 3600; done'] Check it with kubectl exec busybox -- curl $(kubectl get pod vader-service -o=jsonpath='{.status.podIP}'):80 apiVersion: v1 kind: Pod metadata: name: energy-shield-service spec: containers: - name: energy-shield image: ibmcase/energy-shield:1 livenessProbe: httpGet: path: /healthz port: 8080 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 Check STATUS column for not Ready kubectl get pods --all-namespaces Check the description of the deployment kubectl describe deployment hyper-drive Save logs for broken pod kubectl logs <pod name> -n <namespace> > /home/cloud_user/debug/broken-pod-logs.log In the deployment's description you will see the following is wrong: Selector and Label names do not match. The Probe is TCP instead of HTTP Get. The Service Port is 80 instead of 8080. To fix probe, can't kubectl edit, need to delete and recreate the deployment kubectl get deployment <deployment name> -n <namespace> -o yaml --export > hyper-drive.yml Delete pod kubectl delete deployment <deployment name> -n <namespace> Edit yaml, and apply kubectl apply -f hyper-drive.yml -n <namespace> Verify kubectl get deployment <deployment name> -n <namespace> Update the deployment to the new version like so: kubectl set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.1 --record Check the progress of the rolling update: kubectl rollout status deployment/jedi-deployment In another terminal window kubectl get pods -w Get a list of previous revisions. kubectl rollout history deployment/jedi-deployment Undo the last revision. kubectl rollout undo deployment/jedi-deployment Check the status of the rollout. kubectl rollout status deployment/jedi-deployment apiVersion: batch/v1beta1 kind: CronJob metadata: name: xwing-cronjob spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: xwing-status image: ibmcase/xwing-status:1.0 args: - /usr/sbin/xwing-status.sh restartPolicy: OnFailure kubectl get cronjob xwing-cronjob apiVersion: v1 kind: Service metadata: name: jedi-svc spec: type: NodePort selector: app: jedi ports: - protocol: TCP port: 80 targetPort: 8080 apiVersion: v1 kind: Service metadata: name: yoda-svc spec: type: ClusterIP selector: app: yoda ports: - protocol: TCP port: 80 targetPort: 8080 apiVersion: v1 kind: PersistentVolume metadata: name: postgresql-pv spec: storageClassName: localdisk capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgresql-pv-claim spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 500Mi apiVersion: v1 kind: Pod metadata: name: postgresql-pod spec: containers: - name: postgresql image: bitnami/postgresql ports: - containerPort: 5432 env: - name: MYSQL_ROOT_PASSWORD value: password volumeMounts: - name: sql-storage mountPath: /bitnami/postgresql/ volumes: - name: sql-storage persistentVolumeClaim: claimName: postgresql-pv-claim","title":"Kubernetes Lab Solutions"},{"location":"developer-foundation/activities/labs/solutions/#solutions","text":"apiVersion: v1 kind: Pod metadata: name: nginx namespace: web spec: containers: - name: nginx image: nginx command: [\"nginx\"] args: [\"-g\", \"daemon off;\", \"-q\"] ports: - containerPort: 80 apiVersion: v1 kind: ConfigMap metadata: name: yoda-service-config data: yoda.cfg: |- yoda.baby.power=100000000 yoda.strength=10 apiVersion: v1 kind: Secret metadata: name: yoda-db-password stringData: password: 0penSh1ftRul3s! apiVersion: v1 kind: Pod metadata: name: yoda-service spec: serviceAccountName: yoda-svc containers: - name: yoda-service image: bitnami/nginx volumeMounts: - name: config-volume mountPath: /etc/yoda-service env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: yoda-db-password key: password resources: requests: memory: \"64Mi\" cpu: \"250m\" limits: memory: \"128Mi\" cpu: \"500m\" volumes: - name: config-volume configMap: name: yoda-service-config apiVersion: v1 kind: ConfigMap metadata: name: vader-service-ambassador-config data: haproxy.cfg: |- global daemon maxconn 256 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms listen http-in bind *:80 server server1 127.0.0.1:8989 maxconn 32 apiVersion: v1 kind: Pod metadata: name: vader-service spec: containers: - name: millennium-falcon image: ibmcase/millennium-falcon:1 - name: haproxy-ambassador image: haproxy:1.7 ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /usr/local/etc/haproxy volumes: - name: config-volume configMap: name: vader-service-ambassador-config apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: myapp-container image: radial/busyboxplus:curl command: ['sh', '-c', 'while true; do sleep 3600; done'] Check it with kubectl exec busybox -- curl $(kubectl get pod vader-service -o=jsonpath='{.status.podIP}'):80 apiVersion: v1 kind: Pod metadata: name: energy-shield-service spec: containers: - name: energy-shield image: ibmcase/energy-shield:1 livenessProbe: httpGet: path: /healthz port: 8080 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 Check STATUS column for not Ready kubectl get pods --all-namespaces Check the description of the deployment kubectl describe deployment hyper-drive Save logs for broken pod kubectl logs <pod name> -n <namespace> > /home/cloud_user/debug/broken-pod-logs.log In the deployment's description you will see the following is wrong: Selector and Label names do not match. The Probe is TCP instead of HTTP Get. The Service Port is 80 instead of 8080. To fix probe, can't kubectl edit, need to delete and recreate the deployment kubectl get deployment <deployment name> -n <namespace> -o yaml --export > hyper-drive.yml Delete pod kubectl delete deployment <deployment name> -n <namespace> Edit yaml, and apply kubectl apply -f hyper-drive.yml -n <namespace> Verify kubectl get deployment <deployment name> -n <namespace> Update the deployment to the new version like so: kubectl set image deployment/jedi-deployment jedi-ws=bitnami/nginx:1.18.1 --record Check the progress of the rolling update: kubectl rollout status deployment/jedi-deployment In another terminal window kubectl get pods -w Get a list of previous revisions. kubectl rollout history deployment/jedi-deployment Undo the last revision. kubectl rollout undo deployment/jedi-deployment Check the status of the rollout. kubectl rollout status deployment/jedi-deployment apiVersion: batch/v1beta1 kind: CronJob metadata: name: xwing-cronjob spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: xwing-status image: ibmcase/xwing-status:1.0 args: - /usr/sbin/xwing-status.sh restartPolicy: OnFailure kubectl get cronjob xwing-cronjob apiVersion: v1 kind: Service metadata: name: jedi-svc spec: type: NodePort selector: app: jedi ports: - protocol: TCP port: 80 targetPort: 8080 apiVersion: v1 kind: Service metadata: name: yoda-svc spec: type: ClusterIP selector: app: yoda ports: - protocol: TCP port: 80 targetPort: 8080 apiVersion: v1 kind: PersistentVolume metadata: name: postgresql-pv spec: storageClassName: localdisk capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: \"/mnt/data\" apiVersion: v1 kind: PersistentVolumeClaim metadata: name: postgresql-pv-claim spec: storageClassName: localdisk accessModes: - ReadWriteOnce resources: requests: storage: 500Mi apiVersion: v1 kind: Pod metadata: name: postgresql-pod spec: containers: - name: postgresql image: bitnami/postgresql ports: - containerPort: 5432 env: - name: MYSQL_ROOT_PASSWORD value: password volumeMounts: - name: sql-storage mountPath: /bitnami/postgresql/ volumes: - name: sql-storage persistentVolumeClaim: claimName: postgresql-pv-claim","title":"Solutions"},{"location":"developer-foundation/checksetup/","text":"Congratulations on becoming part of the IBM Cloud-Native Learning Journey! You should have received a welcome email that provides details of the IBM Cloud Sandbox you've been assigned to and points to this welcome page. By participating in this Cloud-Native Learning Journey, you get access to: The Crafted Education Journey Agenda. A RedHat OpenShift managed service cluster with the IBM Garage Cloud-Native Toolkit and IBM Cloud Pak for Applications pre-installed. The development environment pre-configured in a pay-as-you-go IBM Cloud account which you will use to perform your learning tasks. Warning If you haven't received the welcome email, it might be due to you not having been given access to the IBM Cloud account yet. Please contact your Business Development Executive or your Lab Leader to validate your invite. Notification \u00b6 The welcome email gives you information on how to access the development environment for your team. IBM Cloud Sandbox Registration Details Account : this is the IBM Cloud account with a configured sandbox development environment that will enable you to complete the learning agenda. Team : this is the name of your development team. GitHub Organization : this is the GitHub organization that will be used for storing your code during your learning activities. Resource Group : this is the IBM Cloud resource group where the development cluster and cloud resources have been provisioned. Example of the key information in your email: Cloud : https://cloud.ibm.com/ Account : 1924691 - GSI Labs - IBM Location : London (eu-gb) Sandbox Team : Cloud-Native Squad GitHub Organization : https://github.com/gsi-enablement-one Resource Group : gsi-cloudnative-squad Sandbox \u00b6 It's intended for Developer Foundation Hands-on Labs and gives access to the following sandbox environments. IBM Kubernetes Containers . IBM Openshift Containers . Katacoda Interactive Shell . Openshift Interactive Shell . IBM Cloud Openshift Containers and Kubernetes Containers \u00b6 Note : If you have any issue with the following steps, please raise an issue on the #gsi-labs-external Slack Channel . All issues should be reported there. Note Follow these steps to check you can access your team's Developer Tools environment. Log in and view the resources: Log in to the IBM Cloud . The IBM Cloud Sandbox registration welcome email will include your account information, team, GitHub organization, and resource group. In the IBM Cloud console, switch to the account specified in the email. Navigate to the Resource List . In the Group filter, select your team's resource group. Click on Expand All (top right) to show all its resources. Under Cluster, you should see the cluster named workshop-ocp46-{team} where {team} is the name of your team. Some clusters may have a different number if there is more than one of that type. Explore the Red Hat OpenShift resources and set up the CLI: In the Tags filter, enter both ocp to see just the Red Hat OpenShift cluster and services. Click on each cloud service instance name and check that you can open each one's dashboard. Click on the Red Hat OpenShift cluster names to open their dashboards. Switch to the Access tab. Follow the instructions to install the CLI, then access the cluster from the CLI and validate that you are successfully connected. Press the OpenShift web console button and verify that you can open the Red Hat OpenShift console. Explore the IKS resources and set up the CLI: Return to the Resource List . Remove the ocp tags and add the iks tag. Repeat the same steps for your team's Kubernetes cluster and services. Success You have completed this task if you have: - Managed to access your IBM Cloud account. - Viewed your team's Red Hat OpenShift cluster. - Accessed the cluster from the command line. Katacoda \u00b6 Katacoda is an interactive learning and training platform. Katacoda provides browser-based hands-on labs without any downloads or configuration. Learn Kubernetes using interactive browser-based scenarios . Learn Openshift using interactive browser-based scenarios . OpenShift Interactive Learning Platform \u00b6 Openshift Interactive Learning Scenarios provides a pre-configured OpenShift instance, accessible from your browser without any downloads or configuration. Learn OpenShift using and interactive browser-based shell .","title":"Developer Foundation Sandbox"},{"location":"developer-foundation/checksetup/#notification","text":"The welcome email gives you information on how to access the development environment for your team. IBM Cloud Sandbox Registration Details Account : this is the IBM Cloud account with a configured sandbox development environment that will enable you to complete the learning agenda. Team : this is the name of your development team. GitHub Organization : this is the GitHub organization that will be used for storing your code during your learning activities. Resource Group : this is the IBM Cloud resource group where the development cluster and cloud resources have been provisioned. Example of the key information in your email: Cloud : https://cloud.ibm.com/ Account : 1924691 - GSI Labs - IBM Location : London (eu-gb) Sandbox Team : Cloud-Native Squad GitHub Organization : https://github.com/gsi-enablement-one Resource Group : gsi-cloudnative-squad","title":"Notification"},{"location":"developer-foundation/checksetup/#sandbox","text":"It's intended for Developer Foundation Hands-on Labs and gives access to the following sandbox environments. IBM Kubernetes Containers . IBM Openshift Containers . Katacoda Interactive Shell . Openshift Interactive Shell .","title":"Sandbox"},{"location":"developer-foundation/checksetup/#ibm-cloud-openshift-containers-and-kubernetes-containers","text":"Note : If you have any issue with the following steps, please raise an issue on the #gsi-labs-external Slack Channel . All issues should be reported there. Note Follow these steps to check you can access your team's Developer Tools environment. Log in and view the resources: Log in to the IBM Cloud . The IBM Cloud Sandbox registration welcome email will include your account information, team, GitHub organization, and resource group. In the IBM Cloud console, switch to the account specified in the email. Navigate to the Resource List . In the Group filter, select your team's resource group. Click on Expand All (top right) to show all its resources. Under Cluster, you should see the cluster named workshop-ocp46-{team} where {team} is the name of your team. Some clusters may have a different number if there is more than one of that type. Explore the Red Hat OpenShift resources and set up the CLI: In the Tags filter, enter both ocp to see just the Red Hat OpenShift cluster and services. Click on each cloud service instance name and check that you can open each one's dashboard. Click on the Red Hat OpenShift cluster names to open their dashboards. Switch to the Access tab. Follow the instructions to install the CLI, then access the cluster from the CLI and validate that you are successfully connected. Press the OpenShift web console button and verify that you can open the Red Hat OpenShift console. Explore the IKS resources and set up the CLI: Return to the Resource List . Remove the ocp tags and add the iks tag. Repeat the same steps for your team's Kubernetes cluster and services. Success You have completed this task if you have: - Managed to access your IBM Cloud account. - Viewed your team's Red Hat OpenShift cluster. - Accessed the cluster from the command line.","title":"IBM Cloud Openshift Containers and Kubernetes Containers"},{"location":"developer-foundation/checksetup/#katacoda","text":"Katacoda is an interactive learning and training platform. Katacoda provides browser-based hands-on labs without any downloads or configuration. Learn Kubernetes using interactive browser-based scenarios . Learn Openshift using interactive browser-based scenarios .","title":"Katacoda"},{"location":"developer-foundation/checksetup/#openshift-interactive-learning-platform","text":"Openshift Interactive Learning Scenarios provides a pre-configured OpenShift instance, accessible from your browser without any downloads or configuration. Learn OpenShift using and interactive browser-based shell .","title":"OpenShift Interactive Learning Platform"},{"location":"developer-foundation/cloud-native-app-dev/","text":"Introduction \u00b6 Cloud-Native is all about the concepts that are used for building and deploying your applications on any cloud platform. It involves many things like adopting microservices architecture, containerization, orchestration etc. Cloud-native applications will be managed by the infrastructure which in turn is managed by applications. In this installment, let us see what you need to include in your cloud-native applications and how to run them on the cloud infrastructure. Application Design \u00b6 An application is called Cloud-Native if it is designed in a way such that it takes advantage of most of the benefits of the cloud. So, these applications are all managed by software like mentioned before. Let's say we have an application packaged in a container and running on Kubernetes. This application does not accept any runtime configuration. There is no logging defined. Some of the configurations like database IP, credentials etc are hardcoded. What happens if the application stops working? How are you going to debug it? Will you call such an application cloud-native? Definitely not. Containers and Kubernetes help you to run the applications smoothly on cloud infrastructure. Along with this, you also need to know how to effectively build and manage these applications. Cloud-native Capabilities \u00b6 While developing cloud-native applications, some of the capabilities that we include in the applications are as follows. Configuration Health Checks Logging Metrics Resiliency Service discovery Usually, you can implement them in your applications in two ways. Using the language libraries Using sidecar To implement these capabilities, you can import the language libraries into your application which will automatically get you most of these capabilities with out defining any extra code. This is easy to do and is well suitable if you have few languages in your applications. But if your application is a polyglot with many different languages, it is difficult to manage all the libraries. In such cases, you can use sidecar which is implemented as separate service including things like Health end point monitoring health of the application, configuration watching changes in the configs and reloading the application when required, registered for service discovery, envoy proxy to handle resiliency and metrics etc. Application Lifecycle \u00b6 Deploy Deploying your cloud-native application is not just taking the existing code and running it on cloud. Cloud-native applications are defined in a way such the software manage them. For this, make sure you have the below embedded in your application. Continuous integration Continuous deployment Health checks Deployments for your application should be automated, tested and verified. If you are introducing new features to your applications, you should be able to deploy them dynamically without restarting your applications. Also, when you are planning on a new feature or a new version to be deployed, make sure you have traffic control mechanisms in place which allows you to route the traffic towards or away from the application as per your requirements to reduce the outage impact. Run Running your application is one of the most important phases in the application lifecycle. While running the application, two most important aspects to keep in mind are Observability Operability While running your application, you need to understand the what the application is doing which is observability and also you you should be able to change the application as needed which is operability. When your application is not meeting the service-level objectives (SLOs) or is broken, what do you do? In a cloud-native application, we follow the below steps to see where the problem resides. Verify infrastructure tests Application debugging - This can be done by using application performance monitoring (APM), distributed tracing etc. More verbose Logging In today's world, as the business keeps increasing, the application grows and you need to make sure that you defined a proper debugging strategy for your application which makes it easy to dynamically debug the applications similar to how we dynamically deploy them. One more important things to remember is that it is always easy to push new applications but the converse is not true. Though that is the case, it is still very important to retire the old applications that are not in use. Retire In cloud-native applications, all the new services are deployed automatically. Also, the services are monitored automatically using the monitoring mechanisms in place. Don't you think the services should be retired in the same way too? If you keep deploying new services without cleaning up the old ones which are in no longer use accrues a lot of technical debt. So, make sure your application includes a telemetry mechanism which helps you to identify if a service is being used. If not, the decision should be made by the business to keep it or retire it. Twelve factor design methodology \u00b6 Code base - One code base tracked in revision control, many deploys. Dependencies - Explicitly declare and isolate dependencies. Config - Store config in the environment. Backing services - Treat backing services as attached resources. Build, release, run - Strictly separate build and run stages. Processes - Execute the app as one (or more) stateless process(es). Port binding - Export services through port binding. Concurrency - Scale-out through the process model. Disposability - Maximize robustness with fast startup and graceful shutdown. Dev/prod parity - Keep development, staging, and production as similar as possible. Logs - Treat logs as event streams. Admin processes - Run admin/management tasks as one-off processes. Application Requirements \u00b6 Runtime and Isolation Your applications must be isolated from the operating system. You should be able to run them any where. This allows you to run multiple applications on same server and also allows to control their dependencies and resources. One way to achieve this is containerization. Among the different container options, Docker is popular. Container is nothing but a way to package your application and run it in an isolated environment. While developing the applications, also make sure all the dependencies are declared in your application before packaging it. Resource Allocation and Scheduling Your applications must include dynamic scheduling. This helps you to figure out where the application must run and this decisions are automatically taken for you by the scheduler. This scheduler collects all the information of resources for different system and chooses the right place to run the application. Operator can override the decisions of the scheduler if he wants to. Environment isolation You need a proper environment isolation to differentiate dev, test, stage, production etc. based on your requirements. With out the complete duplication of your cluster, the infrastructure should be able to separate the dependencies through different application environments. These environments should include all of the resources like databases, network resources etc. needed by the application. Cloud-native infrastructure can create environments with very low overhead. Service discovery In your application, there may be multiple services. These services may depend on one another. How will they find each other if one service needs to communicate with other? For this, the infrastructure should provide a way for services to find each other. This may be in different ways. It can be using API calls or using DNS or with network proxies. There should be a service discovery mechanism in place and how you do this does not matter. Usually cloud-native applications make use their infrastructure for service discovery to identify the dependent services. Some of them are cloud metadata services, DNS, etcd and consul etc. State Management While defining your cloud-native application, you should provide a mechanism to check the status of the application. This can be done by an API or hook that checks the current state of the application like if it is submitted, Scheduled, ready, healthy, unhealthy, terminating etc. We usually have such capabilities in any of the orchestration platform we use. For example, if you consider Kubernetes, you can do this using events, probes and hooks. When the application is submitted, scheduled, or scaled, the event is triggered. Readiness probe checks if the application is ready and liveness probes checks if the application is healthy. Hooks are used for events that need to happen before or after processes start. Monitoring and logging Monitoring and logging should be a part of the cloud-native application. Dynamically monitoring all the services of the application is important. It keeps checking the entire application and is used for debugging purposes when required. Also, make sure your logging system should be able to collect all the logs and consolidate them together based on application, environments, tags etc. Metrics Cloud-native applications must include metrics as a part of their code. All the telemetry data needed will be provided by the metrics. This helps you to know whether your application is meeting the service-level objectives. Metrics are collected at instance level and later aggregated together to provide the complete view of the application. Once the application provides metrics, underlying infrastructure will scrape them out and use them for analysis. Debugging and tracing When an application is deployed and problem occurs, we refer to logging system. But if that does not resolve the issue, we need distributed tracing. Distributed tracing helps us to understand what is happening in the application. They will us to debug problems by providing us an interface to visualize which is different from the details we get from logging. Also, it provides shorter feedback loops which helps you to debug distributed systems easily. Application tracing is always important and make sure it is a part of your cloud-native application. If in case you cannot include it in the application, you can also enable it at infrastructure level using proxies or traffic analysis. Activities \u00b6 Conclusion \u00b6 We discussed the cloud-native application design, implementations of cloud-native patterns, and application life cycle. We also saw how we can design our cloud-native applications using the twelve factor methodology. Along with this, we also explored what we need to include in our cloud naive application while building it. References \u00b6 Managing Cloud-Native applications by Justin Garrison, Kris Nova. Publisher: O'Reilly Media, Inc. (2018) Cloud-Native Architectures by Piyum Zonooz, Erik Farr, Kamal Arora, Tom Laszewski. Publisher: Packt Publishing (2018) The Twelve-Factor App","title":"Cloud-Native App Dev"},{"location":"developer-foundation/cloud-native-app-dev/#introduction","text":"Cloud-Native is all about the concepts that are used for building and deploying your applications on any cloud platform. It involves many things like adopting microservices architecture, containerization, orchestration etc. Cloud-native applications will be managed by the infrastructure which in turn is managed by applications. In this installment, let us see what you need to include in your cloud-native applications and how to run them on the cloud infrastructure.","title":"Introduction"},{"location":"developer-foundation/cloud-native-app-dev/#application-design","text":"An application is called Cloud-Native if it is designed in a way such that it takes advantage of most of the benefits of the cloud. So, these applications are all managed by software like mentioned before. Let's say we have an application packaged in a container and running on Kubernetes. This application does not accept any runtime configuration. There is no logging defined. Some of the configurations like database IP, credentials etc are hardcoded. What happens if the application stops working? How are you going to debug it? Will you call such an application cloud-native? Definitely not. Containers and Kubernetes help you to run the applications smoothly on cloud infrastructure. Along with this, you also need to know how to effectively build and manage these applications.","title":"Application Design"},{"location":"developer-foundation/cloud-native-app-dev/#cloud-native-capabilities","text":"While developing cloud-native applications, some of the capabilities that we include in the applications are as follows. Configuration Health Checks Logging Metrics Resiliency Service discovery Usually, you can implement them in your applications in two ways. Using the language libraries Using sidecar To implement these capabilities, you can import the language libraries into your application which will automatically get you most of these capabilities with out defining any extra code. This is easy to do and is well suitable if you have few languages in your applications. But if your application is a polyglot with many different languages, it is difficult to manage all the libraries. In such cases, you can use sidecar which is implemented as separate service including things like Health end point monitoring health of the application, configuration watching changes in the configs and reloading the application when required, registered for service discovery, envoy proxy to handle resiliency and metrics etc.","title":"Cloud-native Capabilities"},{"location":"developer-foundation/cloud-native-app-dev/#application-lifecycle","text":"Deploy Deploying your cloud-native application is not just taking the existing code and running it on cloud. Cloud-native applications are defined in a way such the software manage them. For this, make sure you have the below embedded in your application. Continuous integration Continuous deployment Health checks Deployments for your application should be automated, tested and verified. If you are introducing new features to your applications, you should be able to deploy them dynamically without restarting your applications. Also, when you are planning on a new feature or a new version to be deployed, make sure you have traffic control mechanisms in place which allows you to route the traffic towards or away from the application as per your requirements to reduce the outage impact. Run Running your application is one of the most important phases in the application lifecycle. While running the application, two most important aspects to keep in mind are Observability Operability While running your application, you need to understand the what the application is doing which is observability and also you you should be able to change the application as needed which is operability. When your application is not meeting the service-level objectives (SLOs) or is broken, what do you do? In a cloud-native application, we follow the below steps to see where the problem resides. Verify infrastructure tests Application debugging - This can be done by using application performance monitoring (APM), distributed tracing etc. More verbose Logging In today's world, as the business keeps increasing, the application grows and you need to make sure that you defined a proper debugging strategy for your application which makes it easy to dynamically debug the applications similar to how we dynamically deploy them. One more important things to remember is that it is always easy to push new applications but the converse is not true. Though that is the case, it is still very important to retire the old applications that are not in use. Retire In cloud-native applications, all the new services are deployed automatically. Also, the services are monitored automatically using the monitoring mechanisms in place. Don't you think the services should be retired in the same way too? If you keep deploying new services without cleaning up the old ones which are in no longer use accrues a lot of technical debt. So, make sure your application includes a telemetry mechanism which helps you to identify if a service is being used. If not, the decision should be made by the business to keep it or retire it.","title":"Application Lifecycle"},{"location":"developer-foundation/cloud-native-app-dev/#twelve-factor-design-methodology","text":"Code base - One code base tracked in revision control, many deploys. Dependencies - Explicitly declare and isolate dependencies. Config - Store config in the environment. Backing services - Treat backing services as attached resources. Build, release, run - Strictly separate build and run stages. Processes - Execute the app as one (or more) stateless process(es). Port binding - Export services through port binding. Concurrency - Scale-out through the process model. Disposability - Maximize robustness with fast startup and graceful shutdown. Dev/prod parity - Keep development, staging, and production as similar as possible. Logs - Treat logs as event streams. Admin processes - Run admin/management tasks as one-off processes.","title":"Twelve factor design methodology"},{"location":"developer-foundation/cloud-native-app-dev/#application-requirements","text":"Runtime and Isolation Your applications must be isolated from the operating system. You should be able to run them any where. This allows you to run multiple applications on same server and also allows to control their dependencies and resources. One way to achieve this is containerization. Among the different container options, Docker is popular. Container is nothing but a way to package your application and run it in an isolated environment. While developing the applications, also make sure all the dependencies are declared in your application before packaging it. Resource Allocation and Scheduling Your applications must include dynamic scheduling. This helps you to figure out where the application must run and this decisions are automatically taken for you by the scheduler. This scheduler collects all the information of resources for different system and chooses the right place to run the application. Operator can override the decisions of the scheduler if he wants to. Environment isolation You need a proper environment isolation to differentiate dev, test, stage, production etc. based on your requirements. With out the complete duplication of your cluster, the infrastructure should be able to separate the dependencies through different application environments. These environments should include all of the resources like databases, network resources etc. needed by the application. Cloud-native infrastructure can create environments with very low overhead. Service discovery In your application, there may be multiple services. These services may depend on one another. How will they find each other if one service needs to communicate with other? For this, the infrastructure should provide a way for services to find each other. This may be in different ways. It can be using API calls or using DNS or with network proxies. There should be a service discovery mechanism in place and how you do this does not matter. Usually cloud-native applications make use their infrastructure for service discovery to identify the dependent services. Some of them are cloud metadata services, DNS, etcd and consul etc. State Management While defining your cloud-native application, you should provide a mechanism to check the status of the application. This can be done by an API or hook that checks the current state of the application like if it is submitted, Scheduled, ready, healthy, unhealthy, terminating etc. We usually have such capabilities in any of the orchestration platform we use. For example, if you consider Kubernetes, you can do this using events, probes and hooks. When the application is submitted, scheduled, or scaled, the event is triggered. Readiness probe checks if the application is ready and liveness probes checks if the application is healthy. Hooks are used for events that need to happen before or after processes start. Monitoring and logging Monitoring and logging should be a part of the cloud-native application. Dynamically monitoring all the services of the application is important. It keeps checking the entire application and is used for debugging purposes when required. Also, make sure your logging system should be able to collect all the logs and consolidate them together based on application, environments, tags etc. Metrics Cloud-native applications must include metrics as a part of their code. All the telemetry data needed will be provided by the metrics. This helps you to know whether your application is meeting the service-level objectives. Metrics are collected at instance level and later aggregated together to provide the complete view of the application. Once the application provides metrics, underlying infrastructure will scrape them out and use them for analysis. Debugging and tracing When an application is deployed and problem occurs, we refer to logging system. But if that does not resolve the issue, we need distributed tracing. Distributed tracing helps us to understand what is happening in the application. They will us to debug problems by providing us an interface to visualize which is different from the details we get from logging. Also, it provides shorter feedback loops which helps you to debug distributed systems easily. Application tracing is always important and make sure it is a part of your cloud-native application. If in case you cannot include it in the application, you can also enable it at infrastructure level using proxies or traffic analysis.","title":"Application Requirements"},{"location":"developer-foundation/cloud-native-app-dev/#activities","text":"","title":"Activities"},{"location":"developer-foundation/cloud-native-app-dev/#conclusion","text":"We discussed the cloud-native application design, implementations of cloud-native patterns, and application life cycle. We also saw how we can design our cloud-native applications using the twelve factor methodology. Along with this, we also explored what we need to include in our cloud naive application while building it.","title":"Conclusion"},{"location":"developer-foundation/cloud-native-app-dev/#references","text":"Managing Cloud-Native applications by Justin Garrison, Kris Nova. Publisher: O'Reilly Media, Inc. (2018) Cloud-Native Architectures by Piyum Zonooz, Erik Farr, Kamal Arora, Tom Laszewski. Publisher: Packt Publishing (2018) The Twelve-Factor App","title":"References"},{"location":"developer-foundation/cloud-native-overview/","text":"Introduction \u00b6 Cloud is everywhere. Today, many companies want to migrate their applications to the cloud. For this to be done, the applications must be re-architected to fully use the cloud advantages. What is Cloud-Native? \u00b6 Cloud-native is about how we build and run applications taking full advantage of cloud computing rather than worrying about where we deploy it: A cloud-native application consists of discrete and reusable components known as microservices that are designed to integrate into any cloud environment. These microservices act as building blocks and are often packaged in containers. Microservices work together as a whole to comprise an application, yet each can be independently scaled, continuously improved, and quickly iterated through automation and orchestration processes. The flexibility of each microservice adds to the agility and continuous improvement of cloud-native applications. CNCF Cloud-Native Definition Cloud-native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach. These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil. The Cloud-Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone. Why Cloud-Native? \u00b6 Cloud-native applications are different from the traditional applications that run in your data centers. Traditional applications are not built with cloud compatibility in mind. They may have strong ties with the internal systems and they cannot take advantage of all the benefits of the cloud. We need a new architecture for our applications to utilize the benefits of the cloud. The applications' design needs to keep the cloud in mind and take advantage of cloud services like storage, queuing, caching, etc. Speed, safety, and scalability come with cloud-native applications. Helps to quickly deliver the advancements. Allows loose ties into the corporate IT where it can destabilize legacy architectures. Allows continuous deployment of applications with zero downtime. Infrastructure is less predictable. Service instances are all disposable. Deployments are immutable. To meet the expectations of today\u2019s world customers, these systems are architected for elastic scalability. Cloud-native concepts \u00b6 Some of the important characteristics of cloud-native applications are as follows. Disposable infrastructure. Isolation. Scalability. Disposable architecture. Value-added cloud services. Polyglot cloud. Self-sufficient, full-stack teams. Cultural Change. Disposable Infrastructure \u00b6 While creating applications on the cloud, you need several cloud resources as part of it. We often hear how easy it is to create all these resources. But did you ever think about how easy is to dispose of them? It's not easy and that's why you don\u2019t hear much about it. In traditional or legacy applications, we have all these resources residing on machines. If these go down, we need to redo them again and most of this is handled by the operations team manually. So, when we are creating applications on the cloud, we bring those resources like load balancers, databases, and gateways as well along with machine images and containers. While creating these applications, you should always keep in mind that if you are creating a resource on demand, you should also be able to destroy it when not required. Without this, we cannot achieve the factors speed, safety, and scalability. If you want this to happen, we need automation. Automation allows you to: Deliver new features at any time. Deliver patches faster. Improve system quality. Facilitate team scale and efficiency. Now you should know what we are talking about. Disposable infrastructure is nothing but Infrastructure as Code . Infrastructure as Code \u00b6 Here, you develop the code for automation exactly as same as you do for the rest of the application using agile methodologies. Automation code is driven by a story. Versioned in the same repository as the rest of the code. Continuously tested as part of the CI/CD pipeline. Test environments are created and destroyed along with test runs. Thus, disposable infrastructure lays the groundwork for scalability and elasticity. Isolation \u00b6 In traditional or legacy monolithic applications when you fix a bug or an error, the entire application needs to be redeployed. This can cause side effects that you can never predict. Changes may break any components in the application as they are all inter related. In cloud-native applications, to avoid the above scenario, the system is decomposed into bounded isolated components. Each service will be defined as one component independent of others. This way, when there is a bug or error in the application, you can just fix the specific component avoiding any side effects as the components are all unrelated pieces of code. Thus, cloud-native systems are resilient to human-made errors. To achieve this we need isolation to avoid a problem in a single component affecting the entire system. This also helps you to introduce changes the application quickly and with confidence. Scalability \u00b6 Simply deploying your application to the cloud does not make it cloud-native. To be cloud-native it needs to use the full benefits of the cloud. One key feature is scalability. In today\u2019s world, once your business starts growing, the number of users keeps increasing and they may be from different locations. Your application should be able to support a large number of devices while maintaining its responsiveness using an efficient and cost-effective way. To achieve this, a cloud-native application runs in multiple runtimes spread across multiple hosts. The applications should be designed and architected so it supports multiple regions and active-active deployments. This helps you to increase the availability and avoids single-point failures. Disposable architecture \u00b6 Leveraging the disposable infrastructure and scaling isolated components is important for cloud-native applications. Disposable architecture is based on this and it takes disposability and replacement concepts to the next level. Most of us think in a monolithic way because we are used to traditional or legacy applications. This may lead us to make decisions in a monolithic way rather than in a cloud-native way. With monolithic thinking, we tend to be safe and don\u2019t do a lot of experimentation. But disposable architecture is exactly the opposite. In this approach, we develop small pieces of the component and keep experimenting with it to find an optimal solution. When there is a breakthrough in the application, you can\u2019t simply make decisions based on the available information which may be incomplete or inaccurate. So, with disposable architecture, you start with small increments, and invest time to find the optimal solution. Sometimes, there may be a need to completely replace the component, but that initial work was just the cost of getting the information that caused the breakthrough. This helps you to minimize waste allowing you to use your resources on controlled experiments efficiently and get good value out of it in the end. Value-added cloud services \u00b6 When you are defining an application, there are many things you need to care about. Every service will be associated with many things like databases, storage, redundancy, monitoring, etc. For your application, along with your components, you also need to scale the data. You can reduce the operational risk and also get all such things at greater velocity by leveraging the value-added services that are available on the cloud. Sometimes, you may need third party services if they are not available on your cloud. You can externally hook them up with your application as needed. By using the value-added services provided by your cloud provider, you will get to know all the available options on your cloud and you can also learn about all the new services. This will help you to take good long-termed decisions. You can use a different service if you find it more suitable for your component and hook that up with your application based on the requirements. Polyglot cloud \u00b6 Most of you are familiar with polyglot programming. For your application, based on the component, you can choose the programming languages that best suits it. You need not stick to a single programming language for the entire application. If you consider polyglot persistence, you can choose the storage mechanism that better suits in a component by component basis. It allows a better global scale. Similarly, the next thing will be a polyglot cloud. Like above, here you choose a cloud provider that better suits your application in a component by component basis. For the majority of your components, you may have to go to your cloud provider. But, this does not stop you from choosing a different one if it suits well for any of your application components. So, you can run different components of your cloud-native system on different cloud providers based on your requirements. Self-sufficient, full-stack teams \u00b6 In a traditional setup, many organizations have teams based on skill sets like backend, user interface, database, operations, etc. Such a structure will not allow you to build cloud-native systems. In cloud-native systems, the system is composed of bounded isolated components that have their own resources. Each one of such components must be owned by a self-sufficient, full-stack team entirely responsible for all the resources that belong to that particular component. In this setup, this team tends to focus on quality upfront as they are the ones who deploy it and they will be taking care of it if the component is broken. It is more like you build it and then you run it. So, the team can continuously deliver advancements to the components at their own pace. Also, they are completely responsible for delivering these safely. Cultural Change \u00b6 Cloud-native is a different way of thinking. We need to first make up our minds, not just the systems, to utilize the full benefits of the cloud. Compared to the traditional systems, there will be lots of things we do differently in cloud-native systems. To make that happen, cultural change is really important. To change the thinking at a high level, we just to first prove that the low-level practices can truly deliver and encourage lean thinking. With these practices, you can conduct experimentation. Based on the feedback from business, you can quickly and safely deliver your applications that can scale. Cloud-native Roadmap \u00b6 You can define your cloud-native roadmap in many ways. You can get there by choosing different paths. Let us see the trail map defined by CNCF. CNCF defined the Cloud-Native Trail Map providing an overview for enterprises starting their cloud-native journey as follows. This cloud map gives us various steps that an engineering team may use while considering the cloud-native technologies and exploring them. The most common ones among them are containerization, CI/CD, and orchestration. The next crucial pieces will be observability & analysis and service mesh. And later comes the rest of them like networking, distributed database, messaging, container runtime, and software distribution based on your requirements. You cannot build cloud-native applications without containerization. This helps your application to run in any computing environment. Basically, all your code and dependencies are packaged up together into a single unit here. Among the different container platforms available, Docker is the preferred one. It's convenient to set up a CI/CD pipeline to bring all the changes in the code to the container automatically. There are many tools available for this like Jenkins, Travis, etc. We need container orchestration to manage its lifecycle. Kubernetes is a popular solution. Monitoring and observability play a very important role so it's recommended to use techniques like logging, tracing, metrics etc. To enable more complex operational requirements, you can use a service mesh. It helps you out with several things like service discovery, health, routing, A/B testing, etc. Istio is a popular tool. Networking plays a crucial role. You should define flexible networking layers based on your requirements. For this, you can use Calico, Weave Net, etc. Sometimes, you may need distributed databases. These are required if you need more scalability and resiliency. Messaging may be required sometimes too. You can use queues like Kafka, RabbitMQ, etc. Container registry helps you to store all your containers. You can also enable image scanning and signing if required. As a part of your application, sometimes you may need a secure software distribution. Also, if you want to see the cloud-native landscape, check it out here . Summary \u00b6 In this section, we covered the fundamentals of cloud-native systems. Now you should know what cloud-native is, why we need it, and how important it is. Cloud-native is not just deploying your application on the cloud, it's also about taking full advantage of the cloud. Also, with the cloud-native roadmap, you will get an idea about how to design and architect your cloud-native system. You can also get an overview of the different tools, frameworks, platforms, etc. with the cloud-native landscape. You can check Cloud Native Applications if want to learn more. References \u00b6 Cloud Native Applications John Gilbert, (2018). Cloud-Native Development Patterns and Best Practices. Publisher: Packt Publishing CNCF landscape CNCF Definition","title":"What is Cloud-Native?"},{"location":"developer-foundation/cloud-native-overview/#introduction","text":"Cloud is everywhere. Today, many companies want to migrate their applications to the cloud. For this to be done, the applications must be re-architected to fully use the cloud advantages.","title":"Introduction"},{"location":"developer-foundation/cloud-native-overview/#what-is-cloud-native","text":"Cloud-native is about how we build and run applications taking full advantage of cloud computing rather than worrying about where we deploy it: A cloud-native application consists of discrete and reusable components known as microservices that are designed to integrate into any cloud environment. These microservices act as building blocks and are often packaged in containers. Microservices work together as a whole to comprise an application, yet each can be independently scaled, continuously improved, and quickly iterated through automation and orchestration processes. The flexibility of each microservice adds to the agility and continuous improvement of cloud-native applications. CNCF Cloud-Native Definition Cloud-native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach. These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil. The Cloud-Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.","title":"What is Cloud-Native?"},{"location":"developer-foundation/cloud-native-overview/#why-cloud-native","text":"Cloud-native applications are different from the traditional applications that run in your data centers. Traditional applications are not built with cloud compatibility in mind. They may have strong ties with the internal systems and they cannot take advantage of all the benefits of the cloud. We need a new architecture for our applications to utilize the benefits of the cloud. The applications' design needs to keep the cloud in mind and take advantage of cloud services like storage, queuing, caching, etc. Speed, safety, and scalability come with cloud-native applications. Helps to quickly deliver the advancements. Allows loose ties into the corporate IT where it can destabilize legacy architectures. Allows continuous deployment of applications with zero downtime. Infrastructure is less predictable. Service instances are all disposable. Deployments are immutable. To meet the expectations of today\u2019s world customers, these systems are architected for elastic scalability.","title":"Why Cloud-Native?"},{"location":"developer-foundation/cloud-native-overview/#cloud-native-concepts","text":"Some of the important characteristics of cloud-native applications are as follows. Disposable infrastructure. Isolation. Scalability. Disposable architecture. Value-added cloud services. Polyglot cloud. Self-sufficient, full-stack teams. Cultural Change.","title":"Cloud-native concepts"},{"location":"developer-foundation/cloud-native-overview/#disposable-infrastructure","text":"While creating applications on the cloud, you need several cloud resources as part of it. We often hear how easy it is to create all these resources. But did you ever think about how easy is to dispose of them? It's not easy and that's why you don\u2019t hear much about it. In traditional or legacy applications, we have all these resources residing on machines. If these go down, we need to redo them again and most of this is handled by the operations team manually. So, when we are creating applications on the cloud, we bring those resources like load balancers, databases, and gateways as well along with machine images and containers. While creating these applications, you should always keep in mind that if you are creating a resource on demand, you should also be able to destroy it when not required. Without this, we cannot achieve the factors speed, safety, and scalability. If you want this to happen, we need automation. Automation allows you to: Deliver new features at any time. Deliver patches faster. Improve system quality. Facilitate team scale and efficiency. Now you should know what we are talking about. Disposable infrastructure is nothing but Infrastructure as Code .","title":"Disposable Infrastructure"},{"location":"developer-foundation/cloud-native-overview/#infrastructure-as-code","text":"Here, you develop the code for automation exactly as same as you do for the rest of the application using agile methodologies. Automation code is driven by a story. Versioned in the same repository as the rest of the code. Continuously tested as part of the CI/CD pipeline. Test environments are created and destroyed along with test runs. Thus, disposable infrastructure lays the groundwork for scalability and elasticity.","title":"Infrastructure as Code"},{"location":"developer-foundation/cloud-native-overview/#isolation","text":"In traditional or legacy monolithic applications when you fix a bug or an error, the entire application needs to be redeployed. This can cause side effects that you can never predict. Changes may break any components in the application as they are all inter related. In cloud-native applications, to avoid the above scenario, the system is decomposed into bounded isolated components. Each service will be defined as one component independent of others. This way, when there is a bug or error in the application, you can just fix the specific component avoiding any side effects as the components are all unrelated pieces of code. Thus, cloud-native systems are resilient to human-made errors. To achieve this we need isolation to avoid a problem in a single component affecting the entire system. This also helps you to introduce changes the application quickly and with confidence.","title":"Isolation"},{"location":"developer-foundation/cloud-native-overview/#scalability","text":"Simply deploying your application to the cloud does not make it cloud-native. To be cloud-native it needs to use the full benefits of the cloud. One key feature is scalability. In today\u2019s world, once your business starts growing, the number of users keeps increasing and they may be from different locations. Your application should be able to support a large number of devices while maintaining its responsiveness using an efficient and cost-effective way. To achieve this, a cloud-native application runs in multiple runtimes spread across multiple hosts. The applications should be designed and architected so it supports multiple regions and active-active deployments. This helps you to increase the availability and avoids single-point failures.","title":"Scalability"},{"location":"developer-foundation/cloud-native-overview/#disposable-architecture","text":"Leveraging the disposable infrastructure and scaling isolated components is important for cloud-native applications. Disposable architecture is based on this and it takes disposability and replacement concepts to the next level. Most of us think in a monolithic way because we are used to traditional or legacy applications. This may lead us to make decisions in a monolithic way rather than in a cloud-native way. With monolithic thinking, we tend to be safe and don\u2019t do a lot of experimentation. But disposable architecture is exactly the opposite. In this approach, we develop small pieces of the component and keep experimenting with it to find an optimal solution. When there is a breakthrough in the application, you can\u2019t simply make decisions based on the available information which may be incomplete or inaccurate. So, with disposable architecture, you start with small increments, and invest time to find the optimal solution. Sometimes, there may be a need to completely replace the component, but that initial work was just the cost of getting the information that caused the breakthrough. This helps you to minimize waste allowing you to use your resources on controlled experiments efficiently and get good value out of it in the end.","title":"Disposable architecture"},{"location":"developer-foundation/cloud-native-overview/#value-added-cloud-services","text":"When you are defining an application, there are many things you need to care about. Every service will be associated with many things like databases, storage, redundancy, monitoring, etc. For your application, along with your components, you also need to scale the data. You can reduce the operational risk and also get all such things at greater velocity by leveraging the value-added services that are available on the cloud. Sometimes, you may need third party services if they are not available on your cloud. You can externally hook them up with your application as needed. By using the value-added services provided by your cloud provider, you will get to know all the available options on your cloud and you can also learn about all the new services. This will help you to take good long-termed decisions. You can use a different service if you find it more suitable for your component and hook that up with your application based on the requirements.","title":"Value-added cloud services"},{"location":"developer-foundation/cloud-native-overview/#polyglot-cloud","text":"Most of you are familiar with polyglot programming. For your application, based on the component, you can choose the programming languages that best suits it. You need not stick to a single programming language for the entire application. If you consider polyglot persistence, you can choose the storage mechanism that better suits in a component by component basis. It allows a better global scale. Similarly, the next thing will be a polyglot cloud. Like above, here you choose a cloud provider that better suits your application in a component by component basis. For the majority of your components, you may have to go to your cloud provider. But, this does not stop you from choosing a different one if it suits well for any of your application components. So, you can run different components of your cloud-native system on different cloud providers based on your requirements.","title":"Polyglot cloud"},{"location":"developer-foundation/cloud-native-overview/#self-sufficient-full-stack-teams","text":"In a traditional setup, many organizations have teams based on skill sets like backend, user interface, database, operations, etc. Such a structure will not allow you to build cloud-native systems. In cloud-native systems, the system is composed of bounded isolated components that have their own resources. Each one of such components must be owned by a self-sufficient, full-stack team entirely responsible for all the resources that belong to that particular component. In this setup, this team tends to focus on quality upfront as they are the ones who deploy it and they will be taking care of it if the component is broken. It is more like you build it and then you run it. So, the team can continuously deliver advancements to the components at their own pace. Also, they are completely responsible for delivering these safely.","title":"Self-sufficient, full-stack teams"},{"location":"developer-foundation/cloud-native-overview/#cultural-change","text":"Cloud-native is a different way of thinking. We need to first make up our minds, not just the systems, to utilize the full benefits of the cloud. Compared to the traditional systems, there will be lots of things we do differently in cloud-native systems. To make that happen, cultural change is really important. To change the thinking at a high level, we just to first prove that the low-level practices can truly deliver and encourage lean thinking. With these practices, you can conduct experimentation. Based on the feedback from business, you can quickly and safely deliver your applications that can scale.","title":"Cultural Change"},{"location":"developer-foundation/cloud-native-overview/#cloud-native-roadmap","text":"You can define your cloud-native roadmap in many ways. You can get there by choosing different paths. Let us see the trail map defined by CNCF. CNCF defined the Cloud-Native Trail Map providing an overview for enterprises starting their cloud-native journey as follows. This cloud map gives us various steps that an engineering team may use while considering the cloud-native technologies and exploring them. The most common ones among them are containerization, CI/CD, and orchestration. The next crucial pieces will be observability & analysis and service mesh. And later comes the rest of them like networking, distributed database, messaging, container runtime, and software distribution based on your requirements. You cannot build cloud-native applications without containerization. This helps your application to run in any computing environment. Basically, all your code and dependencies are packaged up together into a single unit here. Among the different container platforms available, Docker is the preferred one. It's convenient to set up a CI/CD pipeline to bring all the changes in the code to the container automatically. There are many tools available for this like Jenkins, Travis, etc. We need container orchestration to manage its lifecycle. Kubernetes is a popular solution. Monitoring and observability play a very important role so it's recommended to use techniques like logging, tracing, metrics etc. To enable more complex operational requirements, you can use a service mesh. It helps you out with several things like service discovery, health, routing, A/B testing, etc. Istio is a popular tool. Networking plays a crucial role. You should define flexible networking layers based on your requirements. For this, you can use Calico, Weave Net, etc. Sometimes, you may need distributed databases. These are required if you need more scalability and resiliency. Messaging may be required sometimes too. You can use queues like Kafka, RabbitMQ, etc. Container registry helps you to store all your containers. You can also enable image scanning and signing if required. As a part of your application, sometimes you may need a secure software distribution. Also, if you want to see the cloud-native landscape, check it out here .","title":"Cloud-native Roadmap"},{"location":"developer-foundation/cloud-native-overview/#summary","text":"In this section, we covered the fundamentals of cloud-native systems. Now you should know what cloud-native is, why we need it, and how important it is. Cloud-native is not just deploying your application on the cloud, it's also about taking full advantage of the cloud. Also, with the cloud-native roadmap, you will get an idea about how to design and architect your cloud-native system. You can also get an overview of the different tools, frameworks, platforms, etc. with the cloud-native landscape. You can check Cloud Native Applications if want to learn more.","title":"Summary"},{"location":"developer-foundation/cloud-native-overview/#references","text":"Cloud Native Applications John Gilbert, (2018). Cloud-Native Development Patterns and Best Practices. Publisher: Packt Publishing CNCF landscape CNCF Definition","title":"References"},{"location":"developer-foundation/containers/","text":"Introduction \u00b6 You wanted to run your application on different computing environments. It may be your laptop, test environment, staging environment or production environment. So, when you run it on these different environments, will your application work reliably ? What if some underlying software changes ? What if the security policies are different ? or something else changes ? To solve this problems, we need Containers. Containers \u00b6 Containers are a standard way to package an application and all its dependencies so that it can be moved between environments and run without change. They work by hiding the differences between applications inside the container so that everything outside the container can be standardized. For example, Docker created standard way to create images for Linux Containers. Why containers ? \u00b6 We can run them anywhere. They are lightweight . Isolate your application from others. Different Container Standards \u00b6 There are many different container standards available today. Some of them are as follows. Docker - The most common standard, made Linux containers usable by the masses. Rocket (rkt) - An emerging container standard from CoreOS, the company that developed etcd. Garden - The format Cloud Foundry builds using buildpacks. Among them, Docker was one of the most popular mainstream container software tools. Open Container Initiative (OCI) A Linux Foundation project developing a governed container standard. Docker and Rocket are OCI-compliant. But, Garden is not. Benefits \u00b6 Lightweight Scalable Efficient Portable Supports agile development To know more about Containerization, we have couple of guides. Feel free to check them out. Containerization: A Complete Guide . Containers: A Complete Guide . Docker \u00b6 Docker is one of the most popular Containerization platforms which allows you to develop, deploy, and run application inside containers. It is an open source project. Can run it anywhere. An installation of Docker includes an engine. This comes with a daemon, REST APIs, and CLI. Users can use CLI to interact with the docker using commands. These commands are sent to the daemon which listens for the Docker Rest APIs which in turn manages images and containers. The engine runs a container by retrieving its image from the local system or registry. A running container starts one or more processes in the Linux kernel. Docker Image \u00b6 A read-only snapshot of a container that is stored in Docker Hub or in private repository. You use an image as a template for building containers. These images are build from the Dockerfile . Dockerfile It is a text document that contains all the instructions that are necessary to build a docker image. It is written in an easy-to-understand syntax. It specifies the operating system. It also includes things like environmental variables, ports, file locations etc. If you want to try building docker images, try this course on Katacoda (Interactive Learning Platform). Building Container Images - Estimated Time: 10 minutes. Docker Container \u00b6 The standard unit where the application service is located or transported. It packages up all code and its dependencies so that the application runs quickly and reliably from one computing environment to another. If you want to try deploying a docker container, try this course on Katacoda (Interactive Learning Platform). Deploying Your First Docker Container - Estimated Time: 10 minutes. Docker Engine \u00b6 Docker Engine is a program that creates, ships, and runs application containers. The engine runs on any physical or virtual machine or server locally, in private or public cloud. The client communicates with the engine to run commands. Docker Registry \u00b6 The registry stores, distributes, and shares container images. It is available in software as a service (SaaS) or in an enterprise to deploy anywhere you that you choose. Docker Hub is a popular registry. It is a registry which allows you to download docker images which are built by different communities. You can also store your own images there. You can check out various images available on docker hub here . Docker Commands \u00b6 Below are some commands we use often on Docker. # Know docker version docker -version # Run a container docker run <image> # List containers running docker ps # Stop a container docker stop <container-name|container-id> # Remove a container docker rm <container-name|container-id> # Login into registry docker login # Build an image docker build <image_name>:<tag> . # Pull the image docker pull <image_name>:<tag> # Push an image docker push <image_name>:<tag> # List images docker images If you are interested, check this course out on Containers and Docker. Activities \u00b6 Once you have completed these tasks, you should have a base understanding of containers and how to use Docker. References \u00b6 Docker resources Docker tutorial The Evolution of Linux Containers and Their Future rkt Cloud Foundry Garden container Open Container Initiative (OCI) Cloud-Native Computing Foundation (CNCF) Demystifying the Open Container Initiative (OCI) Specifications","title":"Containers Overview"},{"location":"developer-foundation/containers/#introduction","text":"You wanted to run your application on different computing environments. It may be your laptop, test environment, staging environment or production environment. So, when you run it on these different environments, will your application work reliably ? What if some underlying software changes ? What if the security policies are different ? or something else changes ? To solve this problems, we need Containers.","title":"Introduction"},{"location":"developer-foundation/containers/#containers","text":"Containers are a standard way to package an application and all its dependencies so that it can be moved between environments and run without change. They work by hiding the differences between applications inside the container so that everything outside the container can be standardized. For example, Docker created standard way to create images for Linux Containers.","title":"Containers"},{"location":"developer-foundation/containers/#why-containers","text":"We can run them anywhere. They are lightweight . Isolate your application from others.","title":"Why containers ?"},{"location":"developer-foundation/containers/#different-container-standards","text":"There are many different container standards available today. Some of them are as follows. Docker - The most common standard, made Linux containers usable by the masses. Rocket (rkt) - An emerging container standard from CoreOS, the company that developed etcd. Garden - The format Cloud Foundry builds using buildpacks. Among them, Docker was one of the most popular mainstream container software tools. Open Container Initiative (OCI) A Linux Foundation project developing a governed container standard. Docker and Rocket are OCI-compliant. But, Garden is not.","title":"Different Container Standards"},{"location":"developer-foundation/containers/#benefits","text":"Lightweight Scalable Efficient Portable Supports agile development To know more about Containerization, we have couple of guides. Feel free to check them out. Containerization: A Complete Guide . Containers: A Complete Guide .","title":"Benefits"},{"location":"developer-foundation/containers/#docker","text":"Docker is one of the most popular Containerization platforms which allows you to develop, deploy, and run application inside containers. It is an open source project. Can run it anywhere. An installation of Docker includes an engine. This comes with a daemon, REST APIs, and CLI. Users can use CLI to interact with the docker using commands. These commands are sent to the daemon which listens for the Docker Rest APIs which in turn manages images and containers. The engine runs a container by retrieving its image from the local system or registry. A running container starts one or more processes in the Linux kernel.","title":"Docker"},{"location":"developer-foundation/containers/#docker-image","text":"A read-only snapshot of a container that is stored in Docker Hub or in private repository. You use an image as a template for building containers. These images are build from the Dockerfile . Dockerfile It is a text document that contains all the instructions that are necessary to build a docker image. It is written in an easy-to-understand syntax. It specifies the operating system. It also includes things like environmental variables, ports, file locations etc. If you want to try building docker images, try this course on Katacoda (Interactive Learning Platform). Building Container Images - Estimated Time: 10 minutes.","title":"Docker Image"},{"location":"developer-foundation/containers/#docker-container","text":"The standard unit where the application service is located or transported. It packages up all code and its dependencies so that the application runs quickly and reliably from one computing environment to another. If you want to try deploying a docker container, try this course on Katacoda (Interactive Learning Platform). Deploying Your First Docker Container - Estimated Time: 10 minutes.","title":"Docker Container"},{"location":"developer-foundation/containers/#docker-engine","text":"Docker Engine is a program that creates, ships, and runs application containers. The engine runs on any physical or virtual machine or server locally, in private or public cloud. The client communicates with the engine to run commands.","title":"Docker Engine"},{"location":"developer-foundation/containers/#docker-registry","text":"The registry stores, distributes, and shares container images. It is available in software as a service (SaaS) or in an enterprise to deploy anywhere you that you choose. Docker Hub is a popular registry. It is a registry which allows you to download docker images which are built by different communities. You can also store your own images there. You can check out various images available on docker hub here .","title":"Docker Registry"},{"location":"developer-foundation/containers/#docker-commands","text":"Below are some commands we use often on Docker. # Know docker version docker -version # Run a container docker run <image> # List containers running docker ps # Stop a container docker stop <container-name|container-id> # Remove a container docker rm <container-name|container-id> # Login into registry docker login # Build an image docker build <image_name>:<tag> . # Pull the image docker pull <image_name>:<tag> # Push an image docker push <image_name>:<tag> # List images docker images If you are interested, check this course out on Containers and Docker.","title":"Docker Commands"},{"location":"developer-foundation/containers/#activities","text":"Once you have completed these tasks, you should have a base understanding of containers and how to use Docker.","title":"Activities"},{"location":"developer-foundation/containers/#references","text":"Docker resources Docker tutorial The Evolution of Linux Containers and Their Future rkt Cloud Foundry Garden container Open Container Initiative (OCI) Cloud-Native Computing Foundation (CNCF) Demystifying the Open Container Initiative (OCI) Specifications","title":"References"},{"location":"developer-foundation/garage-development/","text":"IBM Garage Method for Cloud \u00b6 (https://www.ibm.com/garage/method/cloud) \u00b6 The IBM Garage Method for Cloud is IBM's approach to enable business, development, and operations to continuously design, deliver, and validate new solutions leveraging cloud technologies. The practices and workflows provide prescriptive guidance for cloud adoption and transformation, and you can adapt them to your company's specific cloud journey and culture. The Method is industry-recognized and represents a unique, holistic IBM point-of-view and describes the complete cloud adoption and transformation lifecycle with enhancements that support the enterprise at scale. The result is a method that is proven, aligned with industry best practices, and scalable to any size engagement. Use the following links to help you deep dive in IBM Cloud Garage development best practices","title":"Garage Method"},{"location":"developer-foundation/garage-development/#ibm-garage-method-for-cloud","text":"","title":"IBM Garage Method for Cloud"},{"location":"developer-foundation/garage-development/#httpswwwibmcomgaragemethodcloud","text":"The IBM Garage Method for Cloud is IBM's approach to enable business, development, and operations to continuously design, deliver, and validate new solutions leveraging cloud technologies. The practices and workflows provide prescriptive guidance for cloud adoption and transformation, and you can adapt them to your company's specific cloud journey and culture. The Method is industry-recognized and represents a unique, holistic IBM point-of-view and describes the complete cloud adoption and transformation lifecycle with enhancements that support the enterprise at scale. The result is a method that is proven, aligned with industry best practices, and scalable to any size engagement. Use the following links to help you deep dive in IBM Cloud Garage development best practices","title":"(https://www.ibm.com/garage/method/cloud)"},{"location":"developer-foundation/gitops/","text":"What is GitOps? \u00b6 GitOps in short is a set of practices to use Git pull requests to manage infrastructure and application configurations. Git repository in GitOps is considered the only source of truth and contains the entire state of the system so that the trail of changes to the system state are visible and auditable. Traceability of changes in GitOps is no novelty in itself as this approach is almost universally employed for the application source code. However GitOps advocates applying the same principles (reviews, pull requests, tagging, etc) to infrastructure and application configuration so that teams can benefit from the same assurance as they do for the application source code. Although there is no precise definition or agreed upon set of rules, the following principles are an approximation of what constitutes a GitOps practice: Declarative description of the system is stored in Git (configs, monitoring, etc) Changes to the state are made via pull requests Git push reconciled with the state of the running system with the state in the Git repository CI/CD integration \u00b6 For the full end-to-end build and delivery process, both the CI and CD pipelines are required. For this to work, a Git repo is used to convey the configuration values. Within the , we have used certain naming conventions to streamline and simplify the integration between the various components. The naming components are: - app repo - The name of the Git repository for the application - git org - The name of the GitHub organization for the application's repo - tag - The build version - chart version - The version of the Helm chart - region - The geographic location in IBM Cloud The derived names are: - GitHub application path: github.com/{git org}/{app repo} - CI Pipeline name: {git org}.{app repo} - Docker image's path: {region}.icr.io/{git org}/{app repo}:{tag} in the Image Registry - Helm chart's path: generic-local/{git org}/{app repo}-{tag}-{chart version}.tgz in the Helm Repository - Dependencies filename: {app repo}/requirements.yaml in the GitOps repo - CD Pipeline name: {app repo} Resources \u00b6 Presentation: Understanding GitOps What is Argo CD \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a Kubernetes cluster or namespace, which also works for an OpenShift cluster or project. Argo CD models a collection of applications as a project and uses a Git repository to store the project's desired state. Each application is stored as a folder in the repository, and each deployment environment is stored as a branch in the repository. Argo CD supports defining Kubernetes manifests in a number of ways: - helm charts - kustomize - ksonnet - jsonnet - plain directory of yaml/json manifests Argo CD synchronizes the application state with the desired state defined in Git and automates the deployment of Kubernetes resources to ensure they match. Activities \u00b6 These activities give you a chance to walk through building CD pipelines using ArgoCD. These tasks assume that you have: - Reviewed the Continuous Deployment concept page. Task Description Link Time Walk-through GitOps Introduction to GitOps with OpenShift Learn OpenShift 20 min GitOps Multi-cluster Multi-cluster GitOps with OpenShift Learn OpenShift 20 min Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD 30 min Once you have completed these tasks, you will have created an ArgoCD deployment and have an understanding of Continuous Deployment.","title":"GitOps & ArgoCD Overview"},{"location":"developer-foundation/gitops/#what-is-gitops","text":"GitOps in short is a set of practices to use Git pull requests to manage infrastructure and application configurations. Git repository in GitOps is considered the only source of truth and contains the entire state of the system so that the trail of changes to the system state are visible and auditable. Traceability of changes in GitOps is no novelty in itself as this approach is almost universally employed for the application source code. However GitOps advocates applying the same principles (reviews, pull requests, tagging, etc) to infrastructure and application configuration so that teams can benefit from the same assurance as they do for the application source code. Although there is no precise definition or agreed upon set of rules, the following principles are an approximation of what constitutes a GitOps practice: Declarative description of the system is stored in Git (configs, monitoring, etc) Changes to the state are made via pull requests Git push reconciled with the state of the running system with the state in the Git repository","title":"What is GitOps?"},{"location":"developer-foundation/gitops/#cicd-integration","text":"For the full end-to-end build and delivery process, both the CI and CD pipelines are required. For this to work, a Git repo is used to convey the configuration values. Within the , we have used certain naming conventions to streamline and simplify the integration between the various components. The naming components are: - app repo - The name of the Git repository for the application - git org - The name of the GitHub organization for the application's repo - tag - The build version - chart version - The version of the Helm chart - region - The geographic location in IBM Cloud The derived names are: - GitHub application path: github.com/{git org}/{app repo} - CI Pipeline name: {git org}.{app repo} - Docker image's path: {region}.icr.io/{git org}/{app repo}:{tag} in the Image Registry - Helm chart's path: generic-local/{git org}/{app repo}-{tag}-{chart version}.tgz in the Helm Repository - Dependencies filename: {app repo}/requirements.yaml in the GitOps repo - CD Pipeline name: {app repo}","title":"CI/CD integration"},{"location":"developer-foundation/gitops/#resources","text":"Presentation: Understanding GitOps","title":"Resources"},{"location":"developer-foundation/gitops/#what-is-argo-cd","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a Kubernetes cluster or namespace, which also works for an OpenShift cluster or project. Argo CD models a collection of applications as a project and uses a Git repository to store the project's desired state. Each application is stored as a folder in the repository, and each deployment environment is stored as a branch in the repository. Argo CD supports defining Kubernetes manifests in a number of ways: - helm charts - kustomize - ksonnet - jsonnet - plain directory of yaml/json manifests Argo CD synchronizes the application state with the desired state defined in Git and automates the deployment of Kubernetes resources to ensure they match.","title":"What is Argo CD"},{"location":"developer-foundation/gitops/#activities","text":"These activities give you a chance to walk through building CD pipelines using ArgoCD. These tasks assume that you have: - Reviewed the Continuous Deployment concept page. Task Description Link Time Walk-through GitOps Introduction to GitOps with OpenShift Learn OpenShift 20 min GitOps Multi-cluster Multi-cluster GitOps with OpenShift Learn OpenShift 20 min Try It Yourself ArgoCD Lab Learn how to setup ArgoCD and Deploy Application ArgoCD 30 min Once you have completed these tasks, you will have created an ArgoCD deployment and have an understanding of Continuous Deployment.","title":"Activities"},{"location":"developer-foundation/k8s-configuration/","text":"Container Configuration \u00b6 Command and Argument When you create a Pod, you can define a command and arguments for the containers that run in the Pod. The command and arguments that you define in the configuration file override the default command and arguments provided by the container image Dockerfile vs Kubernetes Dockerfile Entrypoint -> k8s command Dockerfile CMD -> k8s args Ports When you create a Pod, you can specify the port number the container exposes, as best practice is good to put a name , this way a service can specify targetport by name reference. Environment Variable When you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the env or envFrom field in the container configuration A Pod can use environment variables to expose information about itself to Containers running in the Pod. Environment variables can expose Pod fields and Container fields Resources \u00b6 IKS & OpenShift - Container Commands - Environment Variables - Pod Exposing References \u00b6 apiVersion : v1 kind : Pod metadata : name : my-cmd-pod spec : containers : - name : myapp-container image : busybox command : [ 'echo' ] restartPolicy : Never apiVersion : v1 kind : Pod metadata : name : my-arg-pod spec : containers : - name : myapp-container image : busybox command : [ 'echo' ] args : [ 'Hello World' ] restartPolicy : Never apiVersion : v1 kind : Pod metadata : name : my-port-pod spec : containers : - name : myapp-container image : bitnami/nginx ports : - containerPort : 8080 apiVersion : v1 kind : Pod metadata : name : my-env-pod spec : restartPolicy : Never containers : - name : c image : busybox env : - name : DEMO_GREETING value : \"Hello from the environment\" command : [ \"echo\" ] args : [ \"$(DEMO_GREETING)\" ] apiVersion : v1 kind : Pod metadata : name : my-inter-pod labels : app : jedi spec : restartPolicy : Never containers : - name : myapp image : busybox ports : - containerPort : 8080 name : http env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName - name : MY_POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : MY_POD_IP valueFrom : fieldRef : fieldPath : status.podIP command : [ \"echo\" ] args : [ \"$(MY_NODE_NAME) $(MY_POD_NAME) $(MY_POD_IP)\" ] Resource Requirements \u00b6 When you specify a Pod, you can optionally specify how much CPU and memory (RAM) each Container needs. When Containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on. CPU and memory are each a resource type. A resource type has a base unit. CPU is specified in units of cores, and memory is specified in units of bytes. Resources \u00b6 IKS & OpenShift - Compute Resources - Memory Management References \u00b6 apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : my-app image : bitnami/nginx ports : - containerPort : 8080 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" Namespaced defaults mem apiVersion : v1 kind : LimitRange metadata : name : mem-limit-range spec : limits : - default : memory : 512Mi defaultRequest : memory : 256Mi type : Container Namespaced defaults mem apiVersion : v1 kind : LimitRange metadata : name : cpu-limit-range spec : limits : - default : cpu : 1 defaultRequest : cpu : 0.5 type : Container ConfigMaps \u00b6 ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable. You can data from a ConfigMap in 3 different ways. - As a single environment variable specific to a single key - As a set of environment variables from all keys - As a set of files, each key represented by a file on mounted volume Resources \u00b6 OpenShift - Mapping Volumes IKS - ConfigMaps References \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : my-cm data : color : blue location : naboo apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : myapp image : busybox command : [ \"echo\" ] args : [ \"color is $(MY_VAR)\" ] env : - name : MY_VAR valueFrom : configMapKeyRef : name : my-cm key : color apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : myapp image : busybox command : [ \"sh\" , \"-c\" , \"ls -l /etc/config; echo located at $(cat /etc/config/location)\" , ] volumeMounts : - name : config-volume mountPath : /etc/config volumes : - name : config-volume configMap : name : my-cm apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : myapp image : busybox command : [ \"/bin/sh\" , \"-c\" , \"env | sort\" ] envFrom : - configMapRef : name : my-cm restartPolicy : Never Secrets \u00b6 Kubernetes secret objects let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image. A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. Resources \u00b6 OpenShift - Secrets - Secret Commands IKS - Secrets - Secret Distribution References \u00b6 apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= stringData : admin : administrator apiVersion : v1 kind : Secret metadata : name : mysecret-config type : Opaque stringData : config.yaml : |- apiUrl: \"https://my.api.com/api/v1\" username: token password: thesecrettoken apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : my-app image : bitnami/nginx ports : - containerPort : 8080 env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : mysecret key : username envFrom : - secretRef : name : mysecret volumeMounts : - name : config mountPath : \"/etc/secrets\" volumes : - name : config secret : secretName : mysecret-config Openshift Create files needed for rest of example. echo -n 'admin' > ./username.txt echo -n '1f2d1e2e67df' > ./password.txt Creating Secret from files. oc create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt Getting Secret oc get secrets Gets the Secret's Description. oc describe secrets/db-user-pass IKS Create files needed for rest of example. echo -n 'admin' > ./username.txt echo -n '1f2d1e2e67df' > ./password.txt Creates the Secret from the files kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt Gets the Secret kubectl get secrets Gets the Secret's Description. kubectl describe secrets/db-user-pass SecurityContexts \u00b6 A security context defines privilege and access control settings for a Pod or Container. To specify security settings for a Pod, include the securityContext field in the Pod specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod. Resources \u00b6 OpenShift - Managing Security Contexts IKS Security Contexts References \u00b6 Setup minikube VM with users minikube ssh su - echo \"container-user-0:x:2000:2000:-:/home/container-user-0:/bin/bash\" >> /etc/passwd echo \"container-user-1:x:2001:2001:-:/home/container-user-1:/bin/bash\" >> /etc/passwd echo \"container-group-0:x:3000:\" >>/etc/group echo \"container-group-1:x:3001:\" >>/etc/group mkdir -p /etc/message/ echo \"Hello, World!\" | sudo tee -a /etc/message/message.txt chown 2000:3000 /etc/message/message.txt chmod 640 /etc/message/message.txt Using the this securityContext the container will be able to read the file /message/message.txt apiVersion : v1 kind : Pod metadata : name : my-securitycontext-pod spec : securityContext : runAsUser : 2000 runAsGroup : 3000 fsGroup : 3000 containers : - name : myapp-container image : busybox command : [ \"sh\" , \"-c\" , \"cat /message/message.txt && sleep 3600\" ] volumeMounts : - name : message-volume mountPath : /message volumes : - name : message-volume hostPath : path : /etc/message Using the this securityContext the container should NOT be able to read the file /message/message.txt apiVersion : v1 kind : Pod metadata : name : my-securitycontext-pod spec : securityContext : runAsUser : 2001 runAsGroup : 3001 fsGroup : 3001 containers : - name : myapp-container image : busybox command : [ \"sh\" , \"-c\" , \"cat /message/message.txt && sleep 3600\" ] volumeMounts : - name : message-volume mountPath : /message volumes : - name : message-volume hostPath : path : /etc/message Run to see the errors Openshift Get Pod Logs oc logs my-securitycontext-pod Should return cat: can't open '/message/message.txt': Permission denied IKS Get Pod Logs kubectl logs my-securitycontext-pod Should return cat: can't open '/message/message.txt': Permission denied Service Accounts \u00b6 A service account provides an identity for processes that run in a Pod. When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default). User accounts are for humans. Service accounts are for processes, which run in pods. User accounts are intended to be global. Names must be unique across all namespaces of a cluster, future user resource will not be namespaced. Service accounts are namespaced. Resources \u00b6 OpenShift Service Accounts Using Service Accounts IKS Service Accounts Service Account Configuration References \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : my-service-account apiVersion : v1 kind : Pod metadata : name : my-pod spec : serviceAccountName : my-service-account containers : - name : my-app image : bitnami/nginx ports : - containerPort : 8080 apiVersion : v1 kind : Secret metadata : name : build-robot-secret annotations : kubernetes.io/service-account.name : my-service-account type : kubernetes.io/service-account-token Openshift Creating a ServiceAccount oc create sa <service_account_name> View ServiceAccount Details oc describe sa <service_account_name> IKS Create a ServiceAccount kubectl create sa <service_account_name> View ServiceAccount Details kubectl describe sa <service_account_name> Activities \u00b6 Task Description Link Try It Yourself Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Pod Creation Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration","title":"Configuration"},{"location":"developer-foundation/k8s-configuration/#container-configuration","text":"Command and Argument When you create a Pod, you can define a command and arguments for the containers that run in the Pod. The command and arguments that you define in the configuration file override the default command and arguments provided by the container image Dockerfile vs Kubernetes Dockerfile Entrypoint -> k8s command Dockerfile CMD -> k8s args Ports When you create a Pod, you can specify the port number the container exposes, as best practice is good to put a name , this way a service can specify targetport by name reference. Environment Variable When you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the env or envFrom field in the container configuration A Pod can use environment variables to expose information about itself to Containers running in the Pod. Environment variables can expose Pod fields and Container fields","title":"Container Configuration"},{"location":"developer-foundation/k8s-configuration/#resources","text":"IKS & OpenShift - Container Commands - Environment Variables - Pod Exposing","title":"Resources"},{"location":"developer-foundation/k8s-configuration/#references","text":"apiVersion : v1 kind : Pod metadata : name : my-cmd-pod spec : containers : - name : myapp-container image : busybox command : [ 'echo' ] restartPolicy : Never apiVersion : v1 kind : Pod metadata : name : my-arg-pod spec : containers : - name : myapp-container image : busybox command : [ 'echo' ] args : [ 'Hello World' ] restartPolicy : Never apiVersion : v1 kind : Pod metadata : name : my-port-pod spec : containers : - name : myapp-container image : bitnami/nginx ports : - containerPort : 8080 apiVersion : v1 kind : Pod metadata : name : my-env-pod spec : restartPolicy : Never containers : - name : c image : busybox env : - name : DEMO_GREETING value : \"Hello from the environment\" command : [ \"echo\" ] args : [ \"$(DEMO_GREETING)\" ] apiVersion : v1 kind : Pod metadata : name : my-inter-pod labels : app : jedi spec : restartPolicy : Never containers : - name : myapp image : busybox ports : - containerPort : 8080 name : http env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName - name : MY_POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : MY_POD_IP valueFrom : fieldRef : fieldPath : status.podIP command : [ \"echo\" ] args : [ \"$(MY_NODE_NAME) $(MY_POD_NAME) $(MY_POD_IP)\" ]","title":"References"},{"location":"developer-foundation/k8s-configuration/#resource-requirements","text":"When you specify a Pod, you can optionally specify how much CPU and memory (RAM) each Container needs. When Containers have resource requests specified, the scheduler can make better decisions about which nodes to place Pods on. CPU and memory are each a resource type. A resource type has a base unit. CPU is specified in units of cores, and memory is specified in units of bytes.","title":"Resource Requirements"},{"location":"developer-foundation/k8s-configuration/#resources_1","text":"IKS & OpenShift - Compute Resources - Memory Management","title":"Resources"},{"location":"developer-foundation/k8s-configuration/#references_1","text":"apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : my-app image : bitnami/nginx ports : - containerPort : 8080 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" Namespaced defaults mem apiVersion : v1 kind : LimitRange metadata : name : mem-limit-range spec : limits : - default : memory : 512Mi defaultRequest : memory : 256Mi type : Container Namespaced defaults mem apiVersion : v1 kind : LimitRange metadata : name : cpu-limit-range spec : limits : - default : cpu : 1 defaultRequest : cpu : 0.5 type : Container","title":"References"},{"location":"developer-foundation/k8s-configuration/#configmaps","text":"ConfigMaps allow you to decouple configuration artifacts from image content to keep containerized applications portable. You can data from a ConfigMap in 3 different ways. - As a single environment variable specific to a single key - As a set of environment variables from all keys - As a set of files, each key represented by a file on mounted volume","title":"ConfigMaps"},{"location":"developer-foundation/k8s-configuration/#resources_2","text":"OpenShift - Mapping Volumes IKS - ConfigMaps","title":"Resources"},{"location":"developer-foundation/k8s-configuration/#references_2","text":"apiVersion : v1 kind : ConfigMap metadata : name : my-cm data : color : blue location : naboo apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : myapp image : busybox command : [ \"echo\" ] args : [ \"color is $(MY_VAR)\" ] env : - name : MY_VAR valueFrom : configMapKeyRef : name : my-cm key : color apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : myapp image : busybox command : [ \"sh\" , \"-c\" , \"ls -l /etc/config; echo located at $(cat /etc/config/location)\" , ] volumeMounts : - name : config-volume mountPath : /etc/config volumes : - name : config-volume configMap : name : my-cm apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : myapp image : busybox command : [ \"/bin/sh\" , \"-c\" , \"env | sort\" ] envFrom : - configMapRef : name : my-cm restartPolicy : Never","title":"References"},{"location":"developer-foundation/k8s-configuration/#secrets","text":"Kubernetes secret objects let you store and manage sensitive information, such as passwords, OAuth tokens, and ssh keys. Putting this information in a secret is safer and more flexible than putting it verbatim in a Pod definition or in a container image. A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure.","title":"Secrets"},{"location":"developer-foundation/k8s-configuration/#resources_3","text":"OpenShift - Secrets - Secret Commands IKS - Secrets - Secret Distribution","title":"Resources"},{"location":"developer-foundation/k8s-configuration/#references_3","text":"apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= stringData : admin : administrator apiVersion : v1 kind : Secret metadata : name : mysecret-config type : Opaque stringData : config.yaml : |- apiUrl: \"https://my.api.com/api/v1\" username: token password: thesecrettoken apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : my-app image : bitnami/nginx ports : - containerPort : 8080 env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : mysecret key : username envFrom : - secretRef : name : mysecret volumeMounts : - name : config mountPath : \"/etc/secrets\" volumes : - name : config secret : secretName : mysecret-config Openshift Create files needed for rest of example. echo -n 'admin' > ./username.txt echo -n '1f2d1e2e67df' > ./password.txt Creating Secret from files. oc create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt Getting Secret oc get secrets Gets the Secret's Description. oc describe secrets/db-user-pass IKS Create files needed for rest of example. echo -n 'admin' > ./username.txt echo -n '1f2d1e2e67df' > ./password.txt Creates the Secret from the files kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt Gets the Secret kubectl get secrets Gets the Secret's Description. kubectl describe secrets/db-user-pass","title":"References"},{"location":"developer-foundation/k8s-configuration/#securitycontexts","text":"A security context defines privilege and access control settings for a Pod or Container. To specify security settings for a Pod, include the securityContext field in the Pod specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod.","title":"SecurityContexts"},{"location":"developer-foundation/k8s-configuration/#resources_4","text":"OpenShift - Managing Security Contexts IKS Security Contexts","title":"Resources"},{"location":"developer-foundation/k8s-configuration/#references_4","text":"Setup minikube VM with users minikube ssh su - echo \"container-user-0:x:2000:2000:-:/home/container-user-0:/bin/bash\" >> /etc/passwd echo \"container-user-1:x:2001:2001:-:/home/container-user-1:/bin/bash\" >> /etc/passwd echo \"container-group-0:x:3000:\" >>/etc/group echo \"container-group-1:x:3001:\" >>/etc/group mkdir -p /etc/message/ echo \"Hello, World!\" | sudo tee -a /etc/message/message.txt chown 2000:3000 /etc/message/message.txt chmod 640 /etc/message/message.txt Using the this securityContext the container will be able to read the file /message/message.txt apiVersion : v1 kind : Pod metadata : name : my-securitycontext-pod spec : securityContext : runAsUser : 2000 runAsGroup : 3000 fsGroup : 3000 containers : - name : myapp-container image : busybox command : [ \"sh\" , \"-c\" , \"cat /message/message.txt && sleep 3600\" ] volumeMounts : - name : message-volume mountPath : /message volumes : - name : message-volume hostPath : path : /etc/message Using the this securityContext the container should NOT be able to read the file /message/message.txt apiVersion : v1 kind : Pod metadata : name : my-securitycontext-pod spec : securityContext : runAsUser : 2001 runAsGroup : 3001 fsGroup : 3001 containers : - name : myapp-container image : busybox command : [ \"sh\" , \"-c\" , \"cat /message/message.txt && sleep 3600\" ] volumeMounts : - name : message-volume mountPath : /message volumes : - name : message-volume hostPath : path : /etc/message Run to see the errors Openshift Get Pod Logs oc logs my-securitycontext-pod Should return cat: can't open '/message/message.txt': Permission denied IKS Get Pod Logs kubectl logs my-securitycontext-pod Should return cat: can't open '/message/message.txt': Permission denied","title":"References"},{"location":"developer-foundation/k8s-configuration/#service-accounts","text":"A service account provides an identity for processes that run in a Pod. When you (a human) access the cluster (for example, using kubectl), you are authenticated by the apiserver as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default). User accounts are for humans. Service accounts are for processes, which run in pods. User accounts are intended to be global. Names must be unique across all namespaces of a cluster, future user resource will not be namespaced. Service accounts are namespaced.","title":"Service Accounts"},{"location":"developer-foundation/k8s-configuration/#resources_5","text":"OpenShift Service Accounts Using Service Accounts IKS Service Accounts Service Account Configuration","title":"Resources"},{"location":"developer-foundation/k8s-configuration/#references_5","text":"apiVersion : v1 kind : ServiceAccount metadata : name : my-service-account apiVersion : v1 kind : Pod metadata : name : my-pod spec : serviceAccountName : my-service-account containers : - name : my-app image : bitnami/nginx ports : - containerPort : 8080 apiVersion : v1 kind : Secret metadata : name : build-robot-secret annotations : kubernetes.io/service-account.name : my-service-account type : kubernetes.io/service-account-token Openshift Creating a ServiceAccount oc create sa <service_account_name> View ServiceAccount Details oc describe sa <service_account_name> IKS Create a ServiceAccount kubectl create sa <service_account_name> View ServiceAccount Details kubectl describe sa <service_account_name>","title":"References"},{"location":"developer-foundation/k8s-configuration/#activities","text":"Task Description Link Try It Yourself Pod Creation Challenge yourself to create a Pod YAML file to meet certain parameters. Pod Creation Pod Configuration Configure a pod to meet compute resource requirements. Pod Configuration","title":"Activities"},{"location":"developer-foundation/k8s-core-concepts/","text":"Kubernetes API Primitives \u00b6 Kubernetes API primitive, also known as Kubernetes objects, are the basic building blocks of any application running in Kubernetes Examples: - Pod - Node - Service - ServiceAccount Two primary members - Spec, desired state - Status, current state Resources \u00b6 OpenShift - Pods - Nodes IKS - Objects - Kube Basics References \u00b6 Openshift Prints all API Resources oc api-resources Prints all API Resources with their verbs. oc api-resources -o wide Prints all API Resources names only oc api-resources -o name Prints each of the available nodes, projects, services, deployments, and pods oc get nodes,ns,po,deploy,svc Prints the node's description oc describe node IKS Getting API Resources kubectl api-resources Viewing Resources kubectl api-resources -o wide Viewing Resources kubectl api-resources -o name Getting a list of specific objects kubectl get nodes,ns,po,deploy,svc Describing the resources kubectl describe node --all Creating Pods \u00b6 A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents processes running on your Cluster. A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. Resources \u00b6 OpenShift - About Pods - Cluster Configuration for Pods - Pod Autoscaling IKS - Pod Overview - Pod Lifecycle - Pod Usage References \u00b6 apiVersion : v1 kind : Pod metadata : name : myapp-pod labels : app : myapp spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] Openshift Get Current Pods in Project oc get pods Get Pod's Description oc describe pod <pod-name> Get Pods with their IP and node location oc get pods -o wide Get Pods Stats oc adm top pods IKS Get Current Pods in Project kubectl get pods Get Pod's Description kubectl describe pod <pod-name> Delete a Pod kubectl delete pod <pod-name> Edit a Pod kubectl edit pod <pod-name> Projects/Namespaces \u00b6 Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple users (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace. In practice namespaces are used to deploy different versions based on stages of the CICD pipeline (dev, test, stage, prod) Resources \u00b6 OpenShift - Working With Projects - Creating Projects - Configure Project Creation IKS - Namespaces References: \u00b6 apiVersion : v1 kind : Namespace metadata : name : foo apiVersion : v1 kind : Pod metadata : name : myapp-pod namespace : bar labels : app : myapp spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] Openshift Create a new Project oc new-project my-project Viewing Current Project oc project Viewing Project Status oc status IKS Getting all namespaces in cluster kubectl get namespaces Create a new namespace called bar kubectl create ns bar Setting Namespace in Context kubectl set-context --current --namespace=bar","title":"Core Concepts"},{"location":"developer-foundation/k8s-core-concepts/#kubernetes-api-primitives","text":"Kubernetes API primitive, also known as Kubernetes objects, are the basic building blocks of any application running in Kubernetes Examples: - Pod - Node - Service - ServiceAccount Two primary members - Spec, desired state - Status, current state","title":"Kubernetes API Primitives"},{"location":"developer-foundation/k8s-core-concepts/#resources","text":"OpenShift - Pods - Nodes IKS - Objects - Kube Basics","title":"Resources"},{"location":"developer-foundation/k8s-core-concepts/#references","text":"Openshift Prints all API Resources oc api-resources Prints all API Resources with their verbs. oc api-resources -o wide Prints all API Resources names only oc api-resources -o name Prints each of the available nodes, projects, services, deployments, and pods oc get nodes,ns,po,deploy,svc Prints the node's description oc describe node IKS Getting API Resources kubectl api-resources Viewing Resources kubectl api-resources -o wide Viewing Resources kubectl api-resources -o name Getting a list of specific objects kubectl get nodes,ns,po,deploy,svc Describing the resources kubectl describe node --all","title":"References"},{"location":"developer-foundation/k8s-core-concepts/#creating-pods","text":"A Pod is the basic execution unit of a Kubernetes application\u2013the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents processes running on your Cluster. A Pod encapsulates an application\u2019s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources.","title":"Creating Pods"},{"location":"developer-foundation/k8s-core-concepts/#resources_1","text":"OpenShift - About Pods - Cluster Configuration for Pods - Pod Autoscaling IKS - Pod Overview - Pod Lifecycle - Pod Usage","title":"Resources"},{"location":"developer-foundation/k8s-core-concepts/#references_1","text":"apiVersion : v1 kind : Pod metadata : name : myapp-pod labels : app : myapp spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] Openshift Get Current Pods in Project oc get pods Get Pod's Description oc describe pod <pod-name> Get Pods with their IP and node location oc get pods -o wide Get Pods Stats oc adm top pods IKS Get Current Pods in Project kubectl get pods Get Pod's Description kubectl describe pod <pod-name> Delete a Pod kubectl delete pod <pod-name> Edit a Pod kubectl edit pod <pod-name>","title":"References"},{"location":"developer-foundation/k8s-core-concepts/#projectsnamespaces","text":"Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces are a way to divide cluster resources between multiple users (via resource quota). It is not necessary to use multiple namespaces just to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace. In practice namespaces are used to deploy different versions based on stages of the CICD pipeline (dev, test, stage, prod)","title":"Projects/Namespaces"},{"location":"developer-foundation/k8s-core-concepts/#resources_2","text":"OpenShift - Working With Projects - Creating Projects - Configure Project Creation IKS - Namespaces","title":"Resources"},{"location":"developer-foundation/k8s-core-concepts/#references_2","text":"apiVersion : v1 kind : Namespace metadata : name : foo apiVersion : v1 kind : Pod metadata : name : myapp-pod namespace : bar labels : app : myapp spec : containers : - name : myapp-container image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] Openshift Create a new Project oc new-project my-project Viewing Current Project oc project Viewing Project Status oc status IKS Getting all namespaces in cluster kubectl get namespaces Create a new namespace called bar kubectl create ns bar Setting Namespace in Context kubectl set-context --current --namespace=bar","title":"References:"},{"location":"developer-foundation/k8s-multi-container-pods/","text":"Multi-Containers Pod \u00b6 Container images solve many real-world problems with existing packaging and deployment tools, but in addition to these significant benefits, containers offer us an opportunity to fundamentally re-think the way we build distributed applications. Just as service oriented architectures (SOA) encouraged the decomposition of applications into modular, focused services, containers should encourage the further decomposition of these services into closely cooperating modular containers. By virtue of establishing a boundary, containers enable users to build their services using modular, reusable components, and this in turn leads to services that are more reliable, more scalable and faster to build than applications built from monolithic containers. Resources \u00b6 OpenShift IKS - Sidecar Logging - Shared Volume Communication - Toolkit Patterns - Brendan Burns Paper References \u00b6 apiVersion : v1 kind : Pod metadata : name : my-pod spec : volumes : - name : shared-data emptyDir : {} containers : - name : app image : bitnami/nginx volumeMounts : - name : shared-data mountPath : /app ports : - containerPort : 8080 - name : sidecard image : busybox volumeMounts : - name : shared-data mountPath : /pod-data command : [ 'sh' , '-c' , 'echo Hello from the side container > /pod-data/index.html && sleep 3600' ] apiVersion : v1 kind : Pod metadata : name : my-pod spec : shareProcessNamespace : true containers : - name : app image : bitnami/nginx ports : - containerPort : 8080 - name : sidecard image : busybox securityContext : capabilities : add : - SYS_PTRACE stdin : true tty : true Openshift Attach Pods Together oc attach -it my-pod -c sidecard ps ax kill -HUP 7 ps ax IKS Attach Pods Together kubectl attach -it my-pod -c sidecard ps ax kill -HUP 7 ps ax Activities \u00b6 Task Description Link Try It Yourself Multiple Containers Build a container using legacy container image. Multiple Containers","title":"Multi-Container Pods"},{"location":"developer-foundation/k8s-multi-container-pods/#multi-containers-pod","text":"Container images solve many real-world problems with existing packaging and deployment tools, but in addition to these significant benefits, containers offer us an opportunity to fundamentally re-think the way we build distributed applications. Just as service oriented architectures (SOA) encouraged the decomposition of applications into modular, focused services, containers should encourage the further decomposition of these services into closely cooperating modular containers. By virtue of establishing a boundary, containers enable users to build their services using modular, reusable components, and this in turn leads to services that are more reliable, more scalable and faster to build than applications built from monolithic containers.","title":"Multi-Containers Pod"},{"location":"developer-foundation/k8s-multi-container-pods/#resources","text":"OpenShift IKS - Sidecar Logging - Shared Volume Communication - Toolkit Patterns - Brendan Burns Paper","title":"Resources"},{"location":"developer-foundation/k8s-multi-container-pods/#references","text":"apiVersion : v1 kind : Pod metadata : name : my-pod spec : volumes : - name : shared-data emptyDir : {} containers : - name : app image : bitnami/nginx volumeMounts : - name : shared-data mountPath : /app ports : - containerPort : 8080 - name : sidecard image : busybox volumeMounts : - name : shared-data mountPath : /pod-data command : [ 'sh' , '-c' , 'echo Hello from the side container > /pod-data/index.html && sleep 3600' ] apiVersion : v1 kind : Pod metadata : name : my-pod spec : shareProcessNamespace : true containers : - name : app image : bitnami/nginx ports : - containerPort : 8080 - name : sidecard image : busybox securityContext : capabilities : add : - SYS_PTRACE stdin : true tty : true Openshift Attach Pods Together oc attach -it my-pod -c sidecard ps ax kill -HUP 7 ps ax IKS Attach Pods Together kubectl attach -it my-pod -c sidecard ps ax kill -HUP 7 ps ax","title":"References"},{"location":"developer-foundation/k8s-multi-container-pods/#activities","text":"Task Description Link Try It Yourself Multiple Containers Build a container using legacy container image. Multiple Containers","title":"Activities"},{"location":"developer-foundation/k8s-observability/","text":"Liveness and Readiness Probes \u00b6 A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. There are three types of handlers: ExecAction : Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0. TCPSocketAction : Performs a TCP check against the Container\u2019s IP address on a specified port. The diagnostic is considered successful if the port is open. HTTPGetAction : Performs an HTTP Get request against the Container\u2019s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400. The kubelet can optionally perform and react to three kinds of probes on running Containers: livenessProbe : Indicates whether the Container is running. readinessProbe : Indicates whether the Container is ready to service requests. startupProbe : Indicates whether the application within the Container is started. Resources \u00b6 OpenShift - Application Health - Virtual Machine Health IKS - Container Probes - Configure Probes References \u00b6 apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : app image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] livenessProbe : exec : command : [ 'echo' , 'alive' ] apiVersion : v1 kind : Pod metadata : name : my-pod spec : shareProcessNamespace : true containers : - name : app image : bitnami/nginx ports : - containerPort : 8080 livenessProbe : tcpSocket : port : 8080 initialDelaySeconds : 10 readinessProbe : httpGet : path : / port : 8080 periodSeconds : 10 Container Logging \u00b6 Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity. Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster. Resources \u00b6 OpenShift - Logs Command - Cluster Logging - Logging Collector IKS - Logging References \u00b6 apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox command : [ 'sh' , '-c' , 'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done' ] OpenShift Get Logs oc logs Use Stern to View Logs brew install stern stern . -n default IKS Get Logs kubectl logs Use Stern to View Logs brew install stern stern . -n default Monitoring Applications \u00b6 To scale an application and provide a reliable service, you need to understand how the application behaves when it is deployed. You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application\u2019s resource usage at each of these levels. This information allows you to evaluate your application\u2019s performance and where bottlenecks can be removed to improve overall performance. Prometheus, a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself. Resources \u00b6 OpenShift - Monitoring Application Health - Monitoring Services - Custom Application Metrics IKS - Monitoring Resource Usage - Resource Metrics References \u00b6 apiVersion : v1 kind : Pod metadata : name : 500m spec : containers : - name : app image : gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 resources : requests : cpu : 700m memory : 128Mi - name : busybox-sidecar image : radial/busyboxplus:curl command : [ /bin/sh , -c , 'until curl localhost:8080/ConsumeCPU -d \"millicores=500&durationSec=3600\"; do sleep 5; done && sleep 3700' ] apiVersion : v1 kind : Pod metadata : name : 200m spec : containers : - name : app image : gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 resources : requests : cpu : 300m memory : 64Mi - name : busybox-sidecar image : radial/busyboxplus:curl command : [ /bin/sh , -c , 'until curl localhost:8080/ConsumeCPU -d \"millicores=200&durationSec=3600\"; do sleep 5; done && sleep 3700' ] OpenShift oc get projects oc api-resources -o wide oc api-resources -o name oc get nodes,ns,po,deploy,svc oc describe node --all IKS Verify Metrics is enabled kubectl get --raw /apis/metrics.k8s.io/ Get Node Description kubectl describe node Check Resource Usage kubectl top pods kubectl top nodes Activities \u00b6 Task Description Link Try It Yourself Probes Create some Health & Startup Probes to find what's causing an issue. Probes","title":"Observability"},{"location":"developer-foundation/k8s-observability/#liveness-and-readiness-probes","text":"A Probe is a diagnostic performed periodically by the kubelet on a Container. To perform a diagnostic, the kubelet calls a Handler implemented by the Container. There are three types of handlers: ExecAction : Executes a specified command inside the Container. The diagnostic is considered successful if the command exits with a status code of 0. TCPSocketAction : Performs a TCP check against the Container\u2019s IP address on a specified port. The diagnostic is considered successful if the port is open. HTTPGetAction : Performs an HTTP Get request against the Container\u2019s IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400. The kubelet can optionally perform and react to three kinds of probes on running Containers: livenessProbe : Indicates whether the Container is running. readinessProbe : Indicates whether the Container is ready to service requests. startupProbe : Indicates whether the application within the Container is started.","title":"Liveness and Readiness Probes"},{"location":"developer-foundation/k8s-observability/#resources","text":"OpenShift - Application Health - Virtual Machine Health IKS - Container Probes - Configure Probes","title":"Resources"},{"location":"developer-foundation/k8s-observability/#references","text":"apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : app image : busybox command : [ 'sh' , '-c' , \"echo Hello, Kubernetes! && sleep 3600\" ] livenessProbe : exec : command : [ 'echo' , 'alive' ] apiVersion : v1 kind : Pod metadata : name : my-pod spec : shareProcessNamespace : true containers : - name : app image : bitnami/nginx ports : - containerPort : 8080 livenessProbe : tcpSocket : port : 8080 initialDelaySeconds : 10 readinessProbe : httpGet : path : / port : 8080 periodSeconds : 10","title":"References"},{"location":"developer-foundation/k8s-observability/#container-logging","text":"Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity. Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster.","title":"Container Logging"},{"location":"developer-foundation/k8s-observability/#resources_1","text":"OpenShift - Logs Command - Cluster Logging - Logging Collector IKS - Logging","title":"Resources"},{"location":"developer-foundation/k8s-observability/#references_1","text":"apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox command : [ 'sh' , '-c' , 'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done' ] OpenShift Get Logs oc logs Use Stern to View Logs brew install stern stern . -n default IKS Get Logs kubectl logs Use Stern to View Logs brew install stern stern . -n default","title":"References"},{"location":"developer-foundation/k8s-observability/#monitoring-applications","text":"To scale an application and provide a reliable service, you need to understand how the application behaves when it is deployed. You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application\u2019s resource usage at each of these levels. This information allows you to evaluate your application\u2019s performance and where bottlenecks can be removed to improve overall performance. Prometheus, a CNCF project, can natively monitor Kubernetes, nodes, and Prometheus itself.","title":"Monitoring Applications"},{"location":"developer-foundation/k8s-observability/#resources_2","text":"OpenShift - Monitoring Application Health - Monitoring Services - Custom Application Metrics IKS - Monitoring Resource Usage - Resource Metrics","title":"Resources"},{"location":"developer-foundation/k8s-observability/#references_2","text":"apiVersion : v1 kind : Pod metadata : name : 500m spec : containers : - name : app image : gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 resources : requests : cpu : 700m memory : 128Mi - name : busybox-sidecar image : radial/busyboxplus:curl command : [ /bin/sh , -c , 'until curl localhost:8080/ConsumeCPU -d \"millicores=500&durationSec=3600\"; do sleep 5; done && sleep 3700' ] apiVersion : v1 kind : Pod metadata : name : 200m spec : containers : - name : app image : gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 resources : requests : cpu : 300m memory : 64Mi - name : busybox-sidecar image : radial/busyboxplus:curl command : [ /bin/sh , -c , 'until curl localhost:8080/ConsumeCPU -d \"millicores=200&durationSec=3600\"; do sleep 5; done && sleep 3700' ] OpenShift oc get projects oc api-resources -o wide oc api-resources -o name oc get nodes,ns,po,deploy,svc oc describe node --all IKS Verify Metrics is enabled kubectl get --raw /apis/metrics.k8s.io/ Get Node Description kubectl describe node Check Resource Usage kubectl top pods kubectl top nodes","title":"References"},{"location":"developer-foundation/k8s-observability/#activities","text":"Task Description Link Try It Yourself Probes Create some Health & Startup Probes to find what's causing an issue. Probes","title":"Activities"},{"location":"developer-foundation/k8s-observability/logs/","text":"Container Logging \u00b6 Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity. Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster. Resources \u00b6 https://kubernetes.io/docs/concepts/cluster-administration/logging/ References \u00b6 apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox command : [ 'sh' , '-c' , 'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done' ] kubectl logs brew install stern stern . -n default","title":"Logging"},{"location":"developer-foundation/k8s-observability/logs/#container-logging","text":"Application and systems logs can help you understand what is happening inside your cluster. The logs are particularly useful for debugging problems and monitoring cluster activity. Kubernetes provides no native storage solution for log data, but you can integrate many existing logging solutions into your Kubernetes cluster.","title":"Container Logging"},{"location":"developer-foundation/k8s-observability/logs/#resources","text":"https://kubernetes.io/docs/concepts/cluster-administration/logging/","title":"Resources"},{"location":"developer-foundation/k8s-observability/logs/#references","text":"apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : busybox command : [ 'sh' , '-c' , 'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 5; done' ] kubectl logs brew install stern stern . -n default","title":"References"},{"location":"developer-foundation/k8s-overview/","text":"Introduction \u00b6 Kubernetes is an open source container orchestration platform that automates deployment, management and scaling of applications. Learn how Kubernetes enables cost-effective Cloud-Native development. What is Kubernetes? \u00b6 Kubernetes\u2014also known as \u2018k8s\u2019 or \u2018kube\u2019\u2014is a container orchestration platform for scheduling and automating the deployment, management, and scaling of containerized applications. Kubernetes was first developed by engineers at Google before being open sourced in 2014. It is a descendant of \u2018Borg,\u2019 a container orchestration platform used internally at Google. (Kubernetes is Greek for helmsman or pilot, hence the helm in the Kubernetes logo .) Today, Kubernetes and the broader container ecosystem are maturing into a general-purpose computing platform and ecosystem that rivals\u2014if not surpasses\u2014virtual machines (VMs) as the basic building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver a high-productivity Platform-as-a-Service (PaaS) that addresses multiple infrastructure- and operations-related tasks and issues surrounding Cloud-Native development so that development teams can focus solely on coding and innovation. https://www.ibm.com/cloud/learn/kubernetes Predictable Demands Pattern \u00b6 An application's performance, efficiency, and behaviors are reliant upon it's ability to have the appropriate allocation of resources. The Predictable Demands pattern is based on declaring the dependencies and resources needed by a given application. The scheduler will prioritize an application with a defined set of resources and dependencies since it can better manage the workload across nodes in the cluster. Each application has a different set of dependencies which we will touch on next. Runtime Dependencies \u00b6 One of the most common runtime dependency is the exposure of a container's specific port through hostPort. Different applications can specify the same port through hostPort which reserves the port on each node in the cluster for the specific container. This declaration restricts multiple containers with the same hostPort to be deployed on the same nodes in the cluster and restricts the scale of pods to the number of nodes you have in the cluster. Another runtime dependency is file storage for saving the application state. Kubernetes offers Pod-level storage utilities that are capable of surviving container restarts. Applications needing to read or write to these storage mechanisms will require nodes that is provided the type of volume required by the application. If there are no nodes available with the required volume type, then the pod will not be scheduled to be deployed at all. A different kind of dependency are configurations. ConfigMaps are used by Kubernetes to strategically plan out how to consume its settings through either environment variables or the filesystem. Secrets are consumed the same was as a ConfigMap in Kubernetes. Secrets are a more secure way to distribute environment-specific configurations to containers within the pod. Resource Profiles \u00b6 Resource Profiles are definitions for the compute resources required for a container. Resources are categorized in two ways, compressible and incompressible. Compressible resources include resources that can be throttled such as CPU or network bandwidth. Incompressible represents resources that can't be throttled such as memory where there is no other way to release the allocated resource other than killing the container. The difference between compressible and incompressible is very important when it comes to planning the deployment of pods and containers since the resource allocation can be affected by the limits of each. Every application needs to have a specified minimum and maximum amount of resources that are needed. The minimum amount is called \"requests\" and the maximum is the \"limits\". The scheduler uses the requests to determine the assignment of pods to nodes ensuring that the node will have enough capacity to accommodate the pod and all of it's containers required resources. An example of defined resource limits is below: Different levels of Quality of Service (QoS) are offered based on the specified requests and limits. .3 Quality of Service Levels Best Effort;; Lowest priority pod with no requests or limits set for it's containers. These pods will be the first of any pods killed if resources run low. Burstable;; Limits and requests are defined but they are not equal. The pod will use the minimum amount of resources, but will consume more if needed up to the limit. If the needed resources become scarce then these pods will be killed if no Best Effort pods are left. Guaranteed;; Highest priority pods with an equal amount of requests and limits. These pods will be the last to be killed if resources run low and no Best Effort or Burstable pods are left. Pod Priority \u00b6 The priority of pods can be defined through a PriorityClass object. The PriorityClass object allows developers to indicate the importance of a pod relative to the other pods in the cluster. The higher the priority number then the higher the priority of the pod. The scheduler looks at a pods priorityClassName to populate the priority of new pods. As pods are being placed in the scheduling queue for deployment, the scheduler orders them from highest to lowest. Another key feature for pod priority is the Preemption feature. The Preemption feature occurs when there are no nodes with enough capacity to place a pod. If this occurs the scheduler can preempt (remove) lower-priority Pods from nodes to free up resources and place Pods with higher priority. This effectively allows system administrators the ability to control which critical pods get top priority for resources in the cluster as well as controlling which critical workloads are able to be run on the cluster first. If a pod can not be scheduled due to constraints it will continue on with lower-priority nodes. Pod Priority should be used with caution for this gives users the ability to control over the kubernetes scheduler and ability to place or kill pods that may interrupt the cluster's critical functions. New pods with higher priority than others can quickly evict pods with lower priority that may be critical to a container's performance. ResourceQuota and PodDisruptionBudget are two tools that help combat this from happening read more here. Declarative Deployment Pattern \u00b6 With a growing number of microservices, reliance on an updating process for the services has become ever more important. Upgrading services is usually accompanied with some downtime for users or an increase in resource usage. Both of these can lead to an error effecting the performance of the application making the release process a bottleneck. A way to combat this issue in Kubernetes is through the use of Deployments. There are different approaches to the updating process that we will cover below. Any of these approaches can be put to use in order to save time for developers during their release cycles which can last from a few minutes to a few months. Rolling Deployment \u00b6 A Rolling Deployment ensures that there is no downtime during the update process. Kubernetes creates a new ReplicaSet for the new version of the service to be rolled out. From there Kubernetes creates set of pods of the new version while leaving the old pods running. Once the new pods are all up and running they will replace the old pods and become the primary pods users access. The upside to this approach is that there is no downtime and the deployment is handled by kubernetes through a deployment like the one below. The downside is with two sets of pods running at one time there is a higher usage of resources that may lead to performance issues for users. Fixed Deployment \u00b6 A Fixed Deployment uses the Recreate strategy which sets the maxUnavailable setting to the number of declared replicas. This in effect starts the versions of the pods as the old versions are being killed. The starting and stopping of containers does create a little bit of downtime for customers while the starting and stopping is taking place, but the positive side is the users will only have to handle one version at a time. Blue-Green Release \u00b6 A Blue-Green Release involves a manual process of creating a second deployment of pods with the newest version of the application running as well as keeping the old version of pods running in the cluster. Once the new pods are up and running properly the administrator shifts the traffic over to the new pods. Below is a diagram showing both versions up and running with the traffic going to the newer (green) pods. The downfall to this approach is the use of resources with two separate groups of pods running at the same time which could cause performance issues or complications. However, the advantage of this approach is users only experience one version at a time and it's easy to quickly switch back to the old version with no downtime if an issue arises with the newer version. Canary Release \u00b6 A Canary Release involves only standing up one pod of the new application code and shifting only a limited amount of new users traffic to that pod. This approach reduces the number of people exposed to the new service allowing the administrator to see how the new version is performing. Once the team feels comfortable with the performance of the new service then more pods can be stood up to replace the old pods. An advantage to this approach is no downtime with any of the services as the new service is being scaled. Health Probe Pattern \u00b6 The Health Probe pattern revolves the health of applications being communicated to Kubernetes. To be fully-automatable, cloud-applications must be highly observable in order for Kubernetes to know which applications are up and ready to receive traffic and which cannot. Kubernetes can use that information for traffic direction, self-healing, and to achieve the desired state of the application. Process Health Checks \u00b6 The simplest health check in Kubernetes is the Process Health Check. Kubernetes simply probes the application's processes to see if they are running or not. The process check tells Kubernetes when a process for an application needs to be restarted or shut down in the case of a failure. Liveness Probes \u00b6 A Liveness Probe is performed by the Kubernetes Kubelet agent and asks the container to confirm it's health. A simple process check can return that the container is healthy, but the container to users may not be performing correctly. The liveness probe addresses this issue but asking the container for its health from outside of the container itself. If a failure is found it may require that the container be restarted to get back to normal health. A liveness probe can perform the following actions to check health: HTTP GET and expects a success which is code 200-399. A TCP Socket Probe and expects a successful connection. A Exec Probe which executes a command and expects a successful exit code (0). The action chosen to be performed for testing depends on the nature of the application and which action fits best. Always keep in mind that a failing health check results in a restart of the container from Kubernetes, so make sure the right health check is in place if the underlying issue can't be fixed. Readiness Probes \u00b6 A Readiness Probe is very similar to a Liveness probe, but the resulting action to a failed Readiness probe is different. When a liveness probe fails the container is restarted and, in some scenarios, a simple restart won't fix the issue, which is where a readiness probe comes in. A failed readiness probe won't restart the container but will disconnect it from the traffic endpoint. Removing a container from traffic allows it to get up and running smoothly before being tossed into service unready to handle requests from users. Readiness probes give an application time to catch up and make itself ready again to handle more traffic versus shutting down completely and simply creating a new pod. In most cases, liveness and readiness probes are run together on the same application to make sure that the container has time to get up and running properly as well as stays healthy enough to handle the traffic. Managed Lifecycle Pattern \u00b6 The Managed Lifecycle pattern describes how containers need to adapt their lifecycle based on the events that are communicated from a managing platform such as Kubernetes. Containers do not have control of their own lifecycle. It's the managing platforms that allow them to live or die, get traffic or have none, etc. This pattern covers how the different events can affect those lifecycle decisions. SIGTERM \u00b6 The SIGTERM is a signal that is sent from the managing platform to a container or pod that instructs the pod or container to shutdown or restart. This signal can be sent due to a failed liveness test or a failure inside the container. SIGKILL allows the container to cleaning and properly shut itself down versus SIGKILL, which we will get to next. Once received, the application will shutdown as quickly as it can, allowing other processes to stop properly and cleaning up other files. Each application will have a different shutdown time based on the tasks needed to be done. SIGKILL \u00b6 SIGKILL is a signal sent to a container or pod forcing it to shutdown. A SIGKILL is normally sent after the SIGTERM signal. There is a default 30 second grace period between the time that SIGTERM is sent to the application and SIGKILL is sent. The grace period can be adjusted for each pod using the .spec.terminationGracePeriodSeconds field. The overall goal for containerized applications should be aimed to have designed and implemented quick startup and shutdown operations. postStart \u00b6 The postStart hook is a command that is run after the creation of a container and begins asynchronously with the container's primary process. PostStart is put in place in order to give the container time to warm up and check itself during startup. During the postStart loop the container will be labeled in \"pending\" mode in kubernetes while running through it's initial processes. If the postStart function errors out it will do so with a nonzero exit code and the container process will be killed by Kubernetes. Careful planning must be done when deciding what logic goes into the postStart function because if it fails the container will also fail to start. Both postStart and preStop have two handler types that they run: exec: Runs a command directly in the container. httpGet: Executes an HTTP GET request against an opened port on the pod container. preStop \u00b6 The preStop hook is a call that blocks a container from terminating too quickly and makes sure the container has a graceful shutdown. The preStop call must finish before the container is deleted by the container runtime. The preStop signal does not stop the container from being deleted completely, it is only an alternative to a SIGTERM signal for a graceful shutdown.","title":"Kubernetes 'overview'"},{"location":"developer-foundation/k8s-overview/#introduction","text":"Kubernetes is an open source container orchestration platform that automates deployment, management and scaling of applications. Learn how Kubernetes enables cost-effective Cloud-Native development.","title":"Introduction"},{"location":"developer-foundation/k8s-overview/#what-is-kubernetes","text":"Kubernetes\u2014also known as \u2018k8s\u2019 or \u2018kube\u2019\u2014is a container orchestration platform for scheduling and automating the deployment, management, and scaling of containerized applications. Kubernetes was first developed by engineers at Google before being open sourced in 2014. It is a descendant of \u2018Borg,\u2019 a container orchestration platform used internally at Google. (Kubernetes is Greek for helmsman or pilot, hence the helm in the Kubernetes logo .) Today, Kubernetes and the broader container ecosystem are maturing into a general-purpose computing platform and ecosystem that rivals\u2014if not surpasses\u2014virtual machines (VMs) as the basic building blocks of modern cloud infrastructure and applications. This ecosystem enables organizations to deliver a high-productivity Platform-as-a-Service (PaaS) that addresses multiple infrastructure- and operations-related tasks and issues surrounding Cloud-Native development so that development teams can focus solely on coding and innovation. https://www.ibm.com/cloud/learn/kubernetes","title":"What is Kubernetes?"},{"location":"developer-foundation/k8s-overview/#predictable-demands-pattern","text":"An application's performance, efficiency, and behaviors are reliant upon it's ability to have the appropriate allocation of resources. The Predictable Demands pattern is based on declaring the dependencies and resources needed by a given application. The scheduler will prioritize an application with a defined set of resources and dependencies since it can better manage the workload across nodes in the cluster. Each application has a different set of dependencies which we will touch on next.","title":"Predictable Demands Pattern"},{"location":"developer-foundation/k8s-overview/#runtime-dependencies","text":"One of the most common runtime dependency is the exposure of a container's specific port through hostPort. Different applications can specify the same port through hostPort which reserves the port on each node in the cluster for the specific container. This declaration restricts multiple containers with the same hostPort to be deployed on the same nodes in the cluster and restricts the scale of pods to the number of nodes you have in the cluster. Another runtime dependency is file storage for saving the application state. Kubernetes offers Pod-level storage utilities that are capable of surviving container restarts. Applications needing to read or write to these storage mechanisms will require nodes that is provided the type of volume required by the application. If there are no nodes available with the required volume type, then the pod will not be scheduled to be deployed at all. A different kind of dependency are configurations. ConfigMaps are used by Kubernetes to strategically plan out how to consume its settings through either environment variables or the filesystem. Secrets are consumed the same was as a ConfigMap in Kubernetes. Secrets are a more secure way to distribute environment-specific configurations to containers within the pod.","title":"Runtime Dependencies"},{"location":"developer-foundation/k8s-overview/#resource-profiles","text":"Resource Profiles are definitions for the compute resources required for a container. Resources are categorized in two ways, compressible and incompressible. Compressible resources include resources that can be throttled such as CPU or network bandwidth. Incompressible represents resources that can't be throttled such as memory where there is no other way to release the allocated resource other than killing the container. The difference between compressible and incompressible is very important when it comes to planning the deployment of pods and containers since the resource allocation can be affected by the limits of each. Every application needs to have a specified minimum and maximum amount of resources that are needed. The minimum amount is called \"requests\" and the maximum is the \"limits\". The scheduler uses the requests to determine the assignment of pods to nodes ensuring that the node will have enough capacity to accommodate the pod and all of it's containers required resources. An example of defined resource limits is below: Different levels of Quality of Service (QoS) are offered based on the specified requests and limits. .3 Quality of Service Levels Best Effort;; Lowest priority pod with no requests or limits set for it's containers. These pods will be the first of any pods killed if resources run low. Burstable;; Limits and requests are defined but they are not equal. The pod will use the minimum amount of resources, but will consume more if needed up to the limit. If the needed resources become scarce then these pods will be killed if no Best Effort pods are left. Guaranteed;; Highest priority pods with an equal amount of requests and limits. These pods will be the last to be killed if resources run low and no Best Effort or Burstable pods are left.","title":"Resource Profiles"},{"location":"developer-foundation/k8s-overview/#pod-priority","text":"The priority of pods can be defined through a PriorityClass object. The PriorityClass object allows developers to indicate the importance of a pod relative to the other pods in the cluster. The higher the priority number then the higher the priority of the pod. The scheduler looks at a pods priorityClassName to populate the priority of new pods. As pods are being placed in the scheduling queue for deployment, the scheduler orders them from highest to lowest. Another key feature for pod priority is the Preemption feature. The Preemption feature occurs when there are no nodes with enough capacity to place a pod. If this occurs the scheduler can preempt (remove) lower-priority Pods from nodes to free up resources and place Pods with higher priority. This effectively allows system administrators the ability to control which critical pods get top priority for resources in the cluster as well as controlling which critical workloads are able to be run on the cluster first. If a pod can not be scheduled due to constraints it will continue on with lower-priority nodes. Pod Priority should be used with caution for this gives users the ability to control over the kubernetes scheduler and ability to place or kill pods that may interrupt the cluster's critical functions. New pods with higher priority than others can quickly evict pods with lower priority that may be critical to a container's performance. ResourceQuota and PodDisruptionBudget are two tools that help combat this from happening read more here.","title":"Pod Priority"},{"location":"developer-foundation/k8s-overview/#declarative-deployment-pattern","text":"With a growing number of microservices, reliance on an updating process for the services has become ever more important. Upgrading services is usually accompanied with some downtime for users or an increase in resource usage. Both of these can lead to an error effecting the performance of the application making the release process a bottleneck. A way to combat this issue in Kubernetes is through the use of Deployments. There are different approaches to the updating process that we will cover below. Any of these approaches can be put to use in order to save time for developers during their release cycles which can last from a few minutes to a few months.","title":"Declarative Deployment Pattern"},{"location":"developer-foundation/k8s-overview/#rolling-deployment","text":"A Rolling Deployment ensures that there is no downtime during the update process. Kubernetes creates a new ReplicaSet for the new version of the service to be rolled out. From there Kubernetes creates set of pods of the new version while leaving the old pods running. Once the new pods are all up and running they will replace the old pods and become the primary pods users access. The upside to this approach is that there is no downtime and the deployment is handled by kubernetes through a deployment like the one below. The downside is with two sets of pods running at one time there is a higher usage of resources that may lead to performance issues for users.","title":"Rolling Deployment"},{"location":"developer-foundation/k8s-overview/#fixed-deployment","text":"A Fixed Deployment uses the Recreate strategy which sets the maxUnavailable setting to the number of declared replicas. This in effect starts the versions of the pods as the old versions are being killed. The starting and stopping of containers does create a little bit of downtime for customers while the starting and stopping is taking place, but the positive side is the users will only have to handle one version at a time.","title":"Fixed Deployment"},{"location":"developer-foundation/k8s-overview/#blue-green-release","text":"A Blue-Green Release involves a manual process of creating a second deployment of pods with the newest version of the application running as well as keeping the old version of pods running in the cluster. Once the new pods are up and running properly the administrator shifts the traffic over to the new pods. Below is a diagram showing both versions up and running with the traffic going to the newer (green) pods. The downfall to this approach is the use of resources with two separate groups of pods running at the same time which could cause performance issues or complications. However, the advantage of this approach is users only experience one version at a time and it's easy to quickly switch back to the old version with no downtime if an issue arises with the newer version.","title":"Blue-Green Release"},{"location":"developer-foundation/k8s-overview/#canary-release","text":"A Canary Release involves only standing up one pod of the new application code and shifting only a limited amount of new users traffic to that pod. This approach reduces the number of people exposed to the new service allowing the administrator to see how the new version is performing. Once the team feels comfortable with the performance of the new service then more pods can be stood up to replace the old pods. An advantage to this approach is no downtime with any of the services as the new service is being scaled.","title":"Canary Release"},{"location":"developer-foundation/k8s-overview/#health-probe-pattern","text":"The Health Probe pattern revolves the health of applications being communicated to Kubernetes. To be fully-automatable, cloud-applications must be highly observable in order for Kubernetes to know which applications are up and ready to receive traffic and which cannot. Kubernetes can use that information for traffic direction, self-healing, and to achieve the desired state of the application.","title":"Health Probe Pattern"},{"location":"developer-foundation/k8s-overview/#process-health-checks","text":"The simplest health check in Kubernetes is the Process Health Check. Kubernetes simply probes the application's processes to see if they are running or not. The process check tells Kubernetes when a process for an application needs to be restarted or shut down in the case of a failure.","title":"Process Health Checks"},{"location":"developer-foundation/k8s-overview/#liveness-probes","text":"A Liveness Probe is performed by the Kubernetes Kubelet agent and asks the container to confirm it's health. A simple process check can return that the container is healthy, but the container to users may not be performing correctly. The liveness probe addresses this issue but asking the container for its health from outside of the container itself. If a failure is found it may require that the container be restarted to get back to normal health. A liveness probe can perform the following actions to check health: HTTP GET and expects a success which is code 200-399. A TCP Socket Probe and expects a successful connection. A Exec Probe which executes a command and expects a successful exit code (0). The action chosen to be performed for testing depends on the nature of the application and which action fits best. Always keep in mind that a failing health check results in a restart of the container from Kubernetes, so make sure the right health check is in place if the underlying issue can't be fixed.","title":"Liveness Probes"},{"location":"developer-foundation/k8s-overview/#readiness-probes","text":"A Readiness Probe is very similar to a Liveness probe, but the resulting action to a failed Readiness probe is different. When a liveness probe fails the container is restarted and, in some scenarios, a simple restart won't fix the issue, which is where a readiness probe comes in. A failed readiness probe won't restart the container but will disconnect it from the traffic endpoint. Removing a container from traffic allows it to get up and running smoothly before being tossed into service unready to handle requests from users. Readiness probes give an application time to catch up and make itself ready again to handle more traffic versus shutting down completely and simply creating a new pod. In most cases, liveness and readiness probes are run together on the same application to make sure that the container has time to get up and running properly as well as stays healthy enough to handle the traffic.","title":"Readiness Probes"},{"location":"developer-foundation/k8s-overview/#managed-lifecycle-pattern","text":"The Managed Lifecycle pattern describes how containers need to adapt their lifecycle based on the events that are communicated from a managing platform such as Kubernetes. Containers do not have control of their own lifecycle. It's the managing platforms that allow them to live or die, get traffic or have none, etc. This pattern covers how the different events can affect those lifecycle decisions.","title":"Managed Lifecycle Pattern"},{"location":"developer-foundation/k8s-overview/#sigterm","text":"The SIGTERM is a signal that is sent from the managing platform to a container or pod that instructs the pod or container to shutdown or restart. This signal can be sent due to a failed liveness test or a failure inside the container. SIGKILL allows the container to cleaning and properly shut itself down versus SIGKILL, which we will get to next. Once received, the application will shutdown as quickly as it can, allowing other processes to stop properly and cleaning up other files. Each application will have a different shutdown time based on the tasks needed to be done.","title":"SIGTERM"},{"location":"developer-foundation/k8s-overview/#sigkill","text":"SIGKILL is a signal sent to a container or pod forcing it to shutdown. A SIGKILL is normally sent after the SIGTERM signal. There is a default 30 second grace period between the time that SIGTERM is sent to the application and SIGKILL is sent. The grace period can be adjusted for each pod using the .spec.terminationGracePeriodSeconds field. The overall goal for containerized applications should be aimed to have designed and implemented quick startup and shutdown operations.","title":"SIGKILL"},{"location":"developer-foundation/k8s-overview/#poststart","text":"The postStart hook is a command that is run after the creation of a container and begins asynchronously with the container's primary process. PostStart is put in place in order to give the container time to warm up and check itself during startup. During the postStart loop the container will be labeled in \"pending\" mode in kubernetes while running through it's initial processes. If the postStart function errors out it will do so with a nonzero exit code and the container process will be killed by Kubernetes. Careful planning must be done when deciding what logic goes into the postStart function because if it fails the container will also fail to start. Both postStart and preStop have two handler types that they run: exec: Runs a command directly in the container. httpGet: Executes an HTTP GET request against an opened port on the pod container.","title":"postStart"},{"location":"developer-foundation/k8s-overview/#prestop","text":"The preStop hook is a call that blocks a container from terminating too quickly and makes sure the container has a graceful shutdown. The preStop call must finish before the container is deleted by the container runtime. The preStop signal does not stop the container from being deleted completely, it is only an alternative to a SIGTERM signal for a graceful shutdown.","title":"preStop"},{"location":"developer-foundation/k8s-pod-design/","text":"Labels, Selectors, and Annotations \u00b6 Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object. You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata. You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. Resources \u00b6 OpenShift - CLI Label Commands IKS - Labels - Annotations References \u00b6 apiVersion : v1 kind : Pod metadata : name : my-pod labels : app : foo tier : frontend env : dev annotations : imageregistry : \"https://hub.docker.com/\" gitrepo : \"https://github.com/csantanapr/knative\" spec : containers : - name : app image : bitnami/nginx apiVersion : v1 kind : Pod metadata : name : my-pod spec : restartPolicy : Never containers : - name : app image : busybox nodeSelector : disk : ssd OpenShift Change Labels on Objects oc label <objectname> Getting Pods based on their labels. oc get pods --show-labels oc get pods -L tier,env oc get pods -l app oc get pods -l tier=frontend oc get pods -l 'env=dev,tier=frontend' oc get pods -l 'env in (dev, test)' oc get pods -l 'tier!=backend' oc get pods -l 'env,env notin (prod)' IKS Create the Pod kubectl create -f pod.yaml Update label in the YAML file and reapply it. kubectl apply -f pod.yaml You can edit the labels as well. kubectl edit pod myapp-pod Getting Pods based on their labels. kubectl get pods --show-labels kubectl get pods -L tier,env kubectl get pods -l app kubectl get pods -l tier=frontend kubectl get pods -l 'env=dev,tier=frontend' kubectl get pods -l 'env in (dev, test)' kubectl get pods -l 'tier!=backend' kubectl get pods -l 'env,env notin (prod)' Delete the Pod. kubectl delete pod myapp-pod Deployments \u00b6 A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. The following are typical use cases for Deployments: - Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not. - Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment. - Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment. - Scale up the Deployment to facilitate more load. - Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout. - Use the status of the Deployment as an indicator that a rollout has stuck. - Clean up older ReplicaSets that you don\u2019t need anymore. Resources \u00b6 OpenShift Deployments Managing Deployment Processes DeploymentConfig Strategies Route Based Deployment Strategies IKS Deployments Scaling Deployments References \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : my-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : bitnami/nginx:1.16.0 ports : - containerPort : 8080 OpenShift Creates a Deployment oc apply -f <deploymentYAML> Gets Deployments oc get deploy my-deployment Gets the deployments description oc describe deployment my-deployment Edit the deployment oc edit deployment my-deployment Scale the deployment oc scale deployment/my-deployment --replicas=4 Delete the deployment oc delete my-deployment IKS Creates a Deployment kubectl apply -f <deploymentYAML> Get the deployment kubectl get deployment my-deployment Describe the deployment kubectl describe deployment my-deployment Edit the deployment kubectl edit deployent my-deployment Scale the deployment kubectl scale deployment/my-deployment --replicas=4 Delete the deployment kubectl delete my-deployment Deployments rolling updates and rollback \u00b6 Updating a Deployment A Deployment\u2019s rollout is triggered if and only if the Deployment\u2019s Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout. Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0. Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications. Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment\u2019s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit). A Deployment\u2019s revision is created when a Deployment\u2019s rollout is triggered. This means that the new revision is created if and only if the Deployment\u2019s Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment\u2019s Pod template part is rolled back. Resources \u00b6 OpenShift Rollout Rolling Back IKS Updating a Deployment Rolling Back a Deployment References \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : my-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : bitnami/nginx:1.16.0 ports : - containerPort : 8080 OpenShift Get Deployments oc get deployments Sets new image for Deployment oc set image deployment/my-deployment nginx=nginx:1.16.1 --record Get ReplicaSets oc get rs Get Deployment Description oc describe deployment my-deployment Check the status of the rollout oc rollout status my-deployment Get Rollout History oc rollout history deployment my-deployment Undo Rollout oc rollback my-deployment IKS Get Deployments kubectl get deployments Sets new image for Deployment kubectl set image deployment/my-deployment nginx=nginx:1.16.1 --record Get ReplicaSets kubectl get rs Get Deployment Description kubectl describe deployment my-deployment Check the status of the rollout oc rollout status my-deployment Get Rollout History kubectl rollout history deployment my-deployment Undo Rollout kubectl rollout undo deployment my-deployment Jobs and CronJobs \u00b6 Jobs A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. CronJobs One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format. All CronJob schedule: times are based on the timezone of the master where the job is initiated. Resources \u00b6 OpenShift - Jobs - CronJobs IKS - Jobs to Completion - Cron Jobs - Automated Tasks with Cron References \u00b6 It computes \u03c0 to 2000 places and prints it out apiVersion : batch/v1 kind : Job metadata : name : pi spec : template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4 Running in parallel apiVersion : batch/v1 kind : Job metadata : name : pi spec : parallelism : 2 completions : 3 template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4 apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure OpenShift Gets Jobs oc get jobs Gets Job Description oc describe job pi Gets Pods from the Job oc get pods Deletes Job oc delete job pi Gets CronJob oc get cronjobs Describes CronJob oc describe cronjobs pi Gets Pods from CronJob oc get pods Deletes CronJob oc delete cronjobs pi IKS Gets Jobs kubectl get jobs Gets Job Description kubectl describe job pi Gets Pods from the Job kubectl get pods Deletes Job kubectl delete job pi Gets CronJob kubectl get cronjobs Describes CronJob kubectl describe cronjobs pi Gets Pods from CronJob kubectl get pods Deletes CronJob kubectl delete cronjobs pi Activities \u00b6 Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application. Rolling Updates Cron Jobs Lab Using Tekton to test new versions of applications. Crons Jobs","title":"Pod Design"},{"location":"developer-foundation/k8s-pod-design/#labels-selectors-and-annotations","text":"Labels are key/value pairs that are attached to objects, such as pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object. You can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata. You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.","title":"Labels, Selectors, and Annotations"},{"location":"developer-foundation/k8s-pod-design/#resources","text":"OpenShift - CLI Label Commands IKS - Labels - Annotations","title":"Resources"},{"location":"developer-foundation/k8s-pod-design/#references","text":"apiVersion : v1 kind : Pod metadata : name : my-pod labels : app : foo tier : frontend env : dev annotations : imageregistry : \"https://hub.docker.com/\" gitrepo : \"https://github.com/csantanapr/knative\" spec : containers : - name : app image : bitnami/nginx apiVersion : v1 kind : Pod metadata : name : my-pod spec : restartPolicy : Never containers : - name : app image : busybox nodeSelector : disk : ssd OpenShift Change Labels on Objects oc label <objectname> Getting Pods based on their labels. oc get pods --show-labels oc get pods -L tier,env oc get pods -l app oc get pods -l tier=frontend oc get pods -l 'env=dev,tier=frontend' oc get pods -l 'env in (dev, test)' oc get pods -l 'tier!=backend' oc get pods -l 'env,env notin (prod)' IKS Create the Pod kubectl create -f pod.yaml Update label in the YAML file and reapply it. kubectl apply -f pod.yaml You can edit the labels as well. kubectl edit pod myapp-pod Getting Pods based on their labels. kubectl get pods --show-labels kubectl get pods -L tier,env kubectl get pods -l app kubectl get pods -l tier=frontend kubectl get pods -l 'env=dev,tier=frontend' kubectl get pods -l 'env in (dev, test)' kubectl get pods -l 'tier!=backend' kubectl get pods -l 'env,env notin (prod)' Delete the Pod. kubectl delete pod myapp-pod","title":"References"},{"location":"developer-foundation/k8s-pod-design/#deployments","text":"A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. The following are typical use cases for Deployments: - Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not. - Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment. - Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment. - Scale up the Deployment to facilitate more load. - Pause the Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout. - Use the status of the Deployment as an indicator that a rollout has stuck. - Clean up older ReplicaSets that you don\u2019t need anymore.","title":"Deployments"},{"location":"developer-foundation/k8s-pod-design/#resources_1","text":"OpenShift Deployments Managing Deployment Processes DeploymentConfig Strategies Route Based Deployment Strategies IKS Deployments Scaling Deployments","title":"Resources"},{"location":"developer-foundation/k8s-pod-design/#references_1","text":"apiVersion : apps/v1 kind : Deployment metadata : name : my-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : bitnami/nginx:1.16.0 ports : - containerPort : 8080 OpenShift Creates a Deployment oc apply -f <deploymentYAML> Gets Deployments oc get deploy my-deployment Gets the deployments description oc describe deployment my-deployment Edit the deployment oc edit deployment my-deployment Scale the deployment oc scale deployment/my-deployment --replicas=4 Delete the deployment oc delete my-deployment IKS Creates a Deployment kubectl apply -f <deploymentYAML> Get the deployment kubectl get deployment my-deployment Describe the deployment kubectl describe deployment my-deployment Edit the deployment kubectl edit deployent my-deployment Scale the deployment kubectl scale deployment/my-deployment --replicas=4 Delete the deployment kubectl delete my-deployment","title":"References"},{"location":"developer-foundation/k8s-pod-design/#deployments-rolling-updates-and-rollback","text":"Updating a Deployment A Deployment\u2019s rollout is triggered if and only if the Deployment\u2019s Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout. Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0. Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications. Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment\u2019s rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit). A Deployment\u2019s revision is created when a Deployment\u2019s rollout is triggered. This means that the new revision is created if and only if the Deployment\u2019s Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment\u2019s Pod template part is rolled back.","title":"Deployments rolling updates and rollback"},{"location":"developer-foundation/k8s-pod-design/#resources_2","text":"OpenShift Rollout Rolling Back IKS Updating a Deployment Rolling Back a Deployment","title":"Resources"},{"location":"developer-foundation/k8s-pod-design/#references_2","text":"apiVersion : apps/v1 kind : Deployment metadata : name : my-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : bitnami/nginx:1.16.0 ports : - containerPort : 8080 OpenShift Get Deployments oc get deployments Sets new image for Deployment oc set image deployment/my-deployment nginx=nginx:1.16.1 --record Get ReplicaSets oc get rs Get Deployment Description oc describe deployment my-deployment Check the status of the rollout oc rollout status my-deployment Get Rollout History oc rollout history deployment my-deployment Undo Rollout oc rollback my-deployment IKS Get Deployments kubectl get deployments Sets new image for Deployment kubectl set image deployment/my-deployment nginx=nginx:1.16.1 --record Get ReplicaSets kubectl get rs Get Deployment Description kubectl describe deployment my-deployment Check the status of the rollout oc rollout status my-deployment Get Rollout History kubectl rollout history deployment my-deployment Undo Rollout kubectl rollout undo deployment my-deployment","title":"References"},{"location":"developer-foundation/k8s-pod-design/#jobs-and-cronjobs","text":"Jobs A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. CronJobs One CronJob object is like one line of a crontab (cron table) file. It runs a job periodically on a given schedule, written in Cron format. All CronJob schedule: times are based on the timezone of the master where the job is initiated.","title":"Jobs and CronJobs"},{"location":"developer-foundation/k8s-pod-design/#resources_3","text":"OpenShift - Jobs - CronJobs IKS - Jobs to Completion - Cron Jobs - Automated Tasks with Cron","title":"Resources"},{"location":"developer-foundation/k8s-pod-design/#references_3","text":"It computes \u03c0 to 2000 places and prints it out apiVersion : batch/v1 kind : Job metadata : name : pi spec : template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4 Running in parallel apiVersion : batch/v1 kind : Job metadata : name : pi spec : parallelism : 2 completions : 3 template : spec : containers : - name : pi image : perl command : [ \"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ] restartPolicy : Never backoffLimit : 4 apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure OpenShift Gets Jobs oc get jobs Gets Job Description oc describe job pi Gets Pods from the Job oc get pods Deletes Job oc delete job pi Gets CronJob oc get cronjobs Describes CronJob oc describe cronjobs pi Gets Pods from CronJob oc get pods Deletes CronJob oc delete cronjobs pi IKS Gets Jobs kubectl get jobs Gets Job Description kubectl describe job pi Gets Pods from the Job kubectl get pods Deletes Job kubectl delete job pi Gets CronJob kubectl get cronjobs Describes CronJob kubectl describe cronjobs pi Gets Pods from CronJob kubectl get pods Deletes CronJob kubectl delete cronjobs pi","title":"References"},{"location":"developer-foundation/k8s-pod-design/#activities","text":"Task Description Link Try It Yourself Rolling Updates Lab Create a Rolling Update for your application. Rolling Updates Cron Jobs Lab Using Tekton to test new versions of applications. Crons Jobs","title":"Activities"},{"location":"developer-foundation/k8s-services-networking/","text":"Services \u00b6 An abstract way to expose an application running on a set of Pods as a network service. Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically. Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later. In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector). If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes. For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods. Resources \u00b6 IKS & OpenShift - Services - Exposing Services References \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : my-deployment labels : app : nginx version : v1 spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx version : v1 spec : containers : - name : nginx image : bitnami/nginx ports : - containerPort : 8080 name : http --- apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : nginx ports : - name : http port : 80 targetPort : http OpenShift Get Service oc get svc Get Service Description oc describe svc my-service Expose a service oc expose service <service_name> Get Route for the Service oc get route IKS Get Service kubectl get svc Get Service Description kubectl describe svc my-service Get Service Endpoints kubectl get ep my-service Expose a Deployment via a Service kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort Routes \u00b6 (OpenShift Only) Routes are Openshift objects that expose services for external clients to reach them by name. Routes can be insecure or secured on creation using certificates. The new route inherits the name from the service unless you specify one using the --name option. Resources \u00b6 OpenShift - Routes - Route Configuration - Secured Routes References \u00b6 Route Creation apiVersion: v1 kind: Route metadata: name: frontend spec: to: kind: Service name: frontend Secured Route Creation apiVersion: v1 kind: Route metadata: name: frontend spec: to: kind: Service name: frontend tls: termination: edge Commands \u00b6 OpenShift Create Route from YAML oc apply -f route.yaml Get Route oc get route Describe Route oc get route <route-name> Get Route YAML oc get route <route-name> -o yaml Ingress \u00b6 An API object that manages external access to the services in a cluster, typically HTTP. Ingress can provide load balancing, SSL termination and name-based virtual hosting. Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. Resources \u00b6 OpenShift - Ingress Operator - Using Ingress Controllers IKS - Ingress - Ingress Controllers - Minikube Ingress References \u00b6 apiVersion : networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1 kind : Ingress metadata : name : example-ingress spec : rules : - host : hello-world.info http : paths : - path : / backend : serviceName : web servicePort : 8080 Openshift View Ingress Status oc describe clusteroperators/ingress Describe default Ingress Controller oc describe --namespace=openshift-ingress-operator ingresscontroller/default IKS minikube addons enable ingress kubectl get pods -n kube-system | grep ingress kubectl run web --image=bitnami/nginx --port=8080 kubectl expose deployment web --target-port=8080 --type=NodePort kubectl get svc web minikube service --url web stern ingress -n kube-system kubectl get ingress kubcetl describe ingress example-ingress curl hello-world.info --resolve hello-world.info:80:<ADDRESS> Activities \u00b6 Task Description Link Try It Yourself Creating Services Create two services with certain requirements. Setting up Services","title":"Services & Networking"},{"location":"developer-foundation/k8s-services-networking/#services","text":"An abstract way to expose an application running on a set of Pods as a network service. Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically. Each Pod gets its own IP address, however in a Deployment, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later. In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service). The set of Pods targeted by a Service is usually determined by a selector (see below for why you might want a Service without a selector). If you\u2019re able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints, that get updated whenever the set of Pods in a Service changes. For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.","title":"Services"},{"location":"developer-foundation/k8s-services-networking/#resources","text":"IKS & OpenShift - Services - Exposing Services","title":"Resources"},{"location":"developer-foundation/k8s-services-networking/#references","text":"apiVersion : apps/v1 kind : Deployment metadata : name : my-deployment labels : app : nginx version : v1 spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx version : v1 spec : containers : - name : nginx image : bitnami/nginx ports : - containerPort : 8080 name : http --- apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : nginx ports : - name : http port : 80 targetPort : http OpenShift Get Service oc get svc Get Service Description oc describe svc my-service Expose a service oc expose service <service_name> Get Route for the Service oc get route IKS Get Service kubectl get svc Get Service Description kubectl describe svc my-service Get Service Endpoints kubectl get ep my-service Expose a Deployment via a Service kubectl expose deployment my-deployment --port 80 --target-port=http --selector app=nginx --name my-service-2 --type NodePort","title":"References"},{"location":"developer-foundation/k8s-services-networking/#routes","text":"(OpenShift Only) Routes are Openshift objects that expose services for external clients to reach them by name. Routes can be insecure or secured on creation using certificates. The new route inherits the name from the service unless you specify one using the --name option.","title":"Routes"},{"location":"developer-foundation/k8s-services-networking/#resources_1","text":"OpenShift - Routes - Route Configuration - Secured Routes","title":"Resources"},{"location":"developer-foundation/k8s-services-networking/#references_1","text":"Route Creation apiVersion: v1 kind: Route metadata: name: frontend spec: to: kind: Service name: frontend Secured Route Creation apiVersion: v1 kind: Route metadata: name: frontend spec: to: kind: Service name: frontend tls: termination: edge","title":"References"},{"location":"developer-foundation/k8s-services-networking/#commands","text":"OpenShift Create Route from YAML oc apply -f route.yaml Get Route oc get route Describe Route oc get route <route-name> Get Route YAML oc get route <route-name> -o yaml","title":"Commands"},{"location":"developer-foundation/k8s-services-networking/#ingress","text":"An API object that manages external access to the services in a cluster, typically HTTP. Ingress can provide load balancing, SSL termination and name-based virtual hosting. Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.","title":"Ingress"},{"location":"developer-foundation/k8s-services-networking/#resources_2","text":"OpenShift - Ingress Operator - Using Ingress Controllers IKS - Ingress - Ingress Controllers - Minikube Ingress","title":"Resources"},{"location":"developer-foundation/k8s-services-networking/#references_2","text":"apiVersion : networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1 kind : Ingress metadata : name : example-ingress spec : rules : - host : hello-world.info http : paths : - path : / backend : serviceName : web servicePort : 8080 Openshift View Ingress Status oc describe clusteroperators/ingress Describe default Ingress Controller oc describe --namespace=openshift-ingress-operator ingresscontroller/default IKS minikube addons enable ingress kubectl get pods -n kube-system | grep ingress kubectl run web --image=bitnami/nginx --port=8080 kubectl expose deployment web --target-port=8080 --type=NodePort kubectl get svc web minikube service --url web stern ingress -n kube-system kubectl get ingress kubcetl describe ingress example-ingress curl hello-world.info --resolve hello-world.info:80:<ADDRESS>","title":"References"},{"location":"developer-foundation/k8s-services-networking/#activities","text":"Task Description Link Try It Yourself Creating Services Create two services with certain requirements. Setting up Services","title":"Activities"},{"location":"developer-foundation/k8s-state-persistence/","text":"Volumes \u00b6 On-disk files in a Container are ephemeral, which presents some problems for non-trivial applications when running in Containers. First, when a Container crashes, kubelet will restart it, but the files will be lost - the Container starts with a clean state. Second, when running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems. Docker also has a concept of volumes, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another Container. A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the Pod that encloses it. Consequently, a volume outlives any Containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously. Resources \u00b6 OpenShift - Volume Lifecycle IKS - Volumes References \u00b6 apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] name : busybox volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : bitnami/nginx name : test-container volumeMounts : - mountPath : /test-pd name : test-volume volumes : - name : test-volume hostPath : # directory location on host path : /data # this field is optional type : Directory PersistentVolumes and PersistentVolumeClaims \u00b6 Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Claims can request specific size and access modes (e.g., they can be mounted once read/write or many times read-only). While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource. Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod\u2019s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod. PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \u201cMany\u201d modes (ROX, RWX) is only possible within one namespace. Resources \u00b6 OpenShift - Persistent Storage - Persistent Volume Types - Expanding Persistent Volumes IKS - Persistent Volumes - Writing Portable Configurations - Configuring Persistent Volume Storage References \u00b6 kind : PersistentVolume apiVersion : v1 metadata : name : my-pv spec : storageClassName : local-storage capacity : storage : 128Mi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data-1\" apiVersion : v1 kind : PersistentVolumeClaim metadata : name : my-pvc spec : storageClassName : local-storage accessModes : - ReadWriteOnce resources : requests : storage : 100Mi kind : Pod apiVersion : v1 metadata : name : my-pod spec : containers : - name : nginx image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! > /mnt/data/message.txt && sleep 3600' ] volumeMounts : - mountPath : \"/mnt/data\" name : my-data volumes : - name : my-data persistentVolumeClaim : claimName : my-pvc === \"OpenShift\" **Getting the Persistent Volumes in Project** ``` oc get pv ``` **Getting the Persistent Volume Claims ** ``` oc get pvc ``` **Getting a specific Persistent Volume** ``` oc get pv <pv-claim> ``` === \"IKS\" **Getting the PersistentVolume ** ``` kubectl get pv ``` **Getting the PersistentVolumeClaims ** ``` kubectl get pvc ``` Activities \u00b6 Task Description Link Try It Yourself Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes","title":"State Persistence"},{"location":"developer-foundation/k8s-state-persistence/#volumes","text":"On-disk files in a Container are ephemeral, which presents some problems for non-trivial applications when running in Containers. First, when a Container crashes, kubelet will restart it, but the files will be lost - the Container starts with a clean state. Second, when running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems. Docker also has a concept of volumes, though it is somewhat looser and less managed. In Docker, a volume is simply a directory on disk or in another Container. A Kubernetes volume, on the other hand, has an explicit lifetime - the same as the Pod that encloses it. Consequently, a volume outlives any Containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously.","title":"Volumes"},{"location":"developer-foundation/k8s-state-persistence/#resources","text":"OpenShift - Volume Lifecycle IKS - Volumes","title":"Resources"},{"location":"developer-foundation/k8s-state-persistence/#references","text":"apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! && sleep 3600' ] name : busybox volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : bitnami/nginx name : test-container volumeMounts : - mountPath : /test-pd name : test-volume volumes : - name : test-volume hostPath : # directory location on host path : /data # this field is optional type : Directory","title":"References"},{"location":"developer-foundation/k8s-state-persistence/#persistentvolumes-and-persistentvolumeclaims","text":"Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Claims can request specific size and access modes (e.g., they can be mounted once read/write or many times read-only). While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource. Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod\u2019s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod. PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \u201cMany\u201d modes (ROX, RWX) is only possible within one namespace.","title":"PersistentVolumes and PersistentVolumeClaims"},{"location":"developer-foundation/k8s-state-persistence/#resources_1","text":"OpenShift - Persistent Storage - Persistent Volume Types - Expanding Persistent Volumes IKS - Persistent Volumes - Writing Portable Configurations - Configuring Persistent Volume Storage","title":"Resources"},{"location":"developer-foundation/k8s-state-persistence/#references_1","text":"kind : PersistentVolume apiVersion : v1 metadata : name : my-pv spec : storageClassName : local-storage capacity : storage : 128Mi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data-1\" apiVersion : v1 kind : PersistentVolumeClaim metadata : name : my-pvc spec : storageClassName : local-storage accessModes : - ReadWriteOnce resources : requests : storage : 100Mi kind : Pod apiVersion : v1 metadata : name : my-pod spec : containers : - name : nginx image : busybox command : [ 'sh' , '-c' , 'echo Hello Kubernetes! > /mnt/data/message.txt && sleep 3600' ] volumeMounts : - mountPath : \"/mnt/data\" name : my-data volumes : - name : my-data persistentVolumeClaim : claimName : my-pvc === \"OpenShift\" **Getting the Persistent Volumes in Project** ``` oc get pv ``` **Getting the Persistent Volume Claims ** ``` oc get pvc ``` **Getting a specific Persistent Volume** ``` oc get pv <pv-claim> ``` === \"IKS\" **Getting the PersistentVolume ** ``` kubectl get pv ``` **Getting the PersistentVolumeClaims ** ``` kubectl get pvc ```","title":"References"},{"location":"developer-foundation/k8s-state-persistence/#activities","text":"Task Description Link Try It Yourself Setting up Persistent Volumes Create a Persistent Volume that's accessible from a SQL Pod. Setting up Persistent Volumes","title":"Activities"},{"location":"developer-foundation/k8s-troubleshooting/","text":"Debugging Applications \u00b6 Kubernetes provides tools to help troubleshoot and debug problems with applications. Usually is getting familiar with how primitives objects interact with each other, checking the status of objects, and finally checking logs for any last resource clues. Resources \u00b6 OpenShift - Debugging Applications - Debugging Metrics IKS - Debugging Applications - Debugging Services - Debugging Replication Controllers References \u00b6 OpenShift Broken setup to debug curl -sL https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s.64 | base64 -d | oc apply -f - Expose the service using port-forward oc port-forward service/my-service 8080:80 -n debug Try to access the service curl http://localhost:8080 Try Out these Commands to Debug oc get pods --all-namespaces oc get deployments oc describe pod oc explain Pod.spec.containers.resources.requests oc explain Pod.spec.containers.livenessProbe oc edit deployment oc logs oc get service oc get ep oc describe service oc get pods --show-labels oc get deployment --show-labels IKS Broken setup to debug curl -s https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/a92550dd499ebefc205fa2cd5c18123409186712/debugk8s.64 | base64 -d | kubectl apply -f - Expose the service using port-forward kubectl port-forward service/my-service 8080:80 -n debug Try to access the service curl http://localhost:8080 Try Out these Commands to Debug kubectl get pods --all-namespaces kubectl get deployments kubectl describe pod kubectl explain Pod.spec.containers.resources.requests kubectl explain Pod.spec.containers.livenessProbe kubectl edit deployment kubectl logs kubectl get service kubectl get ep kubectl describe service kubectl get pods --show-labels kubectl get deployment --show-labels Solution \u00b6 https://gist.github.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091#file-debugk8s-yaml Activities \u00b6 The continuous integration activities focus around Tekton the integration platform. These labs will show you how to build pipelines and test your code before deployment. Task Description Link Try It Yourself Debugging Find which service is breaking in your cluster and find out why. Debugging","title":"Troubleshoot"},{"location":"developer-foundation/k8s-troubleshooting/#debugging-applications","text":"Kubernetes provides tools to help troubleshoot and debug problems with applications. Usually is getting familiar with how primitives objects interact with each other, checking the status of objects, and finally checking logs for any last resource clues.","title":"Debugging Applications"},{"location":"developer-foundation/k8s-troubleshooting/#resources","text":"OpenShift - Debugging Applications - Debugging Metrics IKS - Debugging Applications - Debugging Services - Debugging Replication Controllers","title":"Resources"},{"location":"developer-foundation/k8s-troubleshooting/#references","text":"OpenShift Broken setup to debug curl -sL https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/1e2a0cca964c7b54ce3df2fc3fbf33a232511877/debugk8s.64 | base64 -d | oc apply -f - Expose the service using port-forward oc port-forward service/my-service 8080:80 -n debug Try to access the service curl http://localhost:8080 Try Out these Commands to Debug oc get pods --all-namespaces oc get deployments oc describe pod oc explain Pod.spec.containers.resources.requests oc explain Pod.spec.containers.livenessProbe oc edit deployment oc logs oc get service oc get ep oc describe service oc get pods --show-labels oc get deployment --show-labels IKS Broken setup to debug curl -s https://gist.githubusercontent.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091/raw/a92550dd499ebefc205fa2cd5c18123409186712/debugk8s.64 | base64 -d | kubectl apply -f - Expose the service using port-forward kubectl port-forward service/my-service 8080:80 -n debug Try to access the service curl http://localhost:8080 Try Out these Commands to Debug kubectl get pods --all-namespaces kubectl get deployments kubectl describe pod kubectl explain Pod.spec.containers.resources.requests kubectl explain Pod.spec.containers.livenessProbe kubectl edit deployment kubectl logs kubectl get service kubectl get ep kubectl describe service kubectl get pods --show-labels kubectl get deployment --show-labels","title":"References"},{"location":"developer-foundation/k8s-troubleshooting/#solution","text":"https://gist.github.com/csantanapr/e823b1bfab24186a26ae4f9ec1ff6091#file-debugk8s-yaml","title":"Solution"},{"location":"developer-foundation/k8s-troubleshooting/#activities","text":"The continuous integration activities focus around Tekton the integration platform. These labs will show you how to build pipelines and test your code before deployment. Task Description Link Try It Yourself Debugging Find which service is breaking in your cluster and find out why. Debugging","title":"Activities"},{"location":"developer-foundation/openshift/","text":"Introduction \u00b6 Red Hat OpenShift Container Platform (OpenShift) is a container application platform that provides developers and IT organizations with a cloud application platform for deploying new applications on secure, scalable resources with minimal configuration and management overhead. Built on Red Hat Enterprise Linux, Docker, and Kubernetes, OpenShift provides a secure and scalable multitenant operating system for today\u2019s enterprise applications, while providing integrated application runtimes and libraries. OpenShift brings a robust, flexible, and scalable container platform to customer data centers, enabling organizations to implement a platform that meets security, privacy, compliance, and governance requirements. Applications run as containers, which are isolated partitions inside a single operating system. Containers provide many of the same benefits as virtual machines, such as security, storage, and network isolation, while requiring far fewer hardware resources and being quicker to launch and terminate. The use of containers by OpenShift helps with the efficiency, elasticity, and portability of the platform itself as well as its hosted applications. Red Hat OpenShift overview \u00b6 What is OpenShift? \u00b6 Red Hat OpenShift on IBM Cloud \u00b6 With Red Hat OpenShift on IBM Cloud, OpenShift developers have a fast and secure way to containerize and deploy enterprise workloads in Kubernetes clusters. OpenShift clusters build on Kubernetes container orchestration that offers consistency and flexibility in operations. Because IBM manages OpenShift Container Platform (OCP), you'll have more time to focus on your core tasks. Features \u00b6 OpenShift experience built on Kubernetes Use the OpenShift tools and APIs you already know for a single, consistent experience, even when working across hybrid environments or different cloud providers. Heightened cluster and app security IBM provides security features to protect your cluster infrastructure, isolate your compute resources, encrypt data, and ensure security compliance in your container deployments. Further, OpenShift sets up strict Security Context Constraints for greater pod security by default. Worldwide, continuous availability Deploy and scale workloads across the globe in all IBM Cloud multizone regions. OpenShift clusters include a managed master that is automatically spread across zones within the region for high availability. Integrated OpenShift catalog Quickly set up a CI/CD with Jenkins or deploy a variety of apps in a guided experience that's fully integrated into your OpenShift cluster. Innovation with Cloud Paks and the complete IBM Cloud platform Easily integrate AI with Watson APIs to extend the power of your apps. Access the IBM middleware in IBM Cloud Paks from within the scalable public cloud. You also get built-in services for monitoring, logging, load-balancing, storage, and security to help you manage an app\u2019s lifecycle. Kubernetes and OpenShift: What's the Difference? \u00b6 RedHat Interactive Learning \u00b6 To help understand OpenShift in more detail work through these KataKoda self paced learning modules offered by RedHat Advanced Topics \u00b6 The following topics are not required to move onto the IBM Sandbox self paced learning but will help you understand more about how the base platform works. References \u00b6 Red Hat Openshift on IBM resources Openshift Interactive Learning Portal Openshift Interactive Learning Portal For Middleware Openshift Interactive Learning Portal For subsystems","title":"Openshift 'overview'"},{"location":"developer-foundation/openshift/#introduction","text":"Red Hat OpenShift Container Platform (OpenShift) is a container application platform that provides developers and IT organizations with a cloud application platform for deploying new applications on secure, scalable resources with minimal configuration and management overhead. Built on Red Hat Enterprise Linux, Docker, and Kubernetes, OpenShift provides a secure and scalable multitenant operating system for today\u2019s enterprise applications, while providing integrated application runtimes and libraries. OpenShift brings a robust, flexible, and scalable container platform to customer data centers, enabling organizations to implement a platform that meets security, privacy, compliance, and governance requirements. Applications run as containers, which are isolated partitions inside a single operating system. Containers provide many of the same benefits as virtual machines, such as security, storage, and network isolation, while requiring far fewer hardware resources and being quicker to launch and terminate. The use of containers by OpenShift helps with the efficiency, elasticity, and portability of the platform itself as well as its hosted applications.","title":"Introduction"},{"location":"developer-foundation/openshift/#red-hat-openshift-overview","text":"","title":"Red Hat OpenShift overview"},{"location":"developer-foundation/openshift/#what-is-openshift","text":"","title":"What is OpenShift?"},{"location":"developer-foundation/openshift/#red-hat-openshift-on-ibm-cloud","text":"With Red Hat OpenShift on IBM Cloud, OpenShift developers have a fast and secure way to containerize and deploy enterprise workloads in Kubernetes clusters. OpenShift clusters build on Kubernetes container orchestration that offers consistency and flexibility in operations. Because IBM manages OpenShift Container Platform (OCP), you'll have more time to focus on your core tasks.","title":"Red Hat OpenShift on IBM Cloud"},{"location":"developer-foundation/openshift/#features","text":"OpenShift experience built on Kubernetes Use the OpenShift tools and APIs you already know for a single, consistent experience, even when working across hybrid environments or different cloud providers. Heightened cluster and app security IBM provides security features to protect your cluster infrastructure, isolate your compute resources, encrypt data, and ensure security compliance in your container deployments. Further, OpenShift sets up strict Security Context Constraints for greater pod security by default. Worldwide, continuous availability Deploy and scale workloads across the globe in all IBM Cloud multizone regions. OpenShift clusters include a managed master that is automatically spread across zones within the region for high availability. Integrated OpenShift catalog Quickly set up a CI/CD with Jenkins or deploy a variety of apps in a guided experience that's fully integrated into your OpenShift cluster. Innovation with Cloud Paks and the complete IBM Cloud platform Easily integrate AI with Watson APIs to extend the power of your apps. Access the IBM middleware in IBM Cloud Paks from within the scalable public cloud. You also get built-in services for monitoring, logging, load-balancing, storage, and security to help you manage an app\u2019s lifecycle.","title":"Features"},{"location":"developer-foundation/openshift/#kubernetes-and-openshift-whats-the-difference","text":"","title":"Kubernetes and OpenShift: What's the Difference?"},{"location":"developer-foundation/openshift/#redhat-interactive-learning","text":"To help understand OpenShift in more detail work through these KataKoda self paced learning modules offered by RedHat","title":"RedHat Interactive Learning"},{"location":"developer-foundation/openshift/#advanced-topics","text":"The following topics are not required to move onto the IBM Sandbox self paced learning but will help you understand more about how the base platform works.","title":"Advanced Topics"},{"location":"developer-foundation/openshift/#references","text":"Red Hat Openshift on IBM resources Openshift Interactive Learning Portal Openshift Interactive Learning Portal For Middleware Openshift Interactive Learning Portal For subsystems","title":"References"},{"location":"developer-intermediate/","text":"Note Before you proceed, make sure you have an Cloud-Native Learning Journey invite. It enables you to access OpenShift on AWS, Azure or IBM Cloud account with the Predefined DevSecOps Tools already installed and ready for you to use. (The environment is locked down to prevent the creation of any new services outside of the scope of the learning journey) This set of learning tasks focuses on how to use Red Hat OpenShift Developer Experience to develop and deploy a set of basic cloud-native applications . It covers the use of common tools designed to help the developer monitor, log and debug their applications. This learning tasks assumes that you have: - You have completed the tasks before the workshop - You have received an Learning-Journey invite email - Have watched the 30 min Introduction Video showing the end to end experience Learning Tasks \u00b6 The learning tasks help you understand the Developer Experience with IBM RedHat OpenShift managed cluster. These are the getting started and initial setup tasks that help you start a project. Note Support is provided in the #<discord-channel-provided> you were provided with in your Learning-Journey invite. This channel will also be used to share any common issues found. It can be used to provide feedback on the content you have just completed Agenda Before the Workshop \u00b6 Please complete these tasks before attending the first session. This will help you get started quickly with the practical exercises. Task Description Link Time Presentations Welcome Message Welcome to Cloud-Native Workshop Introduction 5 mins Prerequisites Install the prerequisite tools Setup Prerequisites 10 mins Dev Env Access Validate access to your Development Cluster Validate 10 min Software Delivery Lifecycle Overview of the Tools you will be using with the OpenShift Environment Video 30 min Day 1 (2.5 Hours) \u00b6 Task Description Link Time Presentations Welcome Message Introductions & Logistics 15 mins Introduction to Cloud-Native Development What is Cloud-Native Introduction to Cloud-Native Development Cloud-Native Development 15 min Link Cloud-Native Applications Cloud-Native Application Characteristics Cloud-Native Applications 15 min Link Container Concepts Containers Containers Overview Containers 15 min Link Break 5 mins OpenShift Overview OpenShift OpenShift Overview OpenShift Overview 15 min Link DevSecOps Continuous Integration Overview of Continuous Integration CI 15 min Link Continuous Delivery Overview of Continuous Delivery CD 15 min Deploy your first app demo Q & A 10 mins Homework \u00b6 Assignment Description Time Deploy a simple nginx container Learn how to create a Docker Image for running a static HTML website using Nginx. 10 mins Explore a deployed container Explore the elements of a deployed application 5-10 mins Expose an application publicly Learn how to expose an application outside of the cluster 10 mins Scale an application Learn how to create multiple instances of an application to meet demand 10 mins Update an application Learn how to perform a rolling update of an application 10 mins OpenShift - Getting Started Learn how to use the OpenShift Container Platform to build and deploy an application with a data backend and a web frontend. 10-15 mins Deploy a Spring-Boot Application Learn more about developing applications using Spring Boot using Red Hat Runtimes. 15 mins Cloud Native Development Reading through different concepts in cloud native development 90 mins Continuous Integration Continuous Integration Hands on Exercise 60 mins Continuous Delivery Continuous Delivery Hands on Exercise 60 mins Continuous Delivery Continuous Delivery Hands on Exercise 60 mins Day 2 (2.5 Hours) \u00b6 Task Description Link Time Presentations Recap Recap of things learned in Day 1 10 mins Learn how to develop and deploy apps with enterprise DevSecOps DevSecOps DevSecOps Overview DevSecOps 20 mins Preparing for the Hands-On Labs Setting up the Development Tools Dev. Tools Setup 10 mins Deploy your first app The very first experience of deploying an app in OpenShift or Kubernetes Deploy First App 25 mins Break 5 mins Code Analysis Code Quality with Sonarqube Code Analysis 15 mins Image Registry Container Registry Image Registry 10 mins Artifact Management Artifact Management with Artifactory Artifact Management 10 min Monitoring Sysdig Monitoring Monitoring 5 mins Logging Logging with LogDNA Logging 5 mins Image Signing Signing Container Images Signing 25 min Q & A 10 mins Homework \u00b6 Assignment Description Time Dev. Tools Setup Setting up the Development Tools 10 mins Deploy First App The very first experience of deploying an application via a Tekton pipeline in OpenShift or Kubernetes 25 mins SonarQube Creating a quality gate for Sonar Scan 20mins Image Registry Accessing the image of your first app deployment 15 mins Monitoring Sysdig Hands on Lab 15mins Logging LogDNA hands on Lab 15mins Day 3 (2.5 Hours) \u00b6 Task Description Link Time Presentations Recap Recap of things learned in Day 1 & 2 10 mins Apply all we have learnt so far to develop a set of microservices and deploy them on OpenShift Preparing for the Hands-On Labs Setting up the Development Tools Dev. Tools Setup 10 mins Inventory Application Objective of the exercise Objective 15 mins Inventory Backend Creating the Inventory Service with Java Inventory Backend 30 mins Break 5 mins Inventory BFF Creating the Inventory BFF with GraphQL Inventory BFF 25 mins Inventory UI Creating the UI microservice Inventory UI 25 mins AppID Authentication with AppID AppID 10 mins Q & A 10 mins **Optional: inventory-app) 60 mins Homework \u00b6 Assignment Description Time Dev. Tools Setup Setting up the Development Tools 10 mins Objective Objective of the exercise 15 mins Inventory Backend Creating the Inventory Service with Java 30 mins Inventory BFF Creating the Inventory BFF with GraphQL 25 mins Inventory UI Creating the UI microservice 25 mins Inventory Backend with Cloudant Cloudant Database integration 30 mins Inventory UI with Appid Protecting Inventory UI with Appid integration 30 mins Success You have successfully completed the Developer Intermediate material - You have become familiar with IBM Cloud and Red Hat OpenShift - You have deployed your first application with OpenShift 4.10 - You have understood how monitor, access logs - Understand where your artifacts are being managed Review Learning Tasks \u00b6 Working with colleagues review your learning tasks. The Cloud Ecosystem team is looking for feedback from three perspectives: What went well? What needs improvement? Next steps? Developers will begin the learning journey with different skill levels and familiarity with this material, but will complete the Cloud-Native enablement with a more detailed understanding of how to build solutions for the IBM Cloud. Each student's Next Steps will help identify that person's skill gaps to ensure they cover the topics needed to complete the learning journey successfully.","title":"Agenda"},{"location":"developer-intermediate/#learning-tasks","text":"The learning tasks help you understand the Developer Experience with IBM RedHat OpenShift managed cluster. These are the getting started and initial setup tasks that help you start a project. Note Support is provided in the #<discord-channel-provided> you were provided with in your Learning-Journey invite. This channel will also be used to share any common issues found. It can be used to provide feedback on the content you have just completed Agenda","title":"Learning Tasks"},{"location":"developer-intermediate/#before-the-workshop","text":"Please complete these tasks before attending the first session. This will help you get started quickly with the practical exercises. Task Description Link Time Presentations Welcome Message Welcome to Cloud-Native Workshop Introduction 5 mins Prerequisites Install the prerequisite tools Setup Prerequisites 10 mins Dev Env Access Validate access to your Development Cluster Validate 10 min Software Delivery Lifecycle Overview of the Tools you will be using with the OpenShift Environment Video 30 min","title":"Before the Workshop"},{"location":"developer-intermediate/#day-1-25-hours","text":"Task Description Link Time Presentations Welcome Message Introductions & Logistics 15 mins Introduction to Cloud-Native Development What is Cloud-Native Introduction to Cloud-Native Development Cloud-Native Development 15 min Link Cloud-Native Applications Cloud-Native Application Characteristics Cloud-Native Applications 15 min Link Container Concepts Containers Containers Overview Containers 15 min Link Break 5 mins OpenShift Overview OpenShift OpenShift Overview OpenShift Overview 15 min Link DevSecOps Continuous Integration Overview of Continuous Integration CI 15 min Link Continuous Delivery Overview of Continuous Delivery CD 15 min Deploy your first app demo Q & A 10 mins","title":"Day 1 (2.5 Hours)"},{"location":"developer-intermediate/#homework","text":"Assignment Description Time Deploy a simple nginx container Learn how to create a Docker Image for running a static HTML website using Nginx. 10 mins Explore a deployed container Explore the elements of a deployed application 5-10 mins Expose an application publicly Learn how to expose an application outside of the cluster 10 mins Scale an application Learn how to create multiple instances of an application to meet demand 10 mins Update an application Learn how to perform a rolling update of an application 10 mins OpenShift - Getting Started Learn how to use the OpenShift Container Platform to build and deploy an application with a data backend and a web frontend. 10-15 mins Deploy a Spring-Boot Application Learn more about developing applications using Spring Boot using Red Hat Runtimes. 15 mins Cloud Native Development Reading through different concepts in cloud native development 90 mins Continuous Integration Continuous Integration Hands on Exercise 60 mins Continuous Delivery Continuous Delivery Hands on Exercise 60 mins Continuous Delivery Continuous Delivery Hands on Exercise 60 mins","title":"Homework"},{"location":"developer-intermediate/#day-2-25-hours","text":"Task Description Link Time Presentations Recap Recap of things learned in Day 1 10 mins Learn how to develop and deploy apps with enterprise DevSecOps DevSecOps DevSecOps Overview DevSecOps 20 mins Preparing for the Hands-On Labs Setting up the Development Tools Dev. Tools Setup 10 mins Deploy your first app The very first experience of deploying an app in OpenShift or Kubernetes Deploy First App 25 mins Break 5 mins Code Analysis Code Quality with Sonarqube Code Analysis 15 mins Image Registry Container Registry Image Registry 10 mins Artifact Management Artifact Management with Artifactory Artifact Management 10 min Monitoring Sysdig Monitoring Monitoring 5 mins Logging Logging with LogDNA Logging 5 mins Image Signing Signing Container Images Signing 25 min Q & A 10 mins","title":"Day 2 (2.5 Hours)"},{"location":"developer-intermediate/#homework_1","text":"Assignment Description Time Dev. Tools Setup Setting up the Development Tools 10 mins Deploy First App The very first experience of deploying an application via a Tekton pipeline in OpenShift or Kubernetes 25 mins SonarQube Creating a quality gate for Sonar Scan 20mins Image Registry Accessing the image of your first app deployment 15 mins Monitoring Sysdig Hands on Lab 15mins Logging LogDNA hands on Lab 15mins","title":"Homework"},{"location":"developer-intermediate/#day-3-25-hours","text":"Task Description Link Time Presentations Recap Recap of things learned in Day 1 & 2 10 mins Apply all we have learnt so far to develop a set of microservices and deploy them on OpenShift Preparing for the Hands-On Labs Setting up the Development Tools Dev. Tools Setup 10 mins Inventory Application Objective of the exercise Objective 15 mins Inventory Backend Creating the Inventory Service with Java Inventory Backend 30 mins Break 5 mins Inventory BFF Creating the Inventory BFF with GraphQL Inventory BFF 25 mins Inventory UI Creating the UI microservice Inventory UI 25 mins AppID Authentication with AppID AppID 10 mins Q & A 10 mins **Optional: inventory-app) 60 mins","title":"Day 3 (2.5 Hours)"},{"location":"developer-intermediate/#homework_2","text":"Assignment Description Time Dev. Tools Setup Setting up the Development Tools 10 mins Objective Objective of the exercise 15 mins Inventory Backend Creating the Inventory Service with Java 30 mins Inventory BFF Creating the Inventory BFF with GraphQL 25 mins Inventory UI Creating the UI microservice 25 mins Inventory Backend with Cloudant Cloudant Database integration 30 mins Inventory UI with Appid Protecting Inventory UI with Appid integration 30 mins Success You have successfully completed the Developer Intermediate material - You have become familiar with IBM Cloud and Red Hat OpenShift - You have deployed your first application with OpenShift 4.10 - You have understood how monitor, access logs - Understand where your artifacts are being managed","title":"Homework"},{"location":"developer-intermediate/#review-learning-tasks","text":"Working with colleagues review your learning tasks. The Cloud Ecosystem team is looking for feedback from three perspectives: What went well? What needs improvement? Next steps? Developers will begin the learning journey with different skill levels and familiarity with this material, but will complete the Cloud-Native enablement with a more detailed understanding of how to build solutions for the IBM Cloud. Each student's Next Steps will help identify that person's skill gaps to ensure they cover the topics needed to complete the learning journey successfully.","title":"Review Learning Tasks"},{"location":"developer-intermediate/artifact-management/","text":"In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. The uses Artifactory as an artifact repository manager, which it uses to host its Helm repository. What is Artifactory \u00b6 Artifactory is an artifact management repository. An artifact repository manager hosts multiple binary repositories, like a database management system for artifacts. The binary repository compliments the source code repository: the code from an SCM is the input to the build process, whereas a binary repository stores the output of the build process, often called artifacts. The artifacts are often individual application components that can later be assembled into a full product. An artifact repository manager is an integral part of a CI/CD solution, a companion to the pipeline. As the pipeline builds artifacts, they're stored in the repositories. When the pipeline later needs artifacts that have already been built, they're retrieved from the repositories. This enables a build to be broken into smaller stages with well-defined inputs and outputs and provides better tracking of each stage's results. Often a failed pipeline can restart in the middle using artifacts that were already built and stored. An artifact repository often serves as the storage for a package manager, which assembles an application from artifacts. Here are some common package managers and their repositories: - Maven : Builds Java artifacts (such as Jar, War, Ear, etc.) and projects stored in Maven repositories such as Maven Central - npm : Assembles programs from JavaScript packages stored in npm-registries such as the public npm registry - PIP : Installs Python packages from index repositories such as the Python Package Index (PyPI) - Helm : Deploys applications to Kubernetes using charts stored in Helm repositories such as the Helm Hub catalog of repositories Docker is not a package manager, but its architecture includes an artifact repository: - Docker : Stores images in Docker registries such as Docker Hub Note that you do not need a very large team to start reaping benefits from an artifact repository manager. The initial investment is not very large and the benefits are felt immediately. Artifact management in the Pipeline \u00b6 The will eventually be extended to store a number of artifact types in Artifactory. Thus far, the CI and CD pipelines exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: - Docker images : The Developer Tools Image Registry - Helm charts : A Helm repository in Artifactory The have also been configured to store their Helm charts in Artifactory. Artifactory is part of the 's complete CI/CD solution: Artifactory dashboard \u00b6 Use the Artifactory dashboard to browse the repositories and their artifacts. Open the Artifactory web UI for your environment. Go to the OpenShift console. On the top, you will find an icon of a square made of 9 small squares. Clicking on that icon will give you all the tools shortcut. Select Artifactory to navigate to the Artifactory dashboard. Browse the Helm repository. - In the sidebar on the left, select Artifactory > Artifacts. - Expand the tree for the generic-local repository, which is the Helm repository - Expand the branch for your environment's cluster, such as showcase-dev-iks The artifacts in the cluster's branch follow the Helm chart repository structure: - index.yaml : Helm's index of all of the charts in the repository - charts : The tgz files named for the application they deploy Browse the artifacts to see how a Helm repository is organized. - Select the index.yaml file and View it to see its contents - Expand a chart's branch to see that the tgz file contains the chart directory structure Notice that each chart has its own URL in Artifactory, and index lists the URL for a chart. Conclusion \u00b6 The includes an artifact repository manager called Artifactory, which it uses to host a Helm repository. As the CI pipeline builds the Helm chart for an application, it stores the chart in the Artifactory repository. When the ArgoCD pipeline deploys an application, it retrieves the chart from the Artifactory repository.","title":"Artifact Management"},{"location":"developer-intermediate/artifact-management/#what-is-artifactory","text":"Artifactory is an artifact management repository. An artifact repository manager hosts multiple binary repositories, like a database management system for artifacts. The binary repository compliments the source code repository: the code from an SCM is the input to the build process, whereas a binary repository stores the output of the build process, often called artifacts. The artifacts are often individual application components that can later be assembled into a full product. An artifact repository manager is an integral part of a CI/CD solution, a companion to the pipeline. As the pipeline builds artifacts, they're stored in the repositories. When the pipeline later needs artifacts that have already been built, they're retrieved from the repositories. This enables a build to be broken into smaller stages with well-defined inputs and outputs and provides better tracking of each stage's results. Often a failed pipeline can restart in the middle using artifacts that were already built and stored. An artifact repository often serves as the storage for a package manager, which assembles an application from artifacts. Here are some common package managers and their repositories: - Maven : Builds Java artifacts (such as Jar, War, Ear, etc.) and projects stored in Maven repositories such as Maven Central - npm : Assembles programs from JavaScript packages stored in npm-registries such as the public npm registry - PIP : Installs Python packages from index repositories such as the Python Package Index (PyPI) - Helm : Deploys applications to Kubernetes using charts stored in Helm repositories such as the Helm Hub catalog of repositories Docker is not a package manager, but its architecture includes an artifact repository: - Docker : Stores images in Docker registries such as Docker Hub Note that you do not need a very large team to start reaping benefits from an artifact repository manager. The initial investment is not very large and the benefits are felt immediately.","title":"What is Artifactory"},{"location":"developer-intermediate/artifact-management/#artifact-management-in-the-pipeline","text":"The will eventually be extended to store a number of artifact types in Artifactory. Thus far, the CI and CD pipelines exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: - Docker images : The Developer Tools Image Registry - Helm charts : A Helm repository in Artifactory The have also been configured to store their Helm charts in Artifactory. Artifactory is part of the 's complete CI/CD solution:","title":"Artifact management in the Pipeline"},{"location":"developer-intermediate/artifact-management/#artifactory-dashboard","text":"Use the Artifactory dashboard to browse the repositories and their artifacts. Open the Artifactory web UI for your environment. Go to the OpenShift console. On the top, you will find an icon of a square made of 9 small squares. Clicking on that icon will give you all the tools shortcut. Select Artifactory to navigate to the Artifactory dashboard. Browse the Helm repository. - In the sidebar on the left, select Artifactory > Artifacts. - Expand the tree for the generic-local repository, which is the Helm repository - Expand the branch for your environment's cluster, such as showcase-dev-iks The artifacts in the cluster's branch follow the Helm chart repository structure: - index.yaml : Helm's index of all of the charts in the repository - charts : The tgz files named for the application they deploy Browse the artifacts to see how a Helm repository is organized. - Select the index.yaml file and View it to see its contents - Expand a chart's branch to see that the tgz file contains the chart directory structure Notice that each chart has its own URL in Artifactory, and index lists the URL for a chart.","title":"Artifactory dashboard"},{"location":"developer-intermediate/artifact-management/#conclusion","text":"The includes an artifact repository manager called Artifactory, which it uses to host a Helm repository. As the CI pipeline builds the Helm chart for an application, it stores the chart in the Artifactory repository. When the ArgoCD pipeline deploys an application, it retrieves the chart from the Artifactory repository.","title":"Conclusion"},{"location":"developer-intermediate/code-analysis/","text":"In IBM Garage Method, one of the Develop practices is to automate tests for continuous delivery , in part by using static source code analysis tools . SonarQube automates performing static code analysis and enables it to be added to a continuous integration pipeline. The 's CI pipeline ( Jenkins , Tekton , etc.) includes a SonarQube stage. Simply by building your app using the pipeline, your code gets analyzed, and the SonarQube UI displays the findings. What is code analysis \u00b6 Static code analysis (a.k.a. code analysis) is a method of debugging by performing automated evaluation of code without executing the program. The analysis is structured as a set of coding rules that evaluate the code's quality. Analysis can be performed on source code or compiled code. The analyzer must support the programming language the code is written in so that it can parse the code like a compiler or simulate its execution. Static code analysis differs from dynamic analysis, which observes and evaluates a running program. Dynamic analysis requires test inputs and can measure user functionality as well as runtime qualities like execution time and resource consumption. A code review is static code analysis performed by a human. Static code analysis can evaluate several different aspects of code quality, such as: Reliability Bug : Programming error that breaks functionality Security Vulnerability : A point in a program that can be attacked Hotspot : Code that uses a security-sensitive API Maintainability Coding standards : Practices that increase the human readability and understandability of code Code smell : Code that is confusing and difficult to maintain Technical debt : Estimated time required to fix all maintainability issues Complexity Code complexity : Code's control flow and number of paths through the code Duplications Duplicated code : The same code sequence appearing more than once in the same program Manageability Testability : How easily tests can be developed and used to show the program meets requirements Portability : How easily the program can be reused in different environments Reusability : The program's modularity, loose coupling, and limited interdependencies Static code analysis collects several metrics that measure code quality: Issues Type : Bug, Vulnerability, Code Smell Severity Blocker : Bug with a high probability to impact the behavior of the application in production Critical : Bug with a low probability to impact the behavior of the application in production, or a security vulnerability Major : Code smell with high impact to developer productivity Minor : Code smell with slight impact to developer productivity Info : Neither a bug nor a code smell, just a finding Size Classes : Number of class definitions (concrete, abstract, nested, interfaces, enums, annotations) Lines of code : Linespace separated text that is not whitespace or comments Comment lines : Linespace separated text containing significant commentary or commented-out code Coverage Test coverage : Code that was executed by a test suite A Quality gate defines a policy that assesses pass/fail whether or not the number of issues and their severity is acceptable. What is SonarQube \u00b6 SonarQube performs static code analysis to evaluate code quality, using analysis rules that focus on three areas: Code Reliability : Detect bugs that will impact end-user functionality Application Security : Detect vulnerabilities and hotspots that can be exploited to compromise the program Technical Debt : Keep your codebase maintainable to increase developer velocity SonarQube plugs into the application lifecycle management (ALM) process to make continuous inspection part of continuous integration. Adding code analysis to ALM provides regular, timely feedback on the quality of the code being produced. The goal is to detect problems as soon as possible so that they can be resolved before they can impact production end users. The continuous integration (CI) server integrates SonarQube into the ALM. The SonarQube solution consists of several components: The central component is the SonarQube Server, which runs the SonarScanner, processes the resulting analysis reports, stores the reports in SonarQube Database, and displays the reports in the SonarQube UI. A CI server uses a stage/goal/task in its build automation to trigger the language-specific SonarScanner to scan the code being built. Developers can view the resulting analysis report in the SonarQube UI. Code Analysis in the Pipeline \u00b6 In the CI pipeline, the Sonar scan stage triggers the SonarScanner in SonarQube. Follow these directions to see code analysis in action: Deploy the named Spring Boot Microservice. Follow the directions in Deploying an App Deploy the Spring Boot Microservice template Name the new repo something like sonar-java Be sure to run the CI pipeline (using the version igc-java-gradle-v1-x-0) for your project, and confirm that it runs the Sonar scan in test stage Examine SonarQube's analysis report for your app Go to the OpenShift console. On the top, you will find an icon of a square made of 9 small squares. Clicking on that icon will give you all the tools shortcut. Select SonarQube to open the SonarQube dashboard. Use oc credentials to get the user name and password if you a prompted to log in Go to the Projects page You should see your project in the list, such as bwoolf1.sonar-java . The project summary shows several characteristics measured in the app: - The quality gate passed - Several issues were found, categorized by type - 2 bugs for a C rating - 0 vulnerabilities for an A rating - 17 code smells but an A rating - None of the code was tested - None of it is duplicate code - It scanned 1.5k lines of code written in Java, a small program (XS, S, M, L, XL) In the Projects list, click on the project name (such as bwoolf1.sonar-java ) to open your project The project overview shows more detail about how many issues were found in the app - Reliability: 2 bugs for a C rating - Security: 1 security hotspot but an A rating - Maintainability: 17 code smells, 2 hrs of technical debt but an A rating - Coverage: 7 unit tests - Duplications: 0 duplicated blocks Examine the issues \u00b6 Use the SonarQube dashboard to explore the issues that it found in your project. In the Reliability pane of the project's Overview page, click on the \"2\" to open the Issues page The Issues page, filtered for bugs, shows two issues. Both concern \"synchronized\" methods. In the Issues list, click on either issue to see where the issues appeared in the code The Issues detail shows the source code file for the Java class. The issue descriptions are embedded after the mark and reset method signatures. In either issue, press the Why is this an issue? button. SonarQube displays the details of its \"Overrides should match their parent class methods in synchronization\" rule. Now you need to investigate to figure out why the code violates this rule. Want to see if you can track down the problem before seeing the solution? What to fix is pretty obvious--the Rule explains what to do--but tracking down why takes some effort. Here's the solution: The error is not in the file's parent class, ResettableHttpServletRequest , but in its embedded class, SimpleServletInputStream , which extends javax.servlet.ServletInputStream . The Javadocs for ServletInputStream show that it extends java.io.InputStream . The original Java 1.0 Javadocs show that these methods in InputStream are indeed synchronized: public synchronized void mark ( int readlimit ) . . . public synchronized void reset () throws IOException More recent Javadocs haven't shown these signatures for years, but the compiler says the class is still defined that way. There's some debate about whether mark and reset really need to be synchronized . But SonarQube doesn't judge, it just reports: Since the superclass defined the method signatures as synchronized, SonarQube is warning that the subclass is supposed to do so as well. Examine the other issues \u00b6 Besides the bugs, SonarQube also found issues that are hotspots and code smells. In the SonarQube dashboard, go back to the Issues page Click on the \"1\" above Security hotspots The issue warns to \"Make sure that command line arguments are used safely here.\" SonarQube considers any class that has a public static void main(String[] args) method to be a potential vulnerability. As the rule explains, \"Command line arguments can be dangerous just like any other user input. They should never be used without being first validated and sanitized.\" This method passes them through unchecked, which is risky. Back in the Issues page, click on \"17\" code smells SonarQube found issues such as: - Remove this unused import 'java.lang.System.lineSeparator'. - Move constants to a class or enum. - This block of commented-out lines of code should be removed. - Replace this use of System.out or System.err by a logger. None of these break your app's functionality, but they do make the code more difficult to maintain. Give it a try \u00b6 As we saw earlier, SonarQube found two bugs in our Java app. Let's do something about that. Add a quality gate to SonarQube \u00b6 The first problem is that the quality gate says that the app passed. The default quality gate is OK with those two bugs, but we're not. Let's create a new quality gate that checks for bugs. Open the SonarQube dashboard through OpenShift console shortcut as shown below: To create and install a new quality gate, first log in to SonarQube Go to the Quality Gates page Make a copy of the default gate named Sonar way , give it a name like better gate {your initials} , i.e. better gate bw Add a condition, Bugs is greater than 0 Add your project to this gate Run the pipeline again to scan the code again. Back in the OpenShift console, on the Application Console > Builds > Pipelines page, press Start Pipeline After the Sonar Scan stage completes, go back to the SonarQube dashboard and take a look at your project Good news, the quality gate is working and SonarQube fails the project now! Fix the code \u00b6 Let's fix the problems. As discussed before, the bugs are that two methods need to be marked as synchronized. Let's fix the code. Edit the class com.ibm.cloud_garage.logging.inbound.ResettableHttpServletRequest Edit the methods mark and reset to make them both synchronized public synchronized void mark ( int i ) { . . . public synchronized void reset () throws IOException { Push the change to the server repo, which runs the pipeline again, and this time the Quality Gate stage passes Check the project in SonarQube and see that it now has 0 bugs and has now passed Extra credit : The code still has 17 code smells. Go fix those! Conclusion \u00b6 It's a good idea to incorporate code analysis as part of your application development lifecycle, so you can use its findings to help enforce and improve your code quality. Here, the uses SonarQube, but you never had to run SonarQube. Just run the 's build pipeline on your app and it gets scanned automatically. After running the pipeline, open the SonarQube UI and browse the findings in your app's project to figure out what code you ought to fix.","title":"Code Analysis"},{"location":"developer-intermediate/code-analysis/#what-is-code-analysis","text":"Static code analysis (a.k.a. code analysis) is a method of debugging by performing automated evaluation of code without executing the program. The analysis is structured as a set of coding rules that evaluate the code's quality. Analysis can be performed on source code or compiled code. The analyzer must support the programming language the code is written in so that it can parse the code like a compiler or simulate its execution. Static code analysis differs from dynamic analysis, which observes and evaluates a running program. Dynamic analysis requires test inputs and can measure user functionality as well as runtime qualities like execution time and resource consumption. A code review is static code analysis performed by a human. Static code analysis can evaluate several different aspects of code quality, such as: Reliability Bug : Programming error that breaks functionality Security Vulnerability : A point in a program that can be attacked Hotspot : Code that uses a security-sensitive API Maintainability Coding standards : Practices that increase the human readability and understandability of code Code smell : Code that is confusing and difficult to maintain Technical debt : Estimated time required to fix all maintainability issues Complexity Code complexity : Code's control flow and number of paths through the code Duplications Duplicated code : The same code sequence appearing more than once in the same program Manageability Testability : How easily tests can be developed and used to show the program meets requirements Portability : How easily the program can be reused in different environments Reusability : The program's modularity, loose coupling, and limited interdependencies Static code analysis collects several metrics that measure code quality: Issues Type : Bug, Vulnerability, Code Smell Severity Blocker : Bug with a high probability to impact the behavior of the application in production Critical : Bug with a low probability to impact the behavior of the application in production, or a security vulnerability Major : Code smell with high impact to developer productivity Minor : Code smell with slight impact to developer productivity Info : Neither a bug nor a code smell, just a finding Size Classes : Number of class definitions (concrete, abstract, nested, interfaces, enums, annotations) Lines of code : Linespace separated text that is not whitespace or comments Comment lines : Linespace separated text containing significant commentary or commented-out code Coverage Test coverage : Code that was executed by a test suite A Quality gate defines a policy that assesses pass/fail whether or not the number of issues and their severity is acceptable.","title":"What is code analysis"},{"location":"developer-intermediate/code-analysis/#what-is-sonarqube","text":"SonarQube performs static code analysis to evaluate code quality, using analysis rules that focus on three areas: Code Reliability : Detect bugs that will impact end-user functionality Application Security : Detect vulnerabilities and hotspots that can be exploited to compromise the program Technical Debt : Keep your codebase maintainable to increase developer velocity SonarQube plugs into the application lifecycle management (ALM) process to make continuous inspection part of continuous integration. Adding code analysis to ALM provides regular, timely feedback on the quality of the code being produced. The goal is to detect problems as soon as possible so that they can be resolved before they can impact production end users. The continuous integration (CI) server integrates SonarQube into the ALM. The SonarQube solution consists of several components: The central component is the SonarQube Server, which runs the SonarScanner, processes the resulting analysis reports, stores the reports in SonarQube Database, and displays the reports in the SonarQube UI. A CI server uses a stage/goal/task in its build automation to trigger the language-specific SonarScanner to scan the code being built. Developers can view the resulting analysis report in the SonarQube UI.","title":"What is SonarQube"},{"location":"developer-intermediate/code-analysis/#code-analysis-in-the-pipeline","text":"In the CI pipeline, the Sonar scan stage triggers the SonarScanner in SonarQube. Follow these directions to see code analysis in action: Deploy the named Spring Boot Microservice. Follow the directions in Deploying an App Deploy the Spring Boot Microservice template Name the new repo something like sonar-java Be sure to run the CI pipeline (using the version igc-java-gradle-v1-x-0) for your project, and confirm that it runs the Sonar scan in test stage Examine SonarQube's analysis report for your app Go to the OpenShift console. On the top, you will find an icon of a square made of 9 small squares. Clicking on that icon will give you all the tools shortcut. Select SonarQube to open the SonarQube dashboard. Use oc credentials to get the user name and password if you a prompted to log in Go to the Projects page You should see your project in the list, such as bwoolf1.sonar-java . The project summary shows several characteristics measured in the app: - The quality gate passed - Several issues were found, categorized by type - 2 bugs for a C rating - 0 vulnerabilities for an A rating - 17 code smells but an A rating - None of the code was tested - None of it is duplicate code - It scanned 1.5k lines of code written in Java, a small program (XS, S, M, L, XL) In the Projects list, click on the project name (such as bwoolf1.sonar-java ) to open your project The project overview shows more detail about how many issues were found in the app - Reliability: 2 bugs for a C rating - Security: 1 security hotspot but an A rating - Maintainability: 17 code smells, 2 hrs of technical debt but an A rating - Coverage: 7 unit tests - Duplications: 0 duplicated blocks","title":"Code Analysis in the Pipeline"},{"location":"developer-intermediate/code-analysis/#examine-the-issues","text":"Use the SonarQube dashboard to explore the issues that it found in your project. In the Reliability pane of the project's Overview page, click on the \"2\" to open the Issues page The Issues page, filtered for bugs, shows two issues. Both concern \"synchronized\" methods. In the Issues list, click on either issue to see where the issues appeared in the code The Issues detail shows the source code file for the Java class. The issue descriptions are embedded after the mark and reset method signatures. In either issue, press the Why is this an issue? button. SonarQube displays the details of its \"Overrides should match their parent class methods in synchronization\" rule. Now you need to investigate to figure out why the code violates this rule. Want to see if you can track down the problem before seeing the solution? What to fix is pretty obvious--the Rule explains what to do--but tracking down why takes some effort. Here's the solution: The error is not in the file's parent class, ResettableHttpServletRequest , but in its embedded class, SimpleServletInputStream , which extends javax.servlet.ServletInputStream . The Javadocs for ServletInputStream show that it extends java.io.InputStream . The original Java 1.0 Javadocs show that these methods in InputStream are indeed synchronized: public synchronized void mark ( int readlimit ) . . . public synchronized void reset () throws IOException More recent Javadocs haven't shown these signatures for years, but the compiler says the class is still defined that way. There's some debate about whether mark and reset really need to be synchronized . But SonarQube doesn't judge, it just reports: Since the superclass defined the method signatures as synchronized, SonarQube is warning that the subclass is supposed to do so as well.","title":"Examine the issues"},{"location":"developer-intermediate/code-analysis/#examine-the-other-issues","text":"Besides the bugs, SonarQube also found issues that are hotspots and code smells. In the SonarQube dashboard, go back to the Issues page Click on the \"1\" above Security hotspots The issue warns to \"Make sure that command line arguments are used safely here.\" SonarQube considers any class that has a public static void main(String[] args) method to be a potential vulnerability. As the rule explains, \"Command line arguments can be dangerous just like any other user input. They should never be used without being first validated and sanitized.\" This method passes them through unchecked, which is risky. Back in the Issues page, click on \"17\" code smells SonarQube found issues such as: - Remove this unused import 'java.lang.System.lineSeparator'. - Move constants to a class or enum. - This block of commented-out lines of code should be removed. - Replace this use of System.out or System.err by a logger. None of these break your app's functionality, but they do make the code more difficult to maintain.","title":"Examine the other issues"},{"location":"developer-intermediate/code-analysis/#give-it-a-try","text":"As we saw earlier, SonarQube found two bugs in our Java app. Let's do something about that.","title":"Give it a try"},{"location":"developer-intermediate/code-analysis/#add-a-quality-gate-to-sonarqube","text":"The first problem is that the quality gate says that the app passed. The default quality gate is OK with those two bugs, but we're not. Let's create a new quality gate that checks for bugs. Open the SonarQube dashboard through OpenShift console shortcut as shown below: To create and install a new quality gate, first log in to SonarQube Go to the Quality Gates page Make a copy of the default gate named Sonar way , give it a name like better gate {your initials} , i.e. better gate bw Add a condition, Bugs is greater than 0 Add your project to this gate Run the pipeline again to scan the code again. Back in the OpenShift console, on the Application Console > Builds > Pipelines page, press Start Pipeline After the Sonar Scan stage completes, go back to the SonarQube dashboard and take a look at your project Good news, the quality gate is working and SonarQube fails the project now!","title":"Add a quality gate to SonarQube"},{"location":"developer-intermediate/code-analysis/#fix-the-code","text":"Let's fix the problems. As discussed before, the bugs are that two methods need to be marked as synchronized. Let's fix the code. Edit the class com.ibm.cloud_garage.logging.inbound.ResettableHttpServletRequest Edit the methods mark and reset to make them both synchronized public synchronized void mark ( int i ) { . . . public synchronized void reset () throws IOException { Push the change to the server repo, which runs the pipeline again, and this time the Quality Gate stage passes Check the project in SonarQube and see that it now has 0 bugs and has now passed Extra credit : The code still has 17 code smells. Go fix those!","title":"Fix the code"},{"location":"developer-intermediate/code-analysis/#conclusion","text":"It's a good idea to incorporate code analysis as part of your application development lifecycle, so you can use its findings to help enforce and improve your code quality. Here, the uses SonarQube, but you never had to run SonarQube. Just run the 's build pipeline on your app and it gets scanned automatically. After running the pipeline, open the SonarQube UI and browse the findings in your app's project to figure out what code you ought to fix.","title":"Conclusion"},{"location":"developer-intermediate/content-overview/","text":"One of the patterns emerging with development teams is the ability to use CNCF Tools as part of a more open multicloud CI/CD strategy. This approach is aligning around the Kubernetes platform. The IBM Cloud supports both upstream Kubernetes for advanced cutting edge workloads and Red Hat OpenShift, the proven mutlicloud distribution of Kubernetes that can install on IBM Cloud, Azure, AWS, VMWare and on Premise and many more places. This approach removes cloud vendor lock in around CI/CD tools and enables development teams to be more flexible with the target cloud they focus on deploying and developing in. There are two patterns from this approach: There are a core set of Development Tools that are installed inside the Kubernetes environment with the use of Operators the management and support of these tools can be controlled with centralized operations teams Integration of centralized operations tools like Source Code Management, Artifact Management, Image Management, Logging and Monitoring This model enables agile Cloud-Native development teams to execute quickly while also conforming to the enterprise standards required for CI/CD. This pattern also enable the reduction of costs of managing expensive centralized multi-tenant CI services and enables development teams to use a percentage of their development cluster to support CI activities. This approach with Tekton enables the centralized operations teams to impose specific pipeline tasks that need to be supported by development teams without the complex Overview \u00b6 This short video introduces the Cloud-Native concepts of CI/CD DevOps with : Each Cluster can have a selection of CNCF DevSecOps tools installed using IasC (Infrastructure As Code) using Terraform. The cluster then turns from a production state cluster into a cluster that is used for multi tenant development teams. The following describes the requirements: Installation : Install the CNCF Tools using Terraform this create a new Cluster : A or cluster that both hosts the tools and itself is a deployment target for application builds Software Delivery Lifecycle : Deployment target environments that support the application development lifecycle: dev , test , and staging Backend services : Cloud services commonly required by cloud-native applications for monitoring, security, and persistence CI/CD : A prebuilt, ready-to-run continuous delivery pipeline incorporating best-of-breed open source software tools supporting Jenkins , Tekton for CI and ArgoCD for CD : Prebuilt code templates for common application components and tasks incorporating best practices that developers can add to their codebase as needed Dashboard : Integration of the tools into the OpenShift dashboard, and a centralized developer dashboard to help developers use the environment's capabilities A core set of tools are sourced from the IBM Cloud Catalog that can be found in the . This approach helps assemble these reliable open source development tools into an end-to-end developer experience that is fully integrated with 's managed container orchestration services. The tools can also be sourced from the Operator Hub and the Red Hat Marketplace . The Operations team that owns the IasC can decide the best approach to install the tools either using Helm3 or Operators . Typically a Cloud System Admin installs and sets up a new , providing a place for the developers to start developing the minimum viable product (MVP). The objective is to reduce the time required for a team to configure and prepare their development environment. The key benefit is to make the end-to-end CI/CD development lifecycle consistent across each platform and make the out-of-the-box developer experience as simple as possible. The installation is performed using Terraform , driven by scripts with a modular configuration so unneeded tools can be easily disabled or new tools added. The combination of tools selected are proven in the industry to deliver real value for modern cloud-native development. Environment components \u00b6 After installation, the consists of a set of CNCF tools installed into your nominated kubernetes cluster. This diagram illustrates the : The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. Development cluster \u00b6 The heart of the is a cluster: An or 3-node cluster Cluster namespace that encapsulates the tooling installed in the cluster: tools Cluster namespaces for deployment target environments: dev , test , and staging The following IBM Cloud services are created and bound to the cluster: Capability Service Description Logging LogDNA Logging Manage app logging LogDNA Monitoring SysDig Monitoring Manage monitoring of apps with SysDig AppID AppID Application Authentication Secure your apps, APIs and Kubernetes Ingress end points Cloudant Cloudant NoSQL Database NoSQL Database commonly used for data persistence Cloud Object Storage Cloud Object Storage Storage Storage service commonly used for binary content PostgreSQL PostgreSQL (used by SonarQube) SQL Database used for structure data persistence Continuous delivery tools \u00b6 The following best-of-breed open source software tools are installed in the cluster's tools namespace: Capability Tool Bitnami Description Continuous Integration Jenkins CI Yes Jenkins is a common tool for Continuous Integration Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift Code Analysis SonarQube Yes SonarQube can scan code and display the results in a dashboard Artifact and Helm Storage Artifactory Yes Artifactory is an artifact storage and Helm chart repository Continuous Deployment ArgoCD ArgoCD support Continuous Delivery with GitOps Contract API Testing Pact Pact enables API contract testing Code Ready Workspace Eclipse CHE IDE for editing and managing code in a web browser If you want to find out more about IBM assets that help you get these common tools installed","title":"DevSecOps Overview"},{"location":"developer-intermediate/content-overview/#overview","text":"This short video introduces the Cloud-Native concepts of CI/CD DevOps with : Each Cluster can have a selection of CNCF DevSecOps tools installed using IasC (Infrastructure As Code) using Terraform. The cluster then turns from a production state cluster into a cluster that is used for multi tenant development teams. The following describes the requirements: Installation : Install the CNCF Tools using Terraform this create a new Cluster : A or cluster that both hosts the tools and itself is a deployment target for application builds Software Delivery Lifecycle : Deployment target environments that support the application development lifecycle: dev , test , and staging Backend services : Cloud services commonly required by cloud-native applications for monitoring, security, and persistence CI/CD : A prebuilt, ready-to-run continuous delivery pipeline incorporating best-of-breed open source software tools supporting Jenkins , Tekton for CI and ArgoCD for CD : Prebuilt code templates for common application components and tasks incorporating best practices that developers can add to their codebase as needed Dashboard : Integration of the tools into the OpenShift dashboard, and a centralized developer dashboard to help developers use the environment's capabilities A core set of tools are sourced from the IBM Cloud Catalog that can be found in the . This approach helps assemble these reliable open source development tools into an end-to-end developer experience that is fully integrated with 's managed container orchestration services. The tools can also be sourced from the Operator Hub and the Red Hat Marketplace . The Operations team that owns the IasC can decide the best approach to install the tools either using Helm3 or Operators . Typically a Cloud System Admin installs and sets up a new , providing a place for the developers to start developing the minimum viable product (MVP). The objective is to reduce the time required for a team to configure and prepare their development environment. The key benefit is to make the end-to-end CI/CD development lifecycle consistent across each platform and make the out-of-the-box developer experience as simple as possible. The installation is performed using Terraform , driven by scripts with a modular configuration so unneeded tools can be easily disabled or new tools added. The combination of tools selected are proven in the industry to deliver real value for modern cloud-native development.","title":"Overview"},{"location":"developer-intermediate/content-overview/#environment-components","text":"After installation, the consists of a set of CNCF tools installed into your nominated kubernetes cluster. This diagram illustrates the : The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools.","title":"Environment components"},{"location":"developer-intermediate/content-overview/#development-cluster","text":"The heart of the is a cluster: An or 3-node cluster Cluster namespace that encapsulates the tooling installed in the cluster: tools Cluster namespaces for deployment target environments: dev , test , and staging The following IBM Cloud services are created and bound to the cluster: Capability Service Description Logging LogDNA Logging Manage app logging LogDNA Monitoring SysDig Monitoring Manage monitoring of apps with SysDig AppID AppID Application Authentication Secure your apps, APIs and Kubernetes Ingress end points Cloudant Cloudant NoSQL Database NoSQL Database commonly used for data persistence Cloud Object Storage Cloud Object Storage Storage Storage service commonly used for binary content PostgreSQL PostgreSQL (used by SonarQube) SQL Database used for structure data persistence","title":"Development cluster"},{"location":"developer-intermediate/content-overview/#continuous-delivery-tools","text":"The following best-of-breed open source software tools are installed in the cluster's tools namespace: Capability Tool Bitnami Description Continuous Integration Jenkins CI Yes Jenkins is a common tool for Continuous Integration Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift Code Analysis SonarQube Yes SonarQube can scan code and display the results in a dashboard Artifact and Helm Storage Artifactory Yes Artifactory is an artifact storage and Helm chart repository Continuous Deployment ArgoCD ArgoCD support Continuous Delivery with GitOps Contract API Testing Pact Pact enables API contract testing Code Ready Workspace Eclipse CHE IDE for editing and managing code in a web browser If you want to find out more about IBM assets that help you get these common tools installed","title":"Continuous delivery tools"},{"location":"developer-intermediate/continuous-delivery/","text":"Introduction \u00b6 Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely. Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner. Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build. Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated. What is continuous delivery \u00b6 Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: - Continuous delivery deploys an application when a user manually triggers deployment - Continuous deployment deploys an application automatically when it is ready An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. These tests must be automated so that deployment can be automated. Until you have this set of automated tests and trust them sufficiently, stick with continuous delivery. ArgoCD \u00b6 ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. In your applications, application definitions, configurations, and environments should be declarative and version controlled. Also application deployment and lifecycle management should be automated, auditable, and easy to understand. All this can be done using Argo. Check these guides out if you want to know more about Argo - Argo CD - Declarative GitOps CD for Kubernetes . Continuous Delivery \u00b6 Continuous Delivery is the next step for Continuous Integration. The artifacts produced in the Continuous Integration stage will be deployed on a production like environment. It is more about making sure that the software is ready to be released and it can be deployed to production like environment at any time. Developer initially write code to implement a feature or make a change, compile it, and runs all the required tests. If it is running fine and working well, the next thing is to decide the release to the customer. Before doing the release, there are so many things one should take care of. We need to make sure all the configurations are done properly. Respective configuration files should be replaced correctly in the corresponding environments. Also, backups of the previous version of the software should be taken just in case to use them if the system breaks. There may be need to stop some of the services. For example, if your release involves updating the database, you need to stop some of the services which use that database. Also, when this process is done, we need to turn on the maintenance page because we don't want our users to panic seeing this site can't be reached. After doing all this, we test the software and if the tests are all working, we can restart the service we stopped previously. Initially, we deploy it to staging environment. If everything looks good, we will receive the approval and then we need to deploy it to production like environment. Once you get the approval, we need to do all the steps we did previously again. This process involves so many steps and it is hard to do all of them correctly every time. To make lives easier and not to miss any of the steps, automated deployment is necessary. This allows us to deploy more often. Just imagine, if you want to follow all the steps mentioned earlier for every small change, you tend to wait. If we do so and the changes are a lot, then there are more chances of failure and the system may break. So, making the release process automated is very important. By automation, it is easier to deploy smaller changes more frequently. It is also easier to rollback if there is a failure. Moreover, it reduces the overall risk of failure. This allows you to deploy to production like environment at any point of time. To make it possible, the software must all always be in a deliverable state. All your code should be compiled, tested, and the build should succeed. Continuous Deployment \u00b6 Continuous Deployment is the final step. In this stage, every change goes through the pipeline and if it passes all the tests, the code will be deployed into the production automatically. Every step should be automated in this process and the release quality depends mostly on the test suite as everything is automated. All the steps that apply in Continuous Delivery also applies here. You may do some things manually in case of Continuous Delivery but in Continuous Deployment, everything is automated. So, basically every piece of code that is pushed in to the SCM system gets automatically deployed in production like environment if the build is successful. The rationale behind the process is that you are going to deploy the code to production sooner or later anyway. But it is always recommended not to use it. Because there may be many factors that need to be considered before the release like marketing etc. It is hard to achieve this using Continuous Deployment. But, Continuous Delivery is a must as we are enabling the capability to deliver the software to any given environment at any given time. To understand continuous delivery and deployment better, feel free to watch this video . You can do some practical exercises and get a better understanding of continuous delivery References \u00b6 Sricharan Vadapalli (2018). DevOps: Continuous Delivery, Integration, and Deployment with DevOps. Publisher: Packt Publishing Sander Rossel (2017). Continuous Integration, Delivery, and Deployment. Publisher: Packt Publishing","title":"Continuous Delivery"},{"location":"developer-intermediate/continuous-delivery/#introduction","text":"Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely. Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner. Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build. Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.","title":"Introduction"},{"location":"developer-intermediate/continuous-delivery/#what-is-continuous-delivery","text":"Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: - Continuous delivery deploys an application when a user manually triggers deployment - Continuous deployment deploys an application automatically when it is ready An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. These tests must be automated so that deployment can be automated. Until you have this set of automated tests and trust them sufficiently, stick with continuous delivery.","title":"What is continuous delivery"},{"location":"developer-intermediate/continuous-delivery/#argocd","text":"ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. In your applications, application definitions, configurations, and environments should be declarative and version controlled. Also application deployment and lifecycle management should be automated, auditable, and easy to understand. All this can be done using Argo. Check these guides out if you want to know more about Argo - Argo CD - Declarative GitOps CD for Kubernetes .","title":"ArgoCD"},{"location":"developer-intermediate/continuous-delivery/#continuous-delivery","text":"Continuous Delivery is the next step for Continuous Integration. The artifacts produced in the Continuous Integration stage will be deployed on a production like environment. It is more about making sure that the software is ready to be released and it can be deployed to production like environment at any time. Developer initially write code to implement a feature or make a change, compile it, and runs all the required tests. If it is running fine and working well, the next thing is to decide the release to the customer. Before doing the release, there are so many things one should take care of. We need to make sure all the configurations are done properly. Respective configuration files should be replaced correctly in the corresponding environments. Also, backups of the previous version of the software should be taken just in case to use them if the system breaks. There may be need to stop some of the services. For example, if your release involves updating the database, you need to stop some of the services which use that database. Also, when this process is done, we need to turn on the maintenance page because we don't want our users to panic seeing this site can't be reached. After doing all this, we test the software and if the tests are all working, we can restart the service we stopped previously. Initially, we deploy it to staging environment. If everything looks good, we will receive the approval and then we need to deploy it to production like environment. Once you get the approval, we need to do all the steps we did previously again. This process involves so many steps and it is hard to do all of them correctly every time. To make lives easier and not to miss any of the steps, automated deployment is necessary. This allows us to deploy more often. Just imagine, if you want to follow all the steps mentioned earlier for every small change, you tend to wait. If we do so and the changes are a lot, then there are more chances of failure and the system may break. So, making the release process automated is very important. By automation, it is easier to deploy smaller changes more frequently. It is also easier to rollback if there is a failure. Moreover, it reduces the overall risk of failure. This allows you to deploy to production like environment at any point of time. To make it possible, the software must all always be in a deliverable state. All your code should be compiled, tested, and the build should succeed.","title":"Continuous Delivery"},{"location":"developer-intermediate/continuous-delivery/#continuous-deployment","text":"Continuous Deployment is the final step. In this stage, every change goes through the pipeline and if it passes all the tests, the code will be deployed into the production automatically. Every step should be automated in this process and the release quality depends mostly on the test suite as everything is automated. All the steps that apply in Continuous Delivery also applies here. You may do some things manually in case of Continuous Delivery but in Continuous Deployment, everything is automated. So, basically every piece of code that is pushed in to the SCM system gets automatically deployed in production like environment if the build is successful. The rationale behind the process is that you are going to deploy the code to production sooner or later anyway. But it is always recommended not to use it. Because there may be many factors that need to be considered before the release like marketing etc. It is hard to achieve this using Continuous Deployment. But, Continuous Delivery is a must as we are enabling the capability to deliver the software to any given environment at any given time. To understand continuous delivery and deployment better, feel free to watch this video . You can do some practical exercises and get a better understanding of continuous delivery","title":"Continuous Deployment"},{"location":"developer-intermediate/continuous-delivery/#references","text":"Sricharan Vadapalli (2018). DevOps: Continuous Delivery, Integration, and Deployment with DevOps. Publisher: Packt Publishing Sander Rossel (2017). Continuous Integration, Delivery, and Deployment. Publisher: Packt Publishing","title":"References"},{"location":"developer-intermediate/continuous-delivery-handson/","text":"OpenShift Pre-requisites \u00b6 Make sure your environment is setup properly for the lab. Check the Environment Setup page for your setup. ArgoCD Installation \u00b6 Create the namespace argocd to install argocd oc new-project argocd Install ArgoCD as follows. oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-operator.yaml When installing the tutorial, make sure you wait until the argocd-operator is finished before installing the argocd-cr..or it will fail. You can do this: oc get ClusterServiceVersion/argocd-operator-helm.v0.0.3 -n argocd NAME DISPLAY VERSION REPLACES PHASE argocd-operator-helm.v0.0.3 Argo CD Operator ( Helm ) 0 .0.3 argocd-operator-helm.v0.0.2 Succeeded and wait for the \"succeeded\" to come up before proceeding. oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-cr.yaml Install the argocd CLI, for example on OSX use brew brew tap argoproj/tap brew install argoproj/tap/argocd Set an environment variable ARGOCD_URL using the EXTERNAL-IP export ARGOCD_SERVER = \" $( oc get route argocd-server -n argocd -o jsonpath = '{.status.ingress[0].host}' ) \" export ARGOCD_URL = \"https:// $ARGOCD_SERVER \" echo ARGOCD_URL = $ARGOCD_URL echo ARGOCD_SERVER = $ARGOCD_SERVER Deploying the app \u00b6 Login into the UI. open $ARGOCD_URL Use admin as the username and get the password with the following command, it's the name of the pod for the argo-server oc get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 For example the output is similar to this: argocd-server-b54756f69-jncc9 Now go back to the ArgoCD home and click on NEW APP . Add the below details: Application Name: sample Project - default SYNC POLICY: Manual REPO URL: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Revision: HEAD Path: openshift Cluster - Select the default one https://kubernetes.default.svc to deploy in-cluster Namespace - default Click Create to finish You will now see the available apps. - Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying. To sync the application, click `SYNC` and then `SYNCHRONIZE`. ![out of sync](images/out_of_sync.png) - Wait till the app is deployed. ![synched app](images/synched_app.png) - Once the app is deployed, click on it to see the details. ![sample app deployed](images/sample_app_deployed.png) ![sample app full deployment](images/sample_app_full_deployment.png) ## Verifying the deployment - Access the app to verify if it is correctly deployed. - List the cloudnativesampleapp-service route ``` oc get route ``` It should have an IP under `EXTERNAL-IP` column ``` NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cloudnative-sample cloudnative-sample-default.apps-crc.testing cloudnativesampleapp-service 9080 None ``` - Set an environment variable `APP_URL` using the `EXTERNAL-IP` ``` export APP_URL=\"http://$(oc get route cloudnative-sample -o jsonpath='{.status.ingress[0].host}')\" echo ARGOCD_SERVER=$APP_URL ``` - Access the url using `curl` ``` curl \"$APP_URL/greeting?name=Carlos\" ``` ``` {\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"} ``` ## Using the ArgoCD CLI - Login using the cli. - Login as the `admin` user. - The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the following command. ```bash oc get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d'/' -f 2 ``` - Now login as follows. ```bash argocd login $ARGOCD_SERVER ``` ``` WARNING: server certificate had error: x509: cannot validate certificate for 10.97.240.99 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y Username: admin Password: 'admin' logged in successfully Context '10.97.240.99' updated ``` - List the applications ```bash argocd app list ``` ``` NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET sample https://kubernetes.default.svc default default Synced Healthy <none> <none> https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy openshift HEAD ``` - Get application details ```bash argocd app get sample ``` ``` Name: sample Project: default Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.240.99/applications/sample Repo: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Target: HEAD Path: openshift SyncWindow: Sync Allowed Sync Policy: <none> Sync Status: Synced to HEAD (9684037) Health Status: Healthy GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default cloudnativesampleapp-service Synced Healthy service/cloudnativesampleapp-service created apps Deployment default cloudnativesampleapp-deployment Synced Healthy deployment.apps/cloudnativesampleapp-deployment created ``` - Show application deployment history ```bash argocd app history sample ``` ``` ID DATE REVISION 0 2020-02-12 21:10:32 -0500 EST HEAD (9684037) ``` ## References - [ArgoCD](https://argoproj.github.io/argo-cd/) IKS Pre-requisites \u00b6 Make sure your environment is setup properly for the lab. Check the Environment Setup page for your setup. ArgoCD Installation \u00b6 Create the namespace argocd to install argocd kubectl create namespace argocd Install ArgoCD as follows. kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Install the argocd CLI, for example on OSX use brew brew tap argoproj/tap brew install argoproj/tap/argocd To allow access via LoadBalancer and leverage the minikube tunnel running in the background, patch the argocd server service kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' List the argocd-server service kubectl get svc argocd-server -n argocd It should have an IP under EXTERNAL-IP column NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE argocd-server LoadBalancer 10.97.240.99 10.97.240.99 80:30286/TCP,443:31716/TCP 41s Set an environment variable ARGOCD_URL using the EXTERNAL-IP export ARGOCD_SERVER = \" $( kubectl get svc argocd-server -n argocd -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) \" export ARGOCD_URL = \"https:// $ARGOCD_SERVER \" echo ARGOCD_URL = $ARGOCD_URL echo ARGOCD_SERVER = $ARGOCD_SERVER Deploying the app \u00b6 Login into the UI. open $ARGOCD_URL Use admin as the username and get the password with the following command, it's the name of the pod for the argo-server kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 For example the output is similar to this: argocd-server-b54756f69-jncc9 Now go back to the ArgoCD home and click on NEW APP . Add the below details: Application Name: sample Project - default SYNC POLICY: Manual REPO URL: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Revision: HEAD Path: kubernetes Cluster - Select the default one https://kubernetes.default.svc to deploy in-cluster Namespace - default Click Create to finish You will now see the available apps. Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying. To sync the application, click SYNC and then SYNCHRONIZE . Wait till the app is deployed. Once the app is deployed, click on it to see the details. Verifying the deployment \u00b6 Access the app to verify if it is correctly deployed. To allow access via LoadBalancer and leverage the minikube tunnel running in the background, patch the argocd server service kubectl patch svc cloudnativesampleapp-service -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' List the cloudnativesampleapp-service service kubectl get svc cloudnativesampleapp-service It should have an IP under EXTERNAL-IP column NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cloudnativesampleapp-service LoadBalancer 10.109.242.212 10.109.242.212 9080:31905/TCP 13m Set an environment variable APP_URL using the EXTERNAL-IP export APP_URL = \"http:// $( kubectl get svc cloudnativesampleapp-service -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) :9080\" echo ARGOCD_SERVER = $APP_URL Access the url using curl curl \" $APP_URL /greeting?name=Carlos\" {\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"} Using the ArgoCD CLI \u00b6 Login using the cli. Login as the admin user. The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the following command. kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 Now login as follows. argocd login $ARGOCD_SERVER WARNING: server certificate had error: x509: cannot validate certificate for 10.97.240.99 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y Username: admin Password: 'admin' logged in successfully Context '10.97.240.99' updated - List the applications argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET sample https://kubernetes.default.svc default default Synced Healthy <none> <none> https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy kubernetes HEAD - Get application details argocd app get sample Name: sample Project: default Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.240.99/applications/sample Repo: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Target: HEAD Path: kubernetes SyncWindow: Sync Allowed Sync Policy: <none> Sync Status: Synced to HEAD (9684037) Health Status: Healthy GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default cloudnativesampleapp-service Synced Healthy service/cloudnativesampleapp-service created apps Deployment default cloudnativesampleapp-deployment Synced Healthy deployment.apps/cloudnativesampleapp-deployment created - Show application deployment history argocd app history sample ID DATE REVISION 0 2020-02-12 21:10:32 -0500 EST HEAD (9684037) References \u00b6 ArgoCD","title":"ArgoCD Lab"},{"location":"developer-intermediate/continuous-delivery-handson/#pre-requisites","text":"Make sure your environment is setup properly for the lab. Check the Environment Setup page for your setup.","title":"Pre-requisites"},{"location":"developer-intermediate/continuous-delivery-handson/#argocd-installation","text":"Create the namespace argocd to install argocd oc new-project argocd Install ArgoCD as follows. oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-operator.yaml When installing the tutorial, make sure you wait until the argocd-operator is finished before installing the argocd-cr..or it will fail. You can do this: oc get ClusterServiceVersion/argocd-operator-helm.v0.0.3 -n argocd NAME DISPLAY VERSION REPLACES PHASE argocd-operator-helm.v0.0.3 Argo CD Operator ( Helm ) 0 .0.3 argocd-operator-helm.v0.0.2 Succeeded and wait for the \"succeeded\" to come up before proceeding. oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/argo-lab/argocd-cr.yaml Install the argocd CLI, for example on OSX use brew brew tap argoproj/tap brew install argoproj/tap/argocd Set an environment variable ARGOCD_URL using the EXTERNAL-IP export ARGOCD_SERVER = \" $( oc get route argocd-server -n argocd -o jsonpath = '{.status.ingress[0].host}' ) \" export ARGOCD_URL = \"https:// $ARGOCD_SERVER \" echo ARGOCD_URL = $ARGOCD_URL echo ARGOCD_SERVER = $ARGOCD_SERVER","title":"ArgoCD Installation"},{"location":"developer-intermediate/continuous-delivery-handson/#deploying-the-app","text":"Login into the UI. open $ARGOCD_URL Use admin as the username and get the password with the following command, it's the name of the pod for the argo-server oc get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 For example the output is similar to this: argocd-server-b54756f69-jncc9 Now go back to the ArgoCD home and click on NEW APP . Add the below details: Application Name: sample Project - default SYNC POLICY: Manual REPO URL: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Revision: HEAD Path: openshift Cluster - Select the default one https://kubernetes.default.svc to deploy in-cluster Namespace - default Click Create to finish You will now see the available apps. - Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying. To sync the application, click `SYNC` and then `SYNCHRONIZE`. ![out of sync](images/out_of_sync.png) - Wait till the app is deployed. ![synched app](images/synched_app.png) - Once the app is deployed, click on it to see the details. ![sample app deployed](images/sample_app_deployed.png) ![sample app full deployment](images/sample_app_full_deployment.png) ## Verifying the deployment - Access the app to verify if it is correctly deployed. - List the cloudnativesampleapp-service route ``` oc get route ``` It should have an IP under `EXTERNAL-IP` column ``` NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cloudnative-sample cloudnative-sample-default.apps-crc.testing cloudnativesampleapp-service 9080 None ``` - Set an environment variable `APP_URL` using the `EXTERNAL-IP` ``` export APP_URL=\"http://$(oc get route cloudnative-sample -o jsonpath='{.status.ingress[0].host}')\" echo ARGOCD_SERVER=$APP_URL ``` - Access the url using `curl` ``` curl \"$APP_URL/greeting?name=Carlos\" ``` ``` {\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"} ``` ## Using the ArgoCD CLI - Login using the cli. - Login as the `admin` user. - The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the following command. ```bash oc get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d'/' -f 2 ``` - Now login as follows. ```bash argocd login $ARGOCD_SERVER ``` ``` WARNING: server certificate had error: x509: cannot validate certificate for 10.97.240.99 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y Username: admin Password: 'admin' logged in successfully Context '10.97.240.99' updated ``` - List the applications ```bash argocd app list ``` ``` NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET sample https://kubernetes.default.svc default default Synced Healthy <none> <none> https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy openshift HEAD ``` - Get application details ```bash argocd app get sample ``` ``` Name: sample Project: default Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.240.99/applications/sample Repo: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Target: HEAD Path: openshift SyncWindow: Sync Allowed Sync Policy: <none> Sync Status: Synced to HEAD (9684037) Health Status: Healthy GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default cloudnativesampleapp-service Synced Healthy service/cloudnativesampleapp-service created apps Deployment default cloudnativesampleapp-deployment Synced Healthy deployment.apps/cloudnativesampleapp-deployment created ``` - Show application deployment history ```bash argocd app history sample ``` ``` ID DATE REVISION 0 2020-02-12 21:10:32 -0500 EST HEAD (9684037) ``` ## References - [ArgoCD](https://argoproj.github.io/argo-cd/) IKS","title":"Deploying the app"},{"location":"developer-intermediate/continuous-delivery-handson/#pre-requisites_1","text":"Make sure your environment is setup properly for the lab. Check the Environment Setup page for your setup.","title":"Pre-requisites"},{"location":"developer-intermediate/continuous-delivery-handson/#argocd-installation_1","text":"Create the namespace argocd to install argocd kubectl create namespace argocd Install ArgoCD as follows. kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Install the argocd CLI, for example on OSX use brew brew tap argoproj/tap brew install argoproj/tap/argocd To allow access via LoadBalancer and leverage the minikube tunnel running in the background, patch the argocd server service kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' List the argocd-server service kubectl get svc argocd-server -n argocd It should have an IP under EXTERNAL-IP column NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE argocd-server LoadBalancer 10.97.240.99 10.97.240.99 80:30286/TCP,443:31716/TCP 41s Set an environment variable ARGOCD_URL using the EXTERNAL-IP export ARGOCD_SERVER = \" $( kubectl get svc argocd-server -n argocd -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) \" export ARGOCD_URL = \"https:// $ARGOCD_SERVER \" echo ARGOCD_URL = $ARGOCD_URL echo ARGOCD_SERVER = $ARGOCD_SERVER","title":"ArgoCD Installation"},{"location":"developer-intermediate/continuous-delivery-handson/#deploying-the-app_1","text":"Login into the UI. open $ARGOCD_URL Use admin as the username and get the password with the following command, it's the name of the pod for the argo-server kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 For example the output is similar to this: argocd-server-b54756f69-jncc9 Now go back to the ArgoCD home and click on NEW APP . Add the below details: Application Name: sample Project - default SYNC POLICY: Manual REPO URL: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Revision: HEAD Path: kubernetes Cluster - Select the default one https://kubernetes.default.svc to deploy in-cluster Namespace - default Click Create to finish You will now see the available apps. Initially, the app will be out of sync. It is yet to be deployed. You need to sync it for deploying. To sync the application, click SYNC and then SYNCHRONIZE . Wait till the app is deployed. Once the app is deployed, click on it to see the details.","title":"Deploying the app"},{"location":"developer-intermediate/continuous-delivery-handson/#verifying-the-deployment","text":"Access the app to verify if it is correctly deployed. To allow access via LoadBalancer and leverage the minikube tunnel running in the background, patch the argocd server service kubectl patch svc cloudnativesampleapp-service -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' List the cloudnativesampleapp-service service kubectl get svc cloudnativesampleapp-service It should have an IP under EXTERNAL-IP column NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cloudnativesampleapp-service LoadBalancer 10.109.242.212 10.109.242.212 9080:31905/TCP 13m Set an environment variable APP_URL using the EXTERNAL-IP export APP_URL = \"http:// $( kubectl get svc cloudnativesampleapp-service -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) :9080\" echo ARGOCD_SERVER = $APP_URL Access the url using curl curl \" $APP_URL /greeting?name=Carlos\" {\"id\":2,\"content\":\"Welcome to Cloudnative bootcamp !!! Hello, Carlos :)\"}","title":"Verifying the deployment"},{"location":"developer-intermediate/continuous-delivery-handson/#using-the-argocd-cli","text":"Login using the cli. Login as the admin user. The initial password is autogenerated to be the pod name of the Argo CD API server. This can be retrieved with the following command. kubectl get pods -n argocd -l app.kubernetes.io/name = argocd-server -o name | cut -d '/' -f 2 Now login as follows. argocd login $ARGOCD_SERVER WARNING: server certificate had error: x509: cannot validate certificate for 10.97.240.99 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y Username: admin Password: 'admin' logged in successfully Context '10.97.240.99' updated - List the applications argocd app list NAME CLUSTER NAMESPACE PROJECT STATUS HEALTH SYNCPOLICY CONDITIONS REPO PATH TARGET sample https://kubernetes.default.svc default default Synced Healthy <none> <none> https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy kubernetes HEAD - Get application details argocd app get sample Name: sample Project: default Server: https://kubernetes.default.svc Namespace: default URL: https://10.97.240.99/applications/sample Repo: https://github.com/ibm-cloud-architecture/cloudnative_sample_app_deploy Target: HEAD Path: kubernetes SyncWindow: Sync Allowed Sync Policy: <none> Sync Status: Synced to HEAD (9684037) Health Status: Healthy GROUP KIND NAMESPACE NAME STATUS HEALTH HOOK MESSAGE Service default cloudnativesampleapp-service Synced Healthy service/cloudnativesampleapp-service created apps Deployment default cloudnativesampleapp-deployment Synced Healthy deployment.apps/cloudnativesampleapp-deployment created - Show application deployment history argocd app history sample ID DATE REVISION 0 2020-02-12 21:10:32 -0500 EST HEAD (9684037)","title":"Using the ArgoCD CLI"},{"location":"developer-intermediate/continuous-delivery-handson/#references","text":"ArgoCD","title":"References"},{"location":"developer-intermediate/continuous-integration/","text":"Introduction \u00b6 Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely. Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner. Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build. Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated. What is continuous integration ? \u00b6 Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. This quote helps explain it: Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly \u2013 Martin Fowler Continuous Integration \u00b6 Continuous Integration is the first step to deliver high quality and efficient software. It is all about making sure that the software is working all the time and it is deployable at all times. It initially begins with the development . Developers implement the new features or changes to the existing ones in small, tested batches and commit this code to a Source Code Management (SCM) system such as Github, SVN, bitbucket etc. The next step will be the build . This is done by the CI server like Tekton, Jenkins, Travis CI, Circle CI etc. The CI server is linked up with the GitHub repository to trigger the build either using web hook or by polling the repository for changes. When it gets the access to the latest code changes in the SCM system, it executes the build script generating new deployment artifacts. In case of build failures, it generates feedback and send the information to the corresponding members. Once this is completed, developers package the build using Docker, Cloud Foundry etc. The builds along with their runtimes are packaged as immutable images. These images are later used for deployment in the further stages. Let us go through these steps in detail. Development \u00b6 During the development, developers initially work in their local environments. They define code for new features or make changes to the existing feature. They make all this changes or additions in their local workspaces which are integrated with an Integrated Development Environment runtime. They may use build tools installed physically in their own workstation or they may use the ones existing on cloud (Web IDEs) based on their convenience. Once the code is defined, they do unit testing. Along with it they also do performance checks, data validations etc. They also test the software locally and validate the code changes using tests. If everything is fine, they push the changes to the source code management. Then a code review will be performed and if everything seems correct, then these changes will be merged into the main stream of the SCM system. Source Control \u00b6 Continuous Integration starts with source control system. Source Control systems helps us to store all the code in a single place. This source code can be accessed by multiple developers. It is easy to pull the changes, change them and push them back into the source control system. These can be viewed by other developers too. If you consider Git, in this source control system, you can have multiple branches of the same software and you can work on different things without conflicting with other parts of the software. For instance, let us say you have three different branches for dev, test and prod. Initially when the code is defined, it goes into dev branch. When the testing is completed, it is moved to test branch. And finally when we get the approval, it goes into prod branch. Or you may have a single main branch and create new branches for every release. These are few examples just for your understanding. This gives developers the capability to work on same project. They can work on different things. Even though if they work on the same thing, they need not worry about the overwriting. Source Control system is the place where you can store pretty much everything related to the software. Not only code, it is good to store all other things that are required to run your software like build scripts, test scripts, configurations etc in it. Whenever a code push is made by the developer to the source control system, it should be validated by the automated build server. It is always a good practice to keep the check-ins small. If you do large check-ins, it is hard to identify any error or bugs if they occur. CI Server \u00b6 CI Server is used to automate the builds. There are many CI server software available today like Jenkins, Tavis CI, Bamboo etc. The source control repository is monitored by the CI server. The CI server is integrated to the repository and it starts the build whenever a change is pushed into the repository. The build has several steps like compiling the code, running unit tests, checking the code coverage, code linting, validating style guidelines, code minification etc. If the build is passed, it creates the build artifact. Otherwise, it will notify the failure. For instance, if the developer made a syntactical error like may be missing a semi colon and pushed in the code, then the unit tests fail. CI server will report this failure to the team by sending an email or however you configured it to send the information. Also, defining the failure conditions is up to the developer. If it is a compilation error, it obviously fails. You can also add conditions like there must be a code coverage of 80 % for your code etc. CI server builds the software. They also inform about the failures and successes. They build artifacts which will be later used to deploy the application. Testing \u00b6 Whenever a build is done on the CI server, it undergoes many tests to make sure the quality of the software is high. Various tests are performed as part of this process. Some of the important ones are as follows. Unit tests - Unit tests are defined to test small isolated parts of code. It is simple piece of code which calls a method to be tested. It passes the required input and verifies if the output obtained is correct. Integration tests - Integration tests tests the system as a whole. It validates if the entire system is working as expected. For example, if you want to know if your application properly interacts with the backend or to validate if the frontend properly interacts with the backend these tests will help to resolve them. Acceptance tests - An acceptance test tests whether a specific functionality works as described in the specification. These tests are written in the perspective of the user and it validates if the created feature is same as the requested one. Smoke tests - Smoke tests test if your software is working in a production environment. It tests the basic functionality and makes sure the most important parts of your software work properly. Automation \u00b6 Automation is a very important. When doing CI, the build process is all automated. It also uses automated tests to make sure that software is properly working after new code changes. Teamwork \u00b6 All the code changes are compiled. To reduce the bugs, we use automation testing as part of CI. Also, if the executable is tested on an environment which resembles your production environment, it makes your life easier. All this process cannot be just done by a single person. For suppose, if you develop a code and define unit tests to make sure everything works and other person from your team pushes large pieces of code without any unit tests, then it will be of no use. So, every member of the team should make efforts to successfully define the Continuous Integration for your software. You can do some practical exercises and get a better understanding of continuous integration References \u00b6 Sricharan Vadapalli (2018). DevOps: Continuous Delivery, Integration, and Deployment with DevOps. Publisher: Packt Publishing Sander Rossel (2017). Continuous Integration, Delivery, and Deployment. Publisher: Packt Publishing","title":"Continuous Integration"},{"location":"developer-intermediate/continuous-integration/#introduction","text":"Continuous Integration, Delivery, and Deployment are important devOps practices and we often hear a lot about them. These processes are valuable and ensures that the software is up to date timely. Continuous Integration is an automation process which allows developers to integrate their work into a repository. When a developer pushes his work into the source code repository, it ensures that the software continues to work properly. It helps to enable collaborative development across the teams and also helps to identify the integration bugs sooner. Continuous Delivery comes after Continuous Integration. It prepares the code for release. It automates the steps that are needed to deploy a build. Continuous Deployment is the final step which succeeds Continuous Delivery. It automatically deploys the code whenever a code change is done. Entire process of deployment is automated.","title":"Introduction"},{"location":"developer-intermediate/continuous-integration/#what-is-continuous-integration","text":"Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. This quote helps explain it: Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly \u2013 Martin Fowler","title":"What is continuous integration ?"},{"location":"developer-intermediate/continuous-integration/#continuous-integration","text":"Continuous Integration is the first step to deliver high quality and efficient software. It is all about making sure that the software is working all the time and it is deployable at all times. It initially begins with the development . Developers implement the new features or changes to the existing ones in small, tested batches and commit this code to a Source Code Management (SCM) system such as Github, SVN, bitbucket etc. The next step will be the build . This is done by the CI server like Tekton, Jenkins, Travis CI, Circle CI etc. The CI server is linked up with the GitHub repository to trigger the build either using web hook or by polling the repository for changes. When it gets the access to the latest code changes in the SCM system, it executes the build script generating new deployment artifacts. In case of build failures, it generates feedback and send the information to the corresponding members. Once this is completed, developers package the build using Docker, Cloud Foundry etc. The builds along with their runtimes are packaged as immutable images. These images are later used for deployment in the further stages. Let us go through these steps in detail.","title":"Continuous Integration"},{"location":"developer-intermediate/continuous-integration/#development","text":"During the development, developers initially work in their local environments. They define code for new features or make changes to the existing feature. They make all this changes or additions in their local workspaces which are integrated with an Integrated Development Environment runtime. They may use build tools installed physically in their own workstation or they may use the ones existing on cloud (Web IDEs) based on their convenience. Once the code is defined, they do unit testing. Along with it they also do performance checks, data validations etc. They also test the software locally and validate the code changes using tests. If everything is fine, they push the changes to the source code management. Then a code review will be performed and if everything seems correct, then these changes will be merged into the main stream of the SCM system.","title":"Development"},{"location":"developer-intermediate/continuous-integration/#source-control","text":"Continuous Integration starts with source control system. Source Control systems helps us to store all the code in a single place. This source code can be accessed by multiple developers. It is easy to pull the changes, change them and push them back into the source control system. These can be viewed by other developers too. If you consider Git, in this source control system, you can have multiple branches of the same software and you can work on different things without conflicting with other parts of the software. For instance, let us say you have three different branches for dev, test and prod. Initially when the code is defined, it goes into dev branch. When the testing is completed, it is moved to test branch. And finally when we get the approval, it goes into prod branch. Or you may have a single main branch and create new branches for every release. These are few examples just for your understanding. This gives developers the capability to work on same project. They can work on different things. Even though if they work on the same thing, they need not worry about the overwriting. Source Control system is the place where you can store pretty much everything related to the software. Not only code, it is good to store all other things that are required to run your software like build scripts, test scripts, configurations etc in it. Whenever a code push is made by the developer to the source control system, it should be validated by the automated build server. It is always a good practice to keep the check-ins small. If you do large check-ins, it is hard to identify any error or bugs if they occur.","title":"Source Control"},{"location":"developer-intermediate/continuous-integration/#ci-server","text":"CI Server is used to automate the builds. There are many CI server software available today like Jenkins, Tavis CI, Bamboo etc. The source control repository is monitored by the CI server. The CI server is integrated to the repository and it starts the build whenever a change is pushed into the repository. The build has several steps like compiling the code, running unit tests, checking the code coverage, code linting, validating style guidelines, code minification etc. If the build is passed, it creates the build artifact. Otherwise, it will notify the failure. For instance, if the developer made a syntactical error like may be missing a semi colon and pushed in the code, then the unit tests fail. CI server will report this failure to the team by sending an email or however you configured it to send the information. Also, defining the failure conditions is up to the developer. If it is a compilation error, it obviously fails. You can also add conditions like there must be a code coverage of 80 % for your code etc. CI server builds the software. They also inform about the failures and successes. They build artifacts which will be later used to deploy the application.","title":"CI Server"},{"location":"developer-intermediate/continuous-integration/#testing","text":"Whenever a build is done on the CI server, it undergoes many tests to make sure the quality of the software is high. Various tests are performed as part of this process. Some of the important ones are as follows. Unit tests - Unit tests are defined to test small isolated parts of code. It is simple piece of code which calls a method to be tested. It passes the required input and verifies if the output obtained is correct. Integration tests - Integration tests tests the system as a whole. It validates if the entire system is working as expected. For example, if you want to know if your application properly interacts with the backend or to validate if the frontend properly interacts with the backend these tests will help to resolve them. Acceptance tests - An acceptance test tests whether a specific functionality works as described in the specification. These tests are written in the perspective of the user and it validates if the created feature is same as the requested one. Smoke tests - Smoke tests test if your software is working in a production environment. It tests the basic functionality and makes sure the most important parts of your software work properly.","title":"Testing"},{"location":"developer-intermediate/continuous-integration/#automation","text":"Automation is a very important. When doing CI, the build process is all automated. It also uses automated tests to make sure that software is properly working after new code changes.","title":"Automation"},{"location":"developer-intermediate/continuous-integration/#teamwork","text":"All the code changes are compiled. To reduce the bugs, we use automation testing as part of CI. Also, if the executable is tested on an environment which resembles your production environment, it makes your life easier. All this process cannot be just done by a single person. For suppose, if you develop a code and define unit tests to make sure everything works and other person from your team pushes large pieces of code without any unit tests, then it will be of no use. So, every member of the team should make efforts to successfully define the Continuous Integration for your software. You can do some practical exercises and get a better understanding of continuous integration","title":"Teamwork"},{"location":"developer-intermediate/continuous-integration/#references","text":"Sricharan Vadapalli (2018). DevOps: Continuous Delivery, Integration, and Deployment with DevOps. Publisher: Packt Publishing Sander Rossel (2017). Continuous Integration, Delivery, and Deployment. Publisher: Packt Publishing","title":"References"},{"location":"developer-intermediate/continuous-integration-handson/","text":"Pre-requisites \u00b6 Make sure your environment is properly setup. Follow the instructions here OpenShift SetUp \u00b6 Tekton CLI Installation \u00b6 Tekton CLI is command line utility used to interact with the Tekton resources. Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn For MacOS for example you can use brew brew tap tektoncd/tools brew install tektoncd/tools/tektoncd-cli Verify the Tekton cli tkn version The command should show a result like: $ tkn version Client version: 0.8.0 If you already have the tkn install you can upgrade running brew upgrade tektoncd/tools/tektoncd-cli Tekton Pipelines Installation \u00b6 To deploy the Tekton pipelines: oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/tekton-lab/tekton-operator.yaml Note : It will take few mins for the Tekton pipeline components to be installed, you an watch the status using the command: oc get pods -n openshift-operators You can use Ctrl+c to terminate the watch A successful deployment of Tekton pipelines will show the following pods: NAME READY STATUS RESTARTS AGE openshift-pipelines-operator-9cdbbb854-x9tvs 1/1 Running 0 25s Create Target Namespace \u00b6 Set the environment variable NAMESPACE to tekton-demo , if you open a new terminal remember to set this environment again export NAMESPACE=tekton-demo Create a the namespace using the variable NAMESPACE oc create namespace $NAMESPACE Pipeline Resources \u00b6 Pipeline Resource Creation \u00b6 Create a PipelineResource of type git \u00b6 Create the file git.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: source spec: type: git params: - name: revision value: master - name: url value: https://github.com/ibm-cloud-architecture/cloudnative_sample_app Verify the file content cat git.yaml Create a PipelineResource of type image \u00b6 Set the environment variable DOCKER_USERNAME to your dockerhub account, replace <REPLACEME> with your docker username, keep the quotes export DOCKER_USERNAME='<REPLACEME>' Create the file image.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: image spec: type: image params: - name: url value: index.docker.io/$DOCKER_USERNAME/cloudnative_sample_app Verify the file content, and make sure the url value is valid with your dockerhub username replaced cat image.yaml Pipeline Resources deployment \u00b6 Each pipeline resource has: name : the name using which it will be referred in other places type : the type of the pipeline resource, in this example we have two types git - this type of resource refers to a GitHub repository image - this type of resource is linux container image params : each type can have one or more parameters that will be used to configure the underlying type. In the above example for the git-source pipeline resource, the parameters url and revision are used to identify the GitHub repository url and revision of the sources respectively. More details on other types of pipeline resource types is available here . Create the pipeline resources using the command: oc apply -f git.yaml -n $NAMESPACE oc apply -f image.yaml -n $NAMESPACE Verify the deployed resource \u00b6 Use the Tekton cli to list the created resources tkn res ls -n $NAMESPACE The above command should list two resources as shown below: NAME TYPE DETAILS source git url: https://github.com/ibm-cloud-architecture/cloudnative_sample_app image image url: index.docker.io/yourdockerhubusername/cloudnative_sample_app Use the command help via tkn res --help Use the Tekton cli to describe the git resource tkn res describe source -n $NAMESPACE The output should look like this: Name: source Namespace: tekton-demo PipelineResource Type: git Params NAME VALUE revision master url https://github.com/ibm-cloud-architecture/cloudnative_sample_app Secret Params No secret params Use the Tekton cli to describe the git resource tkn res describe image -n $NAMESPACE The output should look like this: Name: image Namespace: tekton-demo PipelineResource Type: image Params NAME VALUE url index.docker.io/myusername/cloudnative_sample_app Secret Params No secret params Tasks \u00b6 Task Creation \u00b6 Create the below yaml files. The following snippet shows what a Tekton Task YAML looks like: Create the file test_task.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: java-test spec: inputs: resources: - name: source type: git params: - name: maven-image type: string default: maven:3.3-jdk-8 steps: - name: test image: $(inputs.params.maven-image) workingdir: $(inputs.resources.source.path) command: [\"/bin/bash\"] args: - -c - | set -e mvn test echo \"tests passed with rc=$?\" volumeMounts: - name: m2-repository mountPath: /.m2 volumes: - name: m2-repository emptyDir: {} Each Task has the following: name - the unique name using which the task can be referred inputs - the inputs to the task resources - the pipeline resources that will be used in the task e.g. git-source name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type params - the parameters that will be used in the task steps. Each parameter has name - the name of the parameter description - the description of the parameter default - the default value of parameter Note : The TaskRun or PipelineRun could override the parameter values, if no parameter value is passed then the default value will be used. outputs the pipeline resource that will end artifact of the task. In the above example the build will produce a container image artifact. resources - the pipeline resources that will be used in the task e.g. builtImage name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type steps - One or more sub-tasks that will be executed in the defined order. The step has all the attributes like a Pod spec stepTemplate - when there is a need to have similar container configuration across all steps of a the task, we can have them defined in the stepTemplate, the task steps will inherit them implicitly in all steps. In the example above we define the resources and securityContext for all the steps volumes - the task can also mount external volumes using the volumes attribute. The parameters that were part of the spec inputs params can be used in the steps using the notation $(<variable-name>) . Task Deploy \u00b6 The application test task could be created using the command: oc apply -f test_task.yaml -n $NAMESPACE We will use the Tekton cli to inspect the created resources tkn task ls -n $NAMESPACE The above command should list one Task as shown below: NAME AGE java-test 22 seconds ago TaskRun \u00b6 The TaskRun is used to run a specific task independently. In the following section we will run the build-app task created in the previous step TaskRun Creation \u00b6 The following snippet shows what a Tekton TaskRun YAML looks like: Create the file test_taskrun.yaml apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: generateName: test-task-run- spec: taskRef: name: java-test inputs: resources: - name: source resourceRef: name: source generateName - since the TaskRun can be run many times, in order to have unique name across the TaskRun ( helpful when checking the TaskRun history) we use this generateName instead of name. When Kubernetes sees generateName it will generate unique set of characters and suffix the same to build-app-, similar to how pod names are generated taskRef - this is used to refer to the Task by its name that will be run as part of this TaskRun. In this example we use build-app Task. As described in the earlier section that the Task inputs and outputs could be overridden via TaskRun. In this example we make the Task Run spec > inputs > resources > source to refer to pipeline resource source via the resourceRef . The application test task(java-maven-test) could be run using the command: oc create -n $NAMESPACE -f test_taskrun.yaml Note - As tasks will use generated name, never use oc apply -f test_taskrun.yaml We will use the Tekton cli to inspect the created resources: tkn tr ls -n $NAMESPACE The above command should list one TaskRun as shown below: NAME STARTED DURATION STATUS test-task-run-q6s8c 1 minute ago --- Running(Pending) Note - It will take few seconds for the TaskRun to show status as Running as it needs to download the container images. To check the logs of the Task Run using the tkn : tkn tr logs -f -a -n $NAMESPACE Note - Each task step will be run within a container of its own. The -f or -a allows to tail the logs from all the containers of the task. For more options run tkn tr logs --help If you see the TaskRun status as Failed or Error use the following command to check the reason for error: tkn tr describe <taskrun-name> -n $NAMESPACE If it is successful, you will see something like below. tkn tr ls -n $NAMESPACE The above command should list one TaskRun as shown below: NAME STARTED DURATION STATUS test-task-run-q6s8c 47 seconds ago 34 seconds Succeeded Creating additional tasks and deploying them \u00b6 Create a Task to build a container image and push to the registry This task will be later used by the pipeline. Download the task file task-buildah.yaml to build the image, push the image to the registry: Create the buildah Task using the file and the command: oc apply -f task-buildah.yaml -n $NAMESPACE Use the Tekton cli to inspect the created resources tkn task ls -n $NAMESPACE The above command should list one Task as shown below: NAME AGE buildah 4 seconds ago java-test 46 minutes ago To access the docker registry, create the required secret as follows. Create the environment variables to be use, replace with real values and include the single quotes: export DOCKER_USERNAME='<DOCKER_USERNAME>' export DOCKER_PASSWORD='<DOCKER_PASSWORD>' export DOCKER_EMAIL='<DOCKER_EMAIL>' Run the following command to create a secret regcred in the namespace NAMESPACE oc create secret docker-registry regcred \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=${DOCKER_USERNAME} \\ --docker-password=${DOCKER_PASSWORD} \\ --docker-email=${DOCKER_EMAIL} \\ -n ${NAMESPACE} Before creating, replace the values as mentioned above. Note: If your docker password contains special characters in it, please enclose the password in double quotes or place an escape character before each special character. (Optional) Only if you have problems with the credentials you can recreate it, but you have to deleted first oc delete secret regcred -n $NAMESPACE Before we run the Task using TaskRun let us create the Kubernetes service account and attach the needed permissions to the service account, the following Kubernetes resource defines a service account called pipeline in namespace $NAMESPACE who will have administrative role within the $NAMESPACE namespace. Create the file sa.yaml apiVersion: v1 kind: ServiceAccount metadata: name: pipeline secrets: - name: regcred Create sa role as follows: oc apply -n $NAMESPACE -f sa.yaml Lets create a Task Run for buildah Task using the tkn CLI passing the inputs, outputs and service account tkn task start buildah \\ -i source = source \\ -i image = image \\ -s pipeline \\ -n $NAMESPACE The task will start and logs will start printing automatically Taskrun started: buildah-run-vvrg2 Waiting for logs to be available... Verify the status of the Task Run tkn tr ls -n $NAMESPACE Output should look like this NAME STARTED DURATION STATUS buildah-run-zbsrv 2 minutes ago 1 minute Succeeded To clean up all Pods associated with all Task Runs, delete all the task runs resources oc delete taskrun --all -n $NAMESPACE (Optional) Instead of starting the Task via tkn task start you could also use yaml TaskRun apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: generateName: buildah-task-run- spec: serviceAccountName: pipeline taskRef: name: buildah inputs: resources: - name: source resourceRef: name: source - name: image resourceRef: name: image Then create the TaskRun with generateName oc create -f taskrun.yaml -n $NAMESPACE Follow the logs with: tkn tr logs -f -n $NAMESPACE Pipelines \u00b6 Pipeline Creation \u00b6 Pipelines allows to start multiple Tasks, in parallel or in a certain order Create the file pipeline.yaml , the Pipeline contains two Tasks apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: test-build-push spec: resources: - name: source type: git - name: image type: image tasks: - name: test taskRef: name: java-test resources: inputs: - name: source resource: source - name: build-push taskRef: name: buildah runAfter: [test] resources: inputs: - name: source resource: source - name: image resource: image Pipeline defines a list of Tasks to execute in order, while also indicating if any outputs should be used as inputs of a following Task by using the from field and also indicating the order of executing (using the runAfter and from fields). The same variable substitution you used in Tasks is also available in a Pipeline. Create the Pipeline using the command: oc apply -f pipeline.yaml -n $NAMESPACE Use the Tekton cli to inspect the created resources tkn pipeline ls -n $NAMESPACE The above command should list one Pipeline as shown below: NAME AGE LAST RUN STARTED DURATION STATUS test-build-push 31 seconds ago --- --- --- --- PipelineRun \u00b6 PipelineRun Creation \u00b6 To execute the Tasks in the Pipeline, you must create a PipelineRun. Creation of a PipelineRun will trigger the creation of TaskRuns for each Task in your pipeline. Create the file pipelinerun.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: generateName: test-build-push-run- spec: serviceAccountName: pipeline pipelineRef: name: test-build-push serviceAccountName: pipeline resources: - name: source resourceRef: name: source - name: image resourceRef: name: image serviceAccount - it is always recommended to have a service account associated with PipelineRun, which can then be used to define fine grained roles. Create the PipelineRun using the command: oc create -f pipelinerun.yaml -n $NAMESPACE We will use the Tekton cli to inspect the created resources tkn pipelinerun ls -n $NAMESPACE The above command should list one PipelineRun as shown below: NAME STARTED DURATION STATUS test-build-push-run-c7zgv 8 seconds ago --- Running Wait for few minutes for your pipeline to complete all the tasks. If it is successful, you will see something like below. tkn pipeline ls -n $NAMESPACE NAME AGE LAST RUN STARTED DURATION STATUS test-build-push 33 minutes ago test-build-push-run-c7zgv 2 minutes ago 2 minutes Succeeded Run again the pipeline ls command tkn pipelinerun ls -n $NAMESPACE NAME STARTED DURATION STATUS test-build-push-run-c7zgv 2 minutes ago 2 minutes Succeeded If it is successful, go to your container registry account and verify if you have the cloudnative_sample_app image pushed. (Optional) Run the pipeline again using the tkn CLI tkn pipeline start test-build-push \\ -r source = source \\ -r image = image \\ -s pipeline \\ -n $NAMESPACE (Optional) Re-run the pipeline using last pipelinerun values tkn pipeline start test-build-push --last -n $NAMESPACE Deploy Application \u00b6 Deploy the app as follows: export APP_YAML_URL = 'https://raw.githubusercontent.com/ibm-cloud-architecture/cloudnative_sample_app_deploy/master/yamls/' oc apply -n $NAMESPACE -f $APP_YAML_URL /deployment.yaml oc apply -n $NAMESPACE -f $APP_YAML_URL /service.yaml Replace the default image with the new image you deployed using Tekton Replace <DOCKER_USERNAME> with your username export DOCKER_USERNAME = '<DOCKER_USERNAME>' Replace <SHORT_GIT_HASH> with the tag of the image you push to the registry, you can go the registry Web UI and verify the tag value. export SHORT_GIT_HASH = '<SHORT_GIT_HASH>' Set the environment variable IMAGE_URL to the new image url value sing the two previous environment variables DOCKER_USERNAME and SHORT_GIT_HASH export IMAGE_URL = docker.io/ ${ DOCKER_USERNAME } /cloudnative_sample_app: ${ SHORT_GIT_HASH } echo $IMAGE_URL Replace the image on the deployment oc set image \\ deployment/cloudnativesampleapp-deployment \\ \\* = ${ IMAGE_URL } \\ -n $NAMESPACE --record Verify the image is set oc get deploy \\ cloudnativesampleapp-deployment \\ -o jsonpath = '{.spec.template.spec.containers[0].image}' \\ -n $NAMESPACE Verify if the pods are running: oc get pods -n $NAMESPACE -l app = cloudnativesampleapp-selector NAME READY STATUS RESTARTS AGE cloudnativesampleapp... 1/1 Running 0 82s Retrieve the service NodePort: oc describe svc cloudnativesampleapp-service -n $NAMESPACE | grep NodePort Type: NodePort NodePort: http 30632/TCP In this instance the NodePort assigned is 30632 Get the External Public IP as follows: crc ip 192.168.64.30 Now access the compose the URL of the App using IP and NodePort export APP_EXTERNAL_IP = $( crc ip ) export APP_NODEPORT = $( oc get svc cloudnativesampleapp-service -n $NAMESPACE -o jsonpath = '{.spec.ports[0].nodePort}' ) export APP_URL = \"http:// ${ APP_EXTERNAL_IP } : ${ APP_NODEPORT } /greeting?name=Carlos\" echo $APP_URL http://192.168.64.30:30632//greeting?name=Carlos Now access the app from terminal or browser curl $APP_URL open $APP_URL IKS SetUp \u00b6 Tekton CLI Installation \u00b6 Tekton CLI is command line utility used to interact with the Tekton resources. Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn For MacOS for example you can use brew brew tap tektoncd/tools brew install tektoncd/tools/tektoncd-cli Verify the Tekton cli tkn version The command should show a result like: $ tkn version Client version: 0 .8.0 If you already have the tkn install you can upgrade running brew upgrade tektoncd/tools/tektoncd-cli Tekton Pipelines Installation \u00b6 To deploy the Tekton pipelines: kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml Note : It will take few mins for the Tekton pipeline components to be installed, you an watch the status using the command: kubectl get pods -n tekton-pipelines -w You can use Ctrl+c to terminate the watch A successful deployment of Tekton pipelines will show the following pods: NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-9b8cccff-j6hvr 1/1 Running 0 2m33s tekton-pipelines-webhook-6fc9d4d9b6-kpkp7 1/1 Running 0 2m33s Tekton Dashboard Installation (Optional) \u00b6 To deploy the Tekton dashboard: kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/latest/dashboard_latest_release.yaml Note : It will take few mins for the Tekton dashboard components to be installed, you an watch the status using the command: kubectl get pods -n tekton-pipelines -w You can use Ctrl+c to terminate the watch A successful deployment of Tekton pipelines will show the following pods: NAME READY STATUS RESTARTS AGE tekton-dashboard-59c7fbf49f-79f7q 1/1 Running 0 50s tekton-pipelines-controller-6b7f7cf7d8-r65ps 1/1 Running 0 15m tekton-pipelines-webhook-7bbd8fcc45-sfgxs 1/1 Running 0 15m Access the dashboard as follows: kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097 :9097 You can access the web UI at http://localhost:9097 . Create Target Namespace \u00b6 Set the environment variable NAMESPACE to tekton-demo , if you open a new terminal remember to set this environment again export NAMESPACE = tekton-demo Create a the namespace using the variable NAMESPACE kubectl create namespace $NAMESPACE Pipeline Resources \u00b6 Pipeline Resource Creation \u00b6 Create a PipelineResource of type git \u00b6 Create the file git.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: source spec: type: git params: - name: revision value: master - name: url value: https://github.com/ibm-cloud-architecture/cloudnative_sample_app Verify the file content cat git.yaml Create a PipelineResource of type image \u00b6 Set the environment variable DOCKER_USERNAME to your dockerhub account, replace <REPLACEME> with your docker username, keep the quotes export DOCKER_USERNAME = '<REPLACEME>' Create the file image.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: image spec: type: image params: - name: url value: index.docker.io/ $DOCKER_USERNAME /cloudnative_sample_app Verify the file content, and make sure the url value is valid with your dockerhub username replaced cat image.yaml Pipeline Resources deployment \u00b6 Each pipeline resource has: name : the name using which it will be referred in other places type : the type of the pipeline resource, in this example we have two types git - this type of resource refers to a GitHub repository image - this type of resource is linux container image params : each type can have one or more parameters that will be used to configure the underlying type. In the above example for the git-source pipeline resource, the parameters url and revision are used to identify the GitHub repository url and revision of the sources respectively. More details on other types of pipeline resource types is available here . Create the pipeline resources using the command: kubectl apply -f git.yaml -n $NAMESPACE kubectl apply -f image.yaml -n $NAMESPACE Verify the deployed resource \u00b6 Use the Tekton cli to list the created resources tkn res ls -n $NAMESPACE The above command should list two resources as shown below: NAME TYPE DETAILS source git url: https://github.com/ibm-cloud-architecture/cloudnative_sample_app image image url: index.docker.io/yourdockerhubusername/cloudnative_sample_app Use the command help via tkn res --help Use the Tekton cli to describe the git resource tkn res describe source -n $NAMESPACE The output should look like this: Name: source Namespace: tekton-demo PipelineResource Type: git Params NAME VALUE revision master url https://github.com/ibm-cloud-architecture/cloudnative_sample_app Secret Params No secret params Use the Tekton cli to describe the git resource tkn res describe image -n $NAMESPACE The output should look like this: Name: image Namespace: tekton-demo PipelineResource Type: image Params NAME VALUE url index.docker.io/myusername/cloudnative_sample_app Secret Params No secret params Tasks \u00b6 Task Creation \u00b6 Create the below yaml files. The following snippet shows what a Tekton Task YAML looks like: Create the file test_task.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: java-test spec: inputs: resources: - name: source type: git params: - name: maven-image type: string default: maven:3.3-jdk-8 steps: - name: test image: $( inputs.params.maven-image ) workingdir: $( inputs.resources.source.path ) command: [ \"/bin/bash\" ] args: - -c - | set -e mvn test echo \"tests passed with rc= $? \" volumeMounts: - name: m2-repository mountPath: /.m2 volumes: - name: m2-repository emptyDir: {} Each Task has the following: name - the unique name using which the task can be referred inputs - the inputs to the task resources - the pipeline resources that will be used in the task e.g. git-source name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type params - the parameters that will be used in the task steps. Each parameter has name - the name of the parameter description - the description of the parameter default - the default value of parameter Note : The TaskRun or PipelineRun could override the parameter values, if no parameter value is passed then the default value will be used. outputs the pipeline resource that will end artifact of the task. In the above example the build will produce a container image artifact. resources - the pipeline resources that will be used in the task e.g. builtImage name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type steps - One or more sub-tasks that will be executed in the defined order. The step has all the attributes like a Pod spec stepTemplate - when there is a need to have similar container configuration across all steps of a the task, we can have them defined in the stepTemplate, the task steps will inherit them implicitly in all steps. In the example above we define the resources and securityContext for all the steps volumes - the task can also mount external volumes using the volumes attribute. The parameters that were part of the spec inputs params can be used in the steps using the notation $(<variable-name>) . Task Deploy \u00b6 The application test task could be created using the command: kubectl apply -f test_task.yaml -n $NAMESPACE We will use the Tekton cli to inspect the created resources tkn task ls -n $NAMESPACE The above command should list one Task as shown below: NAME AGE java-test 22 seconds ago TaskRun \u00b6 The TaskRun is used to run a specific task independently. In the following section we will run the build-app task created in the previous step TaskRun Creation \u00b6 The following snippet shows what a Tekton TaskRun YAML looks like: Create the file test_taskrun.yaml apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: generateName: test-task-run- spec: taskRef: name: java-test inputs: resources: - name: source resourceRef: name: source generateName - since the TaskRun can be run many times, in order to have unique name across the TaskRun ( helpful when checking the TaskRun history) we use this generateName instead of name. When Kubernetes sees generateName it will generate unique set of characters and suffix the same to build-app-, similar to how pod names are generated taskRef - this is used to refer to the Task by its name that will be run as part of this TaskRun. In this example we use build-app Task. As described in the earlier section that the Task inputs and outputs could be overridden via TaskRun. In this example we make the Task Run spec > inputs > resources > source to refer to pipeline resource source via the resourceRef . The application test task(java-maven-test) could be run using the command: kubectl create -n $NAMESPACE -f test_taskrun.yaml Note - As tasks will use generated name, never use kubectl apply -f test_taskrun.yaml We will use the Tekton cli to inspect the created resources: tkn tr ls -n $NAMESPACE The above command should list one TaskRun as shown below: NAME STARTED DURATION STATUS test-task-run-q6s8c 1 minute ago --- Running ( Pending ) Note - It will take few seconds for the TaskRun to show status as Running as it needs to download the container images. To check the logs of the Task Run using the tkn : tkn tr logs -f -a -n $NAMESPACE Note - Each task step will be run within a container of its own. The -f or -a allows to tail the logs from all the containers of the task. For more options run tkn tr logs --help - If you see the TaskRun status as Failed or Error use the following command to check the reason for error: tkn tr describe <taskrun-name> -n $NAMESPACE - If it is successful, you will see something like below. tkn tr ls -n $NAMESPACE The above command should list one TaskRun as shown below: NAME STARTED DURATION STATUS test-task-run-q6s8c 47 seconds ago 34 seconds Succeeded ### Creating additional tasks and deploying them - Create a Task to build a container image and push to the registry - This task will be later used by the pipeline. - Download the task file [task-buildah.yaml](/yamls/tekton-lab/task-buildah.yaml) to build the image, push the image to the registry: - Create the `buildah` Task using the file and the command: ```bash kubectl apply -f task-buildah.yaml -n $NAMESPACE ``` - Use the Tekton cli to inspect the created resources ```bash tkn task ls -n $NAMESPACE ``` - The above command should list one Task as shown below: ```bash NAME AGE buildah 4 seconds ago java-test 46 minutes ago ``` - To access the docker registry, create the required secret as follows. - Create the environment variables to be use, replace with real values and include the single quotes: ```bash export DOCKER_USERNAME='<DOCKER_USERNAME>' ``` ```bash export DOCKER_PASSWORD='<DOCKER_PASSWORD>' ``` ```bash export DOCKER_EMAIL='<DOCKER_EMAIL>' ``` - Run the following command to create a secret `regcred` in the namespace `NAMESPACE` ```bash kubectl create secret docker-registry regcred \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=${DOCKER_USERNAME} \\ --docker-password=${DOCKER_PASSWORD} \\ --docker-email=${DOCKER_EMAIL} \\ -n ${NAMESPACE} ``` Before creating, replace the values as mentioned above. Note: If your docker password contains special characters in it, please enclose the password in double quotes or place an escape character before each special character. - (Optional) Only if you have problems with the credentials you can recreate it, but you have to deleted first ```bash kubectl delete secret regcred -n $NAMESPACE ``` - Before we run the Task using TaskRun let us create the Kubernetes service account and attach the needed permissions to the service account, the following Kubernetes resource defines a service account called `pipeline` in namespace `$NAMESPACE` who will have administrative role within the `$NAMESPACE` namespace. - Create the file **sa.yaml** ```bash apiVersion: v1 kind: ServiceAccount metadata: name: pipeline secrets: - name: regcred ``` - Create sa role as follows: ```bash kubectl apply -n $NAMESPACE -f sa.yaml ``` - Lets create a Task Run for `buildah` Task using the `tkn` CLI passing the inputs, outputs and service account ```bash tkn task start buildah \\ -i source=source \\ -i image=image \\ -s pipeline \\ -n $NAMESPACE ``` The task will start and logs will start printing automatically ``` Taskrun started: buildah-run-vvrg2 Waiting for logs to be available... ``` - Verify the status of the Task Run ```bash tkn tr ls -n $NAMESPACE ``` Output should look like this ``` NAME STARTED DURATION STATUS buildah-run-zbsrv 2 minutes ago 1 minute Succeeded ``` - To clean up all Pods associated with all Task Runs, delete all the task runs resources ```bash kubectl delete taskrun --all -n $NAMESPACE ``` - (Optional) Instead of starting the Task via `tkn task start` you could also use yaml TaskRun ```bash apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: generateName: buildah-task-run- spec: serviceAccountName: pipeline taskRef: name: buildah inputs: resources: - name: source resourceRef: name: source - name: image resourceRef: name: image ``` Then create the TaskRun with `generateName` ```bash kubectl create -f taskrun.yaml -n $NAMESPACE ``` Follow the logs with: ``` tkn tr logs -f -n $NAMESPACE ``` ## Pipelines ### Pipeline Creation - Pipelines allows to start multiple Tasks, in parallel or in a [certain order](https://github.com/tektoncd/pipeline/blob/master/docs/pipelines.md#runafter) - Create the file **pipeline.yaml**, the Pipeline contains two Tasks ```bash apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: test-build-push spec: resources: - name: source type: git - name: image type: image tasks: - name: test taskRef: name: java-test resources: inputs: - name: source resource: source - name: build-push taskRef: name: buildah runAfter: [test] resources: inputs: - name: source resource: source - name: image resource: image ``` - Pipeline defines a list of Tasks to execute in order, while also indicating if any outputs should be used as inputs of a following Task by using the from field and also indicating the order of executing (using the runAfter and from fields). The same variable substitution you used in Tasks is also available in a Pipeline. - Create the Pipeline using the command: ```bash kubectl apply -f pipeline.yaml -n $NAMESPACE ``` - Use the Tekton cli to inspect the created resources ```bash tkn pipeline ls -n $NAMESPACE ``` The above command should list one Pipeline as shown below: ```bash NAME AGE LAST RUN STARTED DURATION STATUS test-build-push 31 seconds ago --- --- --- --- ``` ### PipelineRun #### PipelineRun Creation - To execute the Tasks in the Pipeline, you must create a PipelineRun. Creation of a PipelineRun will trigger the creation of TaskRuns for each Task in your pipeline. - Create the file **pipelinerun.yaml** ```bash apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: generateName: test-build-push-run- spec: serviceAccountName: pipeline pipelineRef: name: test-build-push serviceAccountName: pipeline resources: - name: source resourceRef: name: source - name: image resourceRef: name: image ``` **serviceAccount** - it is always recommended to have a service account associated with PipelineRun, which can then be used to define fine grained roles. - Create the PipelineRun using the command: ```bash kubectl create -f pipelinerun.yaml -n $NAMESPACE ``` - We will use the Tekton cli to inspect the created resources ```bash tkn pipelinerun ls -n $NAMESPACE ``` - The above command should list one PipelineRun as shown below: ```bash NAME STARTED DURATION STATUS test-build-push-run-c7zgv 8 seconds ago --- Running ``` - Wait for few minutes for your pipeline to complete all the tasks. If it is successful, you will see something like below. ```bash tkn pipeline ls -n $NAMESPACE ``` ``` NAME AGE LAST RUN STARTED DURATION STATUS test-build-push 33 minutes ago test-build-push-run-c7zgv 2 minutes ago 2 minutes Succeeded ``` - Run again the pipeline ls command ```bash tkn pipelinerun ls -n $NAMESPACE ``` ``` NAME STARTED DURATION STATUS test-build-push-run-c7zgv 2 minutes ago 2 minutes Succeeded ``` If it is successful, go to your container registry account and verify if you have the `cloudnative_sample_app` image pushed. - (Optional) Run the pipeline again using the `tkn` CLI ```bash tkn pipeline start test-build-push \\ -r source=source \\ -r image=image \\ -s pipeline \\ -n $NAMESPACE ``` - (Optional) Re-run the pipeline using last pipelinerun values ```bash tkn pipeline start test-build-push --last -n $NAMESPACE ``` ## Deploy Application - Deploy the app as follows: ```bash export APP_YAML_URL='https://raw.githubusercontent.com/ibm-cloud-architecture/cloudnative_sample_app_deploy/master/yamls/' kubectl apply -n $NAMESPACE -f $APP_YAML_URL/deployment.yaml kubectl apply -n $NAMESPACE -f $APP_YAML_URL/service.yaml ``` - Replace the default image with the new image you deployed using Tekton - Replace `<DOCKER_USERNAME>` with your username ```bash export DOCKER_USERNAME='<DOCKER_USERNAME>' ``` - Replace `<SHORT_GIT_HASH>` with the tag of the image you push to the registry, you can go the registry Web UI and verify the tag value. ```bash export SHORT_GIT_HASH='<SHORT_GIT_HASH>' ``` - Set the environment variable `IMAGE_URL` to the new image url value sing the two previous environment variables `DOCKER_USERNAME` and `SHORT_GIT_HASH` ```bash export IMAGE_URL=docker.io/${DOCKER_USERNAME}/cloudnative_sample_app:${SHORT_GIT_HASH} echo $IMAGE_URL ``` - Replace the image on the deployment ```bash kubectl set image \\ deployment/cloudnativesampleapp-deployment \\ \\*=${IMAGE_URL} \\ -n $NAMESPACE --record ``` - Verify the image is set ```bash kubectl get deploy \\ cloudnativesampleapp-deployment \\ -o jsonpath='{.spec.template.spec.containers[0].image}' \\ -n $NAMESPACE ``` - Verify if the pods are running: ```bash kubectl get pods -n $NAMESPACE -l app=cloudnativesampleapp-selector ``` ``` NAME READY STATUS RESTARTS AGE cloudnativesampleapp... 1/1 Running 0 82s ``` - Retrieve the service NodePort: ```bash kubectl describe svc cloudnativesampleapp-service -n $NAMESPACE | grep NodePort ``` ``` Type: NodePort NodePort: http 30632/TCP ``` In this instance the NodePort assigned is `30632` - Get the External Public IP as follows: ```bash minikube ip ``` ``` 192.168.64.30 ``` - Now access the compose the URL of the App using IP and NodePort ```bash export APP_EXTERNAL_IP=$(minikube ip) export APP_NODEPORT=$(kubectl get svc cloudnativesampleapp-service -n $NAMESPACE -o jsonpath='{.spec.ports[0].nodePort}') export APP_URL=\"http://${APP_EXTERNAL_IP}:${APP_NODEPORT}/greeting?name=Carlos\" echo $APP_URL ``` ``` http://192.168.64.30:30632//greeting?name=Carlos ``` - Now access the app from terminal or browser ```bash curl $APP_URL ``` ```bash open $APP_URL ```","title":"Tekton Lab"},{"location":"developer-intermediate/continuous-integration-handson/#pre-requisites","text":"Make sure your environment is properly setup. Follow the instructions here OpenShift","title":"Pre-requisites"},{"location":"developer-intermediate/continuous-integration-handson/#setup","text":"","title":"SetUp"},{"location":"developer-intermediate/continuous-integration-handson/#tekton-cli-installation","text":"Tekton CLI is command line utility used to interact with the Tekton resources. Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn For MacOS for example you can use brew brew tap tektoncd/tools brew install tektoncd/tools/tektoncd-cli Verify the Tekton cli tkn version The command should show a result like: $ tkn version Client version: 0.8.0 If you already have the tkn install you can upgrade running brew upgrade tektoncd/tools/tektoncd-cli","title":"Tekton CLI Installation"},{"location":"developer-intermediate/continuous-integration-handson/#tekton-pipelines-installation","text":"To deploy the Tekton pipelines: oc apply --filename https://raw.githubusercontent.com/ibm-cloud-architecture/learning-cloudnative-101/master/static/yamls/tekton-lab/tekton-operator.yaml Note : It will take few mins for the Tekton pipeline components to be installed, you an watch the status using the command: oc get pods -n openshift-operators You can use Ctrl+c to terminate the watch A successful deployment of Tekton pipelines will show the following pods: NAME READY STATUS RESTARTS AGE openshift-pipelines-operator-9cdbbb854-x9tvs 1/1 Running 0 25s","title":"Tekton Pipelines Installation"},{"location":"developer-intermediate/continuous-integration-handson/#create-target-namespace","text":"Set the environment variable NAMESPACE to tekton-demo , if you open a new terminal remember to set this environment again export NAMESPACE=tekton-demo Create a the namespace using the variable NAMESPACE oc create namespace $NAMESPACE","title":"Create Target Namespace"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-resources","text":"","title":"Pipeline Resources"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-resource-creation","text":"","title":"Pipeline Resource Creation"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-resources-deployment","text":"Each pipeline resource has: name : the name using which it will be referred in other places type : the type of the pipeline resource, in this example we have two types git - this type of resource refers to a GitHub repository image - this type of resource is linux container image params : each type can have one or more parameters that will be used to configure the underlying type. In the above example for the git-source pipeline resource, the parameters url and revision are used to identify the GitHub repository url and revision of the sources respectively. More details on other types of pipeline resource types is available here . Create the pipeline resources using the command: oc apply -f git.yaml -n $NAMESPACE oc apply -f image.yaml -n $NAMESPACE","title":"Pipeline Resources deployment"},{"location":"developer-intermediate/continuous-integration-handson/#verify-the-deployed-resource","text":"Use the Tekton cli to list the created resources tkn res ls -n $NAMESPACE The above command should list two resources as shown below: NAME TYPE DETAILS source git url: https://github.com/ibm-cloud-architecture/cloudnative_sample_app image image url: index.docker.io/yourdockerhubusername/cloudnative_sample_app Use the command help via tkn res --help Use the Tekton cli to describe the git resource tkn res describe source -n $NAMESPACE The output should look like this: Name: source Namespace: tekton-demo PipelineResource Type: git Params NAME VALUE revision master url https://github.com/ibm-cloud-architecture/cloudnative_sample_app Secret Params No secret params Use the Tekton cli to describe the git resource tkn res describe image -n $NAMESPACE The output should look like this: Name: image Namespace: tekton-demo PipelineResource Type: image Params NAME VALUE url index.docker.io/myusername/cloudnative_sample_app Secret Params No secret params","title":"Verify the deployed resource"},{"location":"developer-intermediate/continuous-integration-handson/#tasks","text":"","title":"Tasks"},{"location":"developer-intermediate/continuous-integration-handson/#task-creation","text":"Create the below yaml files. The following snippet shows what a Tekton Task YAML looks like: Create the file test_task.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: java-test spec: inputs: resources: - name: source type: git params: - name: maven-image type: string default: maven:3.3-jdk-8 steps: - name: test image: $(inputs.params.maven-image) workingdir: $(inputs.resources.source.path) command: [\"/bin/bash\"] args: - -c - | set -e mvn test echo \"tests passed with rc=$?\" volumeMounts: - name: m2-repository mountPath: /.m2 volumes: - name: m2-repository emptyDir: {} Each Task has the following: name - the unique name using which the task can be referred inputs - the inputs to the task resources - the pipeline resources that will be used in the task e.g. git-source name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type params - the parameters that will be used in the task steps. Each parameter has name - the name of the parameter description - the description of the parameter default - the default value of parameter Note : The TaskRun or PipelineRun could override the parameter values, if no parameter value is passed then the default value will be used. outputs the pipeline resource that will end artifact of the task. In the above example the build will produce a container image artifact. resources - the pipeline resources that will be used in the task e.g. builtImage name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type steps - One or more sub-tasks that will be executed in the defined order. The step has all the attributes like a Pod spec stepTemplate - when there is a need to have similar container configuration across all steps of a the task, we can have them defined in the stepTemplate, the task steps will inherit them implicitly in all steps. In the example above we define the resources and securityContext for all the steps volumes - the task can also mount external volumes using the volumes attribute. The parameters that were part of the spec inputs params can be used in the steps using the notation $(<variable-name>) .","title":"Task Creation"},{"location":"developer-intermediate/continuous-integration-handson/#task-deploy","text":"The application test task could be created using the command: oc apply -f test_task.yaml -n $NAMESPACE We will use the Tekton cli to inspect the created resources tkn task ls -n $NAMESPACE The above command should list one Task as shown below: NAME AGE java-test 22 seconds ago","title":"Task Deploy"},{"location":"developer-intermediate/continuous-integration-handson/#taskrun","text":"The TaskRun is used to run a specific task independently. In the following section we will run the build-app task created in the previous step","title":"TaskRun"},{"location":"developer-intermediate/continuous-integration-handson/#creating-additional-tasks-and-deploying-them","text":"Create a Task to build a container image and push to the registry This task will be later used by the pipeline. Download the task file task-buildah.yaml to build the image, push the image to the registry: Create the buildah Task using the file and the command: oc apply -f task-buildah.yaml -n $NAMESPACE Use the Tekton cli to inspect the created resources tkn task ls -n $NAMESPACE The above command should list one Task as shown below: NAME AGE buildah 4 seconds ago java-test 46 minutes ago To access the docker registry, create the required secret as follows. Create the environment variables to be use, replace with real values and include the single quotes: export DOCKER_USERNAME='<DOCKER_USERNAME>' export DOCKER_PASSWORD='<DOCKER_PASSWORD>' export DOCKER_EMAIL='<DOCKER_EMAIL>' Run the following command to create a secret regcred in the namespace NAMESPACE oc create secret docker-registry regcred \\ --docker-server=https://index.docker.io/v1/ \\ --docker-username=${DOCKER_USERNAME} \\ --docker-password=${DOCKER_PASSWORD} \\ --docker-email=${DOCKER_EMAIL} \\ -n ${NAMESPACE} Before creating, replace the values as mentioned above. Note: If your docker password contains special characters in it, please enclose the password in double quotes or place an escape character before each special character. (Optional) Only if you have problems with the credentials you can recreate it, but you have to deleted first oc delete secret regcred -n $NAMESPACE Before we run the Task using TaskRun let us create the Kubernetes service account and attach the needed permissions to the service account, the following Kubernetes resource defines a service account called pipeline in namespace $NAMESPACE who will have administrative role within the $NAMESPACE namespace. Create the file sa.yaml apiVersion: v1 kind: ServiceAccount metadata: name: pipeline secrets: - name: regcred Create sa role as follows: oc apply -n $NAMESPACE -f sa.yaml Lets create a Task Run for buildah Task using the tkn CLI passing the inputs, outputs and service account tkn task start buildah \\ -i source = source \\ -i image = image \\ -s pipeline \\ -n $NAMESPACE The task will start and logs will start printing automatically Taskrun started: buildah-run-vvrg2 Waiting for logs to be available... Verify the status of the Task Run tkn tr ls -n $NAMESPACE Output should look like this NAME STARTED DURATION STATUS buildah-run-zbsrv 2 minutes ago 1 minute Succeeded To clean up all Pods associated with all Task Runs, delete all the task runs resources oc delete taskrun --all -n $NAMESPACE (Optional) Instead of starting the Task via tkn task start you could also use yaml TaskRun apiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: generateName: buildah-task-run- spec: serviceAccountName: pipeline taskRef: name: buildah inputs: resources: - name: source resourceRef: name: source - name: image resourceRef: name: image Then create the TaskRun with generateName oc create -f taskrun.yaml -n $NAMESPACE Follow the logs with: tkn tr logs -f -n $NAMESPACE","title":"Creating additional tasks and deploying them"},{"location":"developer-intermediate/continuous-integration-handson/#pipelines","text":"","title":"Pipelines"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-creation","text":"Pipelines allows to start multiple Tasks, in parallel or in a certain order Create the file pipeline.yaml , the Pipeline contains two Tasks apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: test-build-push spec: resources: - name: source type: git - name: image type: image tasks: - name: test taskRef: name: java-test resources: inputs: - name: source resource: source - name: build-push taskRef: name: buildah runAfter: [test] resources: inputs: - name: source resource: source - name: image resource: image Pipeline defines a list of Tasks to execute in order, while also indicating if any outputs should be used as inputs of a following Task by using the from field and also indicating the order of executing (using the runAfter and from fields). The same variable substitution you used in Tasks is also available in a Pipeline. Create the Pipeline using the command: oc apply -f pipeline.yaml -n $NAMESPACE Use the Tekton cli to inspect the created resources tkn pipeline ls -n $NAMESPACE The above command should list one Pipeline as shown below: NAME AGE LAST RUN STARTED DURATION STATUS test-build-push 31 seconds ago --- --- --- ---","title":"Pipeline Creation"},{"location":"developer-intermediate/continuous-integration-handson/#pipelinerun","text":"","title":"PipelineRun"},{"location":"developer-intermediate/continuous-integration-handson/#deploy-application","text":"Deploy the app as follows: export APP_YAML_URL = 'https://raw.githubusercontent.com/ibm-cloud-architecture/cloudnative_sample_app_deploy/master/yamls/' oc apply -n $NAMESPACE -f $APP_YAML_URL /deployment.yaml oc apply -n $NAMESPACE -f $APP_YAML_URL /service.yaml Replace the default image with the new image you deployed using Tekton Replace <DOCKER_USERNAME> with your username export DOCKER_USERNAME = '<DOCKER_USERNAME>' Replace <SHORT_GIT_HASH> with the tag of the image you push to the registry, you can go the registry Web UI and verify the tag value. export SHORT_GIT_HASH = '<SHORT_GIT_HASH>' Set the environment variable IMAGE_URL to the new image url value sing the two previous environment variables DOCKER_USERNAME and SHORT_GIT_HASH export IMAGE_URL = docker.io/ ${ DOCKER_USERNAME } /cloudnative_sample_app: ${ SHORT_GIT_HASH } echo $IMAGE_URL Replace the image on the deployment oc set image \\ deployment/cloudnativesampleapp-deployment \\ \\* = ${ IMAGE_URL } \\ -n $NAMESPACE --record Verify the image is set oc get deploy \\ cloudnativesampleapp-deployment \\ -o jsonpath = '{.spec.template.spec.containers[0].image}' \\ -n $NAMESPACE Verify if the pods are running: oc get pods -n $NAMESPACE -l app = cloudnativesampleapp-selector NAME READY STATUS RESTARTS AGE cloudnativesampleapp... 1/1 Running 0 82s Retrieve the service NodePort: oc describe svc cloudnativesampleapp-service -n $NAMESPACE | grep NodePort Type: NodePort NodePort: http 30632/TCP In this instance the NodePort assigned is 30632 Get the External Public IP as follows: crc ip 192.168.64.30 Now access the compose the URL of the App using IP and NodePort export APP_EXTERNAL_IP = $( crc ip ) export APP_NODEPORT = $( oc get svc cloudnativesampleapp-service -n $NAMESPACE -o jsonpath = '{.spec.ports[0].nodePort}' ) export APP_URL = \"http:// ${ APP_EXTERNAL_IP } : ${ APP_NODEPORT } /greeting?name=Carlos\" echo $APP_URL http://192.168.64.30:30632//greeting?name=Carlos Now access the app from terminal or browser curl $APP_URL open $APP_URL IKS","title":"Deploy Application"},{"location":"developer-intermediate/continuous-integration-handson/#setup_1","text":"","title":"SetUp"},{"location":"developer-intermediate/continuous-integration-handson/#tekton-cli-installation_1","text":"Tekton CLI is command line utility used to interact with the Tekton resources. Follow the instructions on the tekton CLI github repository https://github.com/tektoncd/cli#installing-tkn For MacOS for example you can use brew brew tap tektoncd/tools brew install tektoncd/tools/tektoncd-cli Verify the Tekton cli tkn version The command should show a result like: $ tkn version Client version: 0 .8.0 If you already have the tkn install you can upgrade running brew upgrade tektoncd/tools/tektoncd-cli","title":"Tekton CLI Installation"},{"location":"developer-intermediate/continuous-integration-handson/#tekton-pipelines-installation_1","text":"To deploy the Tekton pipelines: kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml Note : It will take few mins for the Tekton pipeline components to be installed, you an watch the status using the command: kubectl get pods -n tekton-pipelines -w You can use Ctrl+c to terminate the watch A successful deployment of Tekton pipelines will show the following pods: NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-9b8cccff-j6hvr 1/1 Running 0 2m33s tekton-pipelines-webhook-6fc9d4d9b6-kpkp7 1/1 Running 0 2m33s","title":"Tekton Pipelines Installation"},{"location":"developer-intermediate/continuous-integration-handson/#tekton-dashboard-installation-optional","text":"To deploy the Tekton dashboard: kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/latest/dashboard_latest_release.yaml Note : It will take few mins for the Tekton dashboard components to be installed, you an watch the status using the command: kubectl get pods -n tekton-pipelines -w You can use Ctrl+c to terminate the watch A successful deployment of Tekton pipelines will show the following pods: NAME READY STATUS RESTARTS AGE tekton-dashboard-59c7fbf49f-79f7q 1/1 Running 0 50s tekton-pipelines-controller-6b7f7cf7d8-r65ps 1/1 Running 0 15m tekton-pipelines-webhook-7bbd8fcc45-sfgxs 1/1 Running 0 15m Access the dashboard as follows: kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097 :9097 You can access the web UI at http://localhost:9097 .","title":"Tekton Dashboard Installation (Optional)"},{"location":"developer-intermediate/continuous-integration-handson/#create-target-namespace_1","text":"Set the environment variable NAMESPACE to tekton-demo , if you open a new terminal remember to set this environment again export NAMESPACE = tekton-demo Create a the namespace using the variable NAMESPACE kubectl create namespace $NAMESPACE","title":"Create Target Namespace"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-resources_1","text":"","title":"Pipeline Resources"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-resource-creation_1","text":"","title":"Pipeline Resource Creation"},{"location":"developer-intermediate/continuous-integration-handson/#pipeline-resources-deployment_1","text":"Each pipeline resource has: name : the name using which it will be referred in other places type : the type of the pipeline resource, in this example we have two types git - this type of resource refers to a GitHub repository image - this type of resource is linux container image params : each type can have one or more parameters that will be used to configure the underlying type. In the above example for the git-source pipeline resource, the parameters url and revision are used to identify the GitHub repository url and revision of the sources respectively. More details on other types of pipeline resource types is available here . Create the pipeline resources using the command: kubectl apply -f git.yaml -n $NAMESPACE kubectl apply -f image.yaml -n $NAMESPACE","title":"Pipeline Resources deployment"},{"location":"developer-intermediate/continuous-integration-handson/#verify-the-deployed-resource_1","text":"Use the Tekton cli to list the created resources tkn res ls -n $NAMESPACE The above command should list two resources as shown below: NAME TYPE DETAILS source git url: https://github.com/ibm-cloud-architecture/cloudnative_sample_app image image url: index.docker.io/yourdockerhubusername/cloudnative_sample_app Use the command help via tkn res --help Use the Tekton cli to describe the git resource tkn res describe source -n $NAMESPACE The output should look like this: Name: source Namespace: tekton-demo PipelineResource Type: git Params NAME VALUE revision master url https://github.com/ibm-cloud-architecture/cloudnative_sample_app Secret Params No secret params Use the Tekton cli to describe the git resource tkn res describe image -n $NAMESPACE The output should look like this: Name: image Namespace: tekton-demo PipelineResource Type: image Params NAME VALUE url index.docker.io/myusername/cloudnative_sample_app Secret Params No secret params","title":"Verify the deployed resource"},{"location":"developer-intermediate/continuous-integration-handson/#tasks_1","text":"","title":"Tasks"},{"location":"developer-intermediate/continuous-integration-handson/#task-creation_1","text":"Create the below yaml files. The following snippet shows what a Tekton Task YAML looks like: Create the file test_task.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: java-test spec: inputs: resources: - name: source type: git params: - name: maven-image type: string default: maven:3.3-jdk-8 steps: - name: test image: $( inputs.params.maven-image ) workingdir: $( inputs.resources.source.path ) command: [ \"/bin/bash\" ] args: - -c - | set -e mvn test echo \"tests passed with rc= $? \" volumeMounts: - name: m2-repository mountPath: /.m2 volumes: - name: m2-repository emptyDir: {} Each Task has the following: name - the unique name using which the task can be referred inputs - the inputs to the task resources - the pipeline resources that will be used in the task e.g. git-source name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type params - the parameters that will be used in the task steps. Each parameter has name - the name of the parameter description - the description of the parameter default - the default value of parameter Note : The TaskRun or PipelineRun could override the parameter values, if no parameter value is passed then the default value will be used. outputs the pipeline resource that will end artifact of the task. In the above example the build will produce a container image artifact. resources - the pipeline resources that will be used in the task e.g. builtImage name - the name of the input resource using which it can be referenced and bound via TaskRun type - the type of the input resource, typically the pipeline resource type steps - One or more sub-tasks that will be executed in the defined order. The step has all the attributes like a Pod spec stepTemplate - when there is a need to have similar container configuration across all steps of a the task, we can have them defined in the stepTemplate, the task steps will inherit them implicitly in all steps. In the example above we define the resources and securityContext for all the steps volumes - the task can also mount external volumes using the volumes attribute. The parameters that were part of the spec inputs params can be used in the steps using the notation $(<variable-name>) .","title":"Task Creation"},{"location":"developer-intermediate/continuous-integration-handson/#task-deploy_1","text":"The application test task could be created using the command: kubectl apply -f test_task.yaml -n $NAMESPACE We will use the Tekton cli to inspect the created resources tkn task ls -n $NAMESPACE The above command should list one Task as shown below: NAME AGE java-test 22 seconds ago","title":"Task Deploy"},{"location":"developer-intermediate/continuous-integration-handson/#taskrun_1","text":"The TaskRun is used to run a specific task independently. In the following section we will run the build-app task created in the previous step","title":"TaskRun"},{"location":"developer-intermediate/deploy-app/","text":"Overview \u00b6 The OpenShift development environment supports end-to-end development and deployment of an application. The instructions below will show you how to do it. You can create a new app using one of the Starter Kits . These have been created to include all the key components, configuration, and frameworks to get you started on creating the code you need for your solutions. This video demonstrates how to work through the steps to create an application and use a deployment pipeline to install it into your development cluster. Create an application \u00b6 The following steps will help you deploy your first application in your own namespace inside your OpenShift development environment. 1. Open the Web Terminal \u00b6 To be able to run CLI commands to drive common operations on the cluster you will first need to open your web terminal. - Click on your web terminal >_ icon in the header of the OpenShift Console. You should see a terminal open at the bottom of the console screen. With a banner saying TechZone Automation - Check you can run oc commands, run the following command oc sync --version - You should see the version number printed Info You will now be able to quickly create Tekton pipelines within OpenShift. Watch this short video for more information on how Tekton is replacing Jenkins in the enterprise CI space. 2. Create the development namespace \u00b6 Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync dev- { your intials } 3. Grant required access to the service account of the namespace \u00b6 Openshift Image registry is being used for storing docker images.Hence,permission needs to be given to the service account of the namespace to be able to pull images from registry. oc policy add-role-to-group system:image-puller system:serviceaccounts: ${ DEV_NAMESPACE } 4. Open the Developer Dashboard \u00b6 The Developer Dashboard makes it easy for you to navigate to the tools, including a section that allows you to select a set of preconfigured Starter Kits that make seeding your development project very easy. If you are logged into the OpenShift console, you can select the tools menu and select Developer Dashboard If you are on a laptop/desktop, open a browser and make sure you are logged into Github Open the dashboard by running the following command: oc dashboard 4. Create your app in Git \u00b6 From the Developer Dashboard, click on tab Warning If you are developing on a shared education cluster, you need to make it easy to identify your app. Please suffix the app name with your initials {app name}-{your initials} (e.g. stockbffnode-mjp ) and use the Git Organization for the shared environment. Your browser needs to be logged in to your GitHub account for a template to work. If the link from the tile displays the GitHub 404 page, log in and reload the page. Pick one of the templates that is a good architectural fit for your application and the language and framework that you prefer to work with. For your first application, select the Typescript Microservice . This also works well in the Cloud Shell. Click on a Tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a valid GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a name for your repo. GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template The new repository will be created in your selected organization. 5. Register the application in a OpenShift Pipeline \u00b6 Info We will be using the pipeline command and specifically the tekton technology to setup the CI pipeline. The pipeline command gives an option for both Jenkins and Tekton make sure you select Tekton . For more information about working with the different build engines, please see Continuous Integration with Jenkins Guide and Continuous Integration with Tekton Guide Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Start the process to create a pipeline from your Web Terminal oc pipeline ${ GIT_URL } --tekton For example: oc pipeline https://github.com/gct-showcase/inventory-svc-mjp --tekton For the deployment of your first app with OpenShift we have selected Tekton as the CI engine. The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, the CLI will attempt to determine the runtime used by the repository that was provided and filter the available pipelines. If only one matches then it will be automatically selected. If it cannot find a match or there is more than one match then you will be prompted to select the pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) The pipeline also prompts for linting the dockerfile ? lint-dockerfile: Enable the pipeline to lint the Dockerfile for best practices? ( Y/n ) To skip the scan, you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. Similarly, for skipping the linting of dockerfile,you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Dockerfile lint. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note: if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console. 7. View your application pipeline \u00b6 The steps to view your registered pipeline will vary based on type of pipeline ( Jenkins or Tekton ) and container platform version. From the OpenShift Web Console select the Pipelines menu in the Developer view You will see your microservice now running an OpenShift pipeline using Tekton Tasks 8. Access the running app \u00b6 Once the pipeline has completed successfully, the app will be deployed into the namespace used when registering the pipeline. To validate the app is running follow these steps: Note: Be sure the namespace context is set correctly by running the following command oc project Retrieve the app ingress endpoint using commands below in your terminal. oc endpoints From the endpoints listed, select the URL for the repo that was just deployed and press Enter to open that URL in a web browser. Validate the application is working as expected. 9. Locate the app in the web console \u00b6 The build pipeline is configured to build the source code from the Git repository into a container image. This image is stored in the Image Registry . After that, the image is deployed into the same namespace/project within the development cluster where the pipeline ran and validated for its health. The steps below will walk through locating the installed application within the Web Console. Open the OpenShift web console Change to the Developer view Click on Topology menu Click on your application deployment in the topology view Click on the Overview tab Increase the number of running pods to 2 pods Click on the Resources tab to view the list of pods Click on View Logs link to view the logs from each pod You can see the running state of your application Success Success: You now have your application running inside your development cluster and being delivered using a Tekton based CI pipeline. This is the beginning of the developer journey with IBM Cloud. Having reached this point, we recommend you repeat the process a few more times using different Code Patterns templates and explore the Developer view in OpenShift to get familiar with it. 10. Run the application locally \u00b6 Most developers like to run the code natively in local development environment. To do so, follow the instructions listed in the README.md file to run the code locally. You may be required to install a specific runtime like Java , Node or Go . If you want to quickly access your git repo you can run a helper command to open the git webpage. oc git From inside the folder where the code was cloned from GitHub, run the following command to install the Node.js dependencies. npm install Run the following command to start the application. npm run start 11. Make a change to your code \u00b6 Within GitHub it is possible to open a full Visual Studio code web editor and make changes to you code. Info The limitation is that you cannot do local testing for the learning journey it will be OK to make edits and lets the Tekton pipeline do the validation of the code. Editing Code in GitHub \u00b6 To edit your code in GitHub follow the tests below. Open your GitHub repository where your code was created from the template. Using your computer keyboard type the . key, this will open the repo into the visual editor. All the Starter Kits have a health API endpoint within them. Navigate to the code for the health endpoint. For Go its in routes/Health.go and for Node Typescript it is in src/controllers/health.controller.ts file. Edit the code and change the status: text to something like below. @ GET async healthCheck () : Promise < { status : string , message : string ;} > { return { status : 'UP' , message : \"App is up and running with TechZone Automation\" }; } Save your changes. Now update the test that validates this API open up test/controllers/health.controller.spec.ts and update the expect to match the value that is now being returned in the API. describe ( 'Given /health' , () => { test ( 'should return 200 status' , () => { return request ( app ). get ( '/health' ). expect ( 200 ); }); test ( 'should return {status: \"UP:}' , () => { return request ( app ). get ( '/health' ). expect ({ status : 'UP' , message : \"App is up and running with TechZone Automation\" }); }); }); Click on the Source Control icon on the left it will say 21 pending changes Click on the + icon to stage the changes Click on the tick icon at the top to push the changes, remember to add a commit message something like Update Health API and Test with message value Navigate back to the OpenShift console and click on Pipeline view you will see the tekton pipeline has kicked off again based on your code change. This will make it run through the build and deploy cycle. Local Development \u00b6 You can update your code locally using git command line. Go to your cloned git project and navigate to chart/base directory. cd stockbffnode cd chart/base Open the file Chart.yaml in edit mode and change the description field's value from \"A Helm chart for Kubernetes\" to \"A Helm chart for [yourprojectName]\" Save the edits Push the changes back to your repository git add . git commit -m \"Updated application name in Chart.yaml\" git push As soon as you push your code changes successfully, the webhook will trigger a new pipeline run for your project in your namespace in OCP. Warning Note: if the webhook registration step fails, the git push will not trigger the pipeline.","title":"Deploy First App"},{"location":"developer-intermediate/deploy-app/#overview","text":"The OpenShift development environment supports end-to-end development and deployment of an application. The instructions below will show you how to do it. You can create a new app using one of the Starter Kits . These have been created to include all the key components, configuration, and frameworks to get you started on creating the code you need for your solutions. This video demonstrates how to work through the steps to create an application and use a deployment pipeline to install it into your development cluster.","title":"Overview"},{"location":"developer-intermediate/deploy-app/#create-an-application","text":"The following steps will help you deploy your first application in your own namespace inside your OpenShift development environment.","title":"Create an application"},{"location":"developer-intermediate/deploy-app/#1-open-the-web-terminal","text":"To be able to run CLI commands to drive common operations on the cluster you will first need to open your web terminal. - Click on your web terminal >_ icon in the header of the OpenShift Console. You should see a terminal open at the bottom of the console screen. With a banner saying TechZone Automation - Check you can run oc commands, run the following command oc sync --version - You should see the version number printed Info You will now be able to quickly create Tekton pipelines within OpenShift. Watch this short video for more information on how Tekton is replacing Jenkins in the enterprise CI space.","title":"1. Open the Web Terminal"},{"location":"developer-intermediate/deploy-app/#2-create-the-development-namespace","text":"Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync dev- { your intials }","title":"2. Create the development namespace"},{"location":"developer-intermediate/deploy-app/#3-grant-required-access-to-the-service-account-of-the-namespace","text":"Openshift Image registry is being used for storing docker images.Hence,permission needs to be given to the service account of the namespace to be able to pull images from registry. oc policy add-role-to-group system:image-puller system:serviceaccounts: ${ DEV_NAMESPACE }","title":"3. Grant required access to the service account of the namespace"},{"location":"developer-intermediate/deploy-app/#4-open-the-developer-dashboard","text":"The Developer Dashboard makes it easy for you to navigate to the tools, including a section that allows you to select a set of preconfigured Starter Kits that make seeding your development project very easy. If you are logged into the OpenShift console, you can select the tools menu and select Developer Dashboard If you are on a laptop/desktop, open a browser and make sure you are logged into Github Open the dashboard by running the following command: oc dashboard","title":"4. Open the Developer Dashboard"},{"location":"developer-intermediate/deploy-app/#4-create-your-app-in-git","text":"From the Developer Dashboard, click on tab Warning If you are developing on a shared education cluster, you need to make it easy to identify your app. Please suffix the app name with your initials {app name}-{your initials} (e.g. stockbffnode-mjp ) and use the Git Organization for the shared environment. Your browser needs to be logged in to your GitHub account for a template to work. If the link from the tile displays the GitHub 404 page, log in and reload the page. Pick one of the templates that is a good architectural fit for your application and the language and framework that you prefer to work with. For your first application, select the Typescript Microservice . This also works well in the Cloud Shell. Click on a Tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a valid GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a name for your repo. GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template The new repository will be created in your selected organization.","title":"4. Create your app in Git"},{"location":"developer-intermediate/deploy-app/#5-register-the-application-in-a-openshift-pipeline","text":"Info We will be using the pipeline command and specifically the tekton technology to setup the CI pipeline. The pipeline command gives an option for both Jenkins and Tekton make sure you select Tekton . For more information about working with the different build engines, please see Continuous Integration with Jenkins Guide and Continuous Integration with Tekton Guide Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Start the process to create a pipeline from your Web Terminal oc pipeline ${ GIT_URL } --tekton For example: oc pipeline https://github.com/gct-showcase/inventory-svc-mjp --tekton For the deployment of your first app with OpenShift we have selected Tekton as the CI engine. The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, the CLI will attempt to determine the runtime used by the repository that was provided and filter the available pipelines. If only one matches then it will be automatically selected. If it cannot find a match or there is more than one match then you will be prompted to select the pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) The pipeline also prompts for linting the dockerfile ? lint-dockerfile: Enable the pipeline to lint the Dockerfile for best practices? ( Y/n ) To skip the scan, you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. Similarly, for skipping the linting of dockerfile,you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Dockerfile lint. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note: if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console.","title":"5. Register the application in a OpenShift Pipeline"},{"location":"developer-intermediate/deploy-app/#7-view-your-application-pipeline","text":"The steps to view your registered pipeline will vary based on type of pipeline ( Jenkins or Tekton ) and container platform version. From the OpenShift Web Console select the Pipelines menu in the Developer view You will see your microservice now running an OpenShift pipeline using Tekton Tasks","title":"7. View your application pipeline"},{"location":"developer-intermediate/deploy-app/#8-access-the-running-app","text":"Once the pipeline has completed successfully, the app will be deployed into the namespace used when registering the pipeline. To validate the app is running follow these steps: Note: Be sure the namespace context is set correctly by running the following command oc project Retrieve the app ingress endpoint using commands below in your terminal. oc endpoints From the endpoints listed, select the URL for the repo that was just deployed and press Enter to open that URL in a web browser. Validate the application is working as expected.","title":"8. Access the running app"},{"location":"developer-intermediate/deploy-app/#9-locate-the-app-in-the-web-console","text":"The build pipeline is configured to build the source code from the Git repository into a container image. This image is stored in the Image Registry . After that, the image is deployed into the same namespace/project within the development cluster where the pipeline ran and validated for its health. The steps below will walk through locating the installed application within the Web Console. Open the OpenShift web console Change to the Developer view Click on Topology menu Click on your application deployment in the topology view Click on the Overview tab Increase the number of running pods to 2 pods Click on the Resources tab to view the list of pods Click on View Logs link to view the logs from each pod You can see the running state of your application Success Success: You now have your application running inside your development cluster and being delivered using a Tekton based CI pipeline. This is the beginning of the developer journey with IBM Cloud. Having reached this point, we recommend you repeat the process a few more times using different Code Patterns templates and explore the Developer view in OpenShift to get familiar with it.","title":"9. Locate the app in the web console"},{"location":"developer-intermediate/deploy-app/#10-run-the-application-locally","text":"Most developers like to run the code natively in local development environment. To do so, follow the instructions listed in the README.md file to run the code locally. You may be required to install a specific runtime like Java , Node or Go . If you want to quickly access your git repo you can run a helper command to open the git webpage. oc git From inside the folder where the code was cloned from GitHub, run the following command to install the Node.js dependencies. npm install Run the following command to start the application. npm run start","title":"10. Run the application locally"},{"location":"developer-intermediate/deploy-app/#11-make-a-change-to-your-code","text":"Within GitHub it is possible to open a full Visual Studio code web editor and make changes to you code. Info The limitation is that you cannot do local testing for the learning journey it will be OK to make edits and lets the Tekton pipeline do the validation of the code.","title":"11. Make a change to your code"},{"location":"developer-intermediate/image-registry/","text":"Warning Note Only study this section if you are development on an OpenShift Development environment that is managed on IBM Cloud. If you are on AWS or Azure go to the OpenShift Image Registry content In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. When hosted in IBM Cloud, the uses the IBM Cloud Container Registry for storing container images. What is the IBM Cloud Container Registry \u00b6 IBM Cloud Container Registry is a private, multitenant Docker registry built into IBM Cloud for storing OCI images . Each IBM Cloud region hosts its own highly available registry. When deploying an application to a Kubernetes or OpenShift cluster, the cluster creates containers using the images in the registry. To package an application for deployment, the runtime must be built into an image that is stored in the registry. In this standard Docker diagram, the acme.com domain is effectively an IBM Cloud region and the Docker private registry is the instance of IBM Cloud Container Registry in that region. The diagram shows these components and their relationships: - Developer builds the image; ideally it is automated as part of a CI pipeline - Docker private registry stores the image that was built - UAT , Promote , and Production are deployment environments, such as Kubernetes clusters or namespaces, that run the containers based on the image Accessing the registry \u00b6 There are two ways to work with an IBM Cloud registry : - Web UI : In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry - CLI : Use the container-registry CLI plug-in in the IBM Cloud CLI Of the two approaches, the CLI is much more powerful. The console is mainly for looking at your registry. To use the container-registry plug-in, or even to push an image into the registry using your local Docker install, you must first log into IBM Cloud and then log into the region's IBM Cloud Container Registry: $ ibmcloud login $ ibmcloud cr login The logins determine the namespaces you can access . Other accounts for other tenants also have namespaces in the registry, but IBM Cloud doesn't let you see them and prevents you from accessing their images. Registry organization \u00b6 Like the directories and file names in a file system, a Docker registry is a single collection of images that are cataloged with hierarchical names. A Docker registry such as Docker Hub (or, as we'll see, a registry in IBM Cloud) stores an image with a hierarchical name: namespace, repository, and tag. This path is specified when tagging and pushing the image: $ docker tag <image> <namespace>/<repo-name>:<tag> $ docker push <namespace>/<repo-name>:<tag> To tag and push an image to any registry other than Docker Hub, you have to specify its domain as part of the path: $ docker push <domain>/<namespace>/<repo-name>:<tag> Registry organization in an IBM Cloud account \u00b6 IBM's registry organizes images in this same hierarchical structure. Domain : Each region in IBM Cloud (e.g. Dallas, London, Sydney, etc.) has its own multitenant registry instance with its own domain name (such as us.icr.io for Dallas/us-south). IBM provides its public images in a global registry (domain icr.io (no region)). Namespace : A namespace is associated with an IBM Cloud account and groups the account's images. Every user in the account can view and work with all images in the namespace, but users outside of the account cannot access images in the account's namespaces. An account may use multiple namespaces to organize images for groupings such as development vs. production or approved vs. experimental. Each namespace must have a name that is unique within a region for all accounts (not just your account). Repository : A repo is often thought of as the name of the image, but technically the same image can have different names (but the same image ID). Within a registry, different images with the same name stored in the same namespace will be stored in the same repo as long as they have different tags. Tag : Optional; if a command does not specify it, the default tag is latest . Two different tags enable a namespace to store two images with different image IDs but the same repository name. The tag typically specifies a different build of an image with a different image ID. Two builds usually package two different versions of an application's code, but Docker does not enforce that. The two builds could just be the same Dockerfile run twice with the same inputs (and therefore equivalent), or two completely unrelated sets of software. But two builds usually run software that is similar but at least slightly different, such as a new version of an application's code, but could be simply an alternative implementation of the code (such as for A/B testing). Therefore, when adding an image to a registry in IBM Cloud , the push command specifies the image's hierarchical path like this: docker push <region>.icr.io/<my_namespace>/<image_repo>:<tag> You can see this structure when you show the registry using the CLI: $ ibmcloud cr image-list Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0-5 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0-14 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues Notice the columns labeled Repository (which is domain/namespace/repo) and Tag --those comprise the path for finding each image. And Namespace is specified again in its own column, which is useful for filtering searches. You can also see that the first two items in the list are not two separate images but really the same image with two tags: the image ID (a.k.a. digest) is the same, so the two tags are two different ways to look up the same image. Likewise, the last two images are really the same image with two tags. The registry in the IBM Cloud console shows the same images: It shows more clearly that each image has two tags, rather than being two different images. IBM Cloud Container Registry features \u00b6 IBM Cloud Container Registry is not only a Docker registry hosted in IBM Cloud, it also adds several features to the registry service. The registry in each region is private, multitenant, and highly available--properties that a simple registry doesn't have. Here are some other features and capabilities. Vulnerability Advisor scans images in the registry to search for known security issues and generates reports with advice on how to fix your images and better secure your environment. Lists of the vulnerabilities scanned for are available in Vulnerable packages . An administrator can specify exemptions that should not be reported. Keep in mind that running containers are not scanned , only the images in the registry are. In the image lists shown above, the Security status column shows the number of issues found; the report will explain them in greater detail. In the console, click on the number of issues for details. Trusted content technology : IBM Cloud Container Registry supports images signed using Docker Content Trust (DCT) . The signature confirms who built the image, such as the CI tool. The push and pull commands maintain image signatures. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. User authorization : While all users in an account have access to that account's namespaces in the registry, an account administrator can use IAM to manage the access for different users. For example, a common customer concern is: \"How can an administrator control which images can be downloaded from Docker Hub and deployed into production?\" With IAM in the registry, an administrator can disable the ability to push, build, or delete images, then create a policy to allow these actions and only assign it to certain trusted users such as the CI pipeline's service ID. These privileged users should only add approved images into the registry, thereby limiting the containers that the developers can deploy. The administrators can likewise limit what the CD pipeline can possibly deploy to production by creating a namespace that only includes images approved for production and restricting the CD pipeline to deploy from that namespace. Image registry in the Pipeline \u00b6 The CI and CD pipelines currently exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: - Docker images : This Developer Tools Image Registry - Helm charts : A Helm repository in Artifactory In the CI pipeline, the Build image stage creates the Docker image and stores it in the registry. Then the Deploy to DEV env stage specifies the image's path in the Helm chart's values file, which the chart will use to deploy the app. Likewise, the CD pipeline specifies the image's registry path in the values files for the other deployments. When the Helm chart runs, it and Kubernetes read the image from the registry and start the containers. Give it a try \u00b6 Let's take a look at using the registry. If you haven't already, deploy your first app For example, deploy the named Typescript Microservice I deployed my in a project named dev-guide-example-bw In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry It doesn't matter whether you navigate via the Kubernetes panel or the OpenShift panel, they both share the same registry The registry is the IBM Cloud Container Registry for your region In the registry, search for the image named for your project In the CLI, run the corresponding command $ ibmcloud cr image-list | grep dev-guide-example-bw us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0-1 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues Back in the console, click on the image to see its details Under Security Status , click on 5 issues to see the issues that Vulnerability Advisor found in this image What it found were configuration issues Scroll down to see the list of configuration issues Conclusion \u00b6 We've seen that the CI pipeline packages an app as a Docker image and stores it in the registry, and also builds a Helm chart that references the image in the registry so that the chart can deploy containers built from the image. In IBM Cloud, the registry is the IBM Cloud Container Registry. The registry displays all of the namespaces in your account but not those in other tenants' accounts. In addition to basic registry functions, IBM Cloud adds additional features: private, multitenant, highly available, Vulnerability Advisor, Trusted content technology, Container Image Security Enforcement, and User authorization. Using the registry, you can see your list of images, details about the image, and details about any issues that Vulnerability Advisor found.","title":"IBM Cloud"},{"location":"developer-intermediate/image-registry/#what-is-the-ibm-cloud-container-registry","text":"IBM Cloud Container Registry is a private, multitenant Docker registry built into IBM Cloud for storing OCI images . Each IBM Cloud region hosts its own highly available registry. When deploying an application to a Kubernetes or OpenShift cluster, the cluster creates containers using the images in the registry. To package an application for deployment, the runtime must be built into an image that is stored in the registry. In this standard Docker diagram, the acme.com domain is effectively an IBM Cloud region and the Docker private registry is the instance of IBM Cloud Container Registry in that region. The diagram shows these components and their relationships: - Developer builds the image; ideally it is automated as part of a CI pipeline - Docker private registry stores the image that was built - UAT , Promote , and Production are deployment environments, such as Kubernetes clusters or namespaces, that run the containers based on the image","title":"What is the IBM Cloud Container Registry"},{"location":"developer-intermediate/image-registry/#accessing-the-registry","text":"There are two ways to work with an IBM Cloud registry : - Web UI : In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry - CLI : Use the container-registry CLI plug-in in the IBM Cloud CLI Of the two approaches, the CLI is much more powerful. The console is mainly for looking at your registry. To use the container-registry plug-in, or even to push an image into the registry using your local Docker install, you must first log into IBM Cloud and then log into the region's IBM Cloud Container Registry: $ ibmcloud login $ ibmcloud cr login The logins determine the namespaces you can access . Other accounts for other tenants also have namespaces in the registry, but IBM Cloud doesn't let you see them and prevents you from accessing their images.","title":"Accessing the registry"},{"location":"developer-intermediate/image-registry/#registry-organization","text":"Like the directories and file names in a file system, a Docker registry is a single collection of images that are cataloged with hierarchical names. A Docker registry such as Docker Hub (or, as we'll see, a registry in IBM Cloud) stores an image with a hierarchical name: namespace, repository, and tag. This path is specified when tagging and pushing the image: $ docker tag <image> <namespace>/<repo-name>:<tag> $ docker push <namespace>/<repo-name>:<tag> To tag and push an image to any registry other than Docker Hub, you have to specify its domain as part of the path: $ docker push <domain>/<namespace>/<repo-name>:<tag>","title":"Registry organization"},{"location":"developer-intermediate/image-registry/#registry-organization-in-an-ibm-cloud-account","text":"IBM's registry organizes images in this same hierarchical structure. Domain : Each region in IBM Cloud (e.g. Dallas, London, Sydney, etc.) has its own multitenant registry instance with its own domain name (such as us.icr.io for Dallas/us-south). IBM provides its public images in a global registry (domain icr.io (no region)). Namespace : A namespace is associated with an IBM Cloud account and groups the account's images. Every user in the account can view and work with all images in the namespace, but users outside of the account cannot access images in the account's namespaces. An account may use multiple namespaces to organize images for groupings such as development vs. production or approved vs. experimental. Each namespace must have a name that is unique within a region for all accounts (not just your account). Repository : A repo is often thought of as the name of the image, but technically the same image can have different names (but the same image ID). Within a registry, different images with the same name stored in the same namespace will be stored in the same repo as long as they have different tags. Tag : Optional; if a command does not specify it, the default tag is latest . Two different tags enable a namespace to store two images with different image IDs but the same repository name. The tag typically specifies a different build of an image with a different image ID. Two builds usually package two different versions of an application's code, but Docker does not enforce that. The two builds could just be the same Dockerfile run twice with the same inputs (and therefore equivalent), or two completely unrelated sets of software. But two builds usually run software that is similar but at least slightly different, such as a new version of an application's code, but could be simply an alternative implementation of the code (such as for A/B testing). Therefore, when adding an image to a registry in IBM Cloud , the push command specifies the image's hierarchical path like this: docker push <region>.icr.io/<my_namespace>/<image_repo>:<tag> You can see this structure when you show the registry using the CLI: $ ibmcloud cr image-list Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0-5 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0-14 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues Notice the columns labeled Repository (which is domain/namespace/repo) and Tag --those comprise the path for finding each image. And Namespace is specified again in its own column, which is useful for filtering searches. You can also see that the first two items in the list are not two separate images but really the same image with two tags: the image ID (a.k.a. digest) is the same, so the two tags are two different ways to look up the same image. Likewise, the last two images are really the same image with two tags. The registry in the IBM Cloud console shows the same images: It shows more clearly that each image has two tags, rather than being two different images.","title":"Registry organization in an IBM Cloud account"},{"location":"developer-intermediate/image-registry/#ibm-cloud-container-registry-features","text":"IBM Cloud Container Registry is not only a Docker registry hosted in IBM Cloud, it also adds several features to the registry service. The registry in each region is private, multitenant, and highly available--properties that a simple registry doesn't have. Here are some other features and capabilities. Vulnerability Advisor scans images in the registry to search for known security issues and generates reports with advice on how to fix your images and better secure your environment. Lists of the vulnerabilities scanned for are available in Vulnerable packages . An administrator can specify exemptions that should not be reported. Keep in mind that running containers are not scanned , only the images in the registry are. In the image lists shown above, the Security status column shows the number of issues found; the report will explain them in greater detail. In the console, click on the number of issues for details. Trusted content technology : IBM Cloud Container Registry supports images signed using Docker Content Trust (DCT) . The signature confirms who built the image, such as the CI tool. The push and pull commands maintain image signatures. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. User authorization : While all users in an account have access to that account's namespaces in the registry, an account administrator can use IAM to manage the access for different users. For example, a common customer concern is: \"How can an administrator control which images can be downloaded from Docker Hub and deployed into production?\" With IAM in the registry, an administrator can disable the ability to push, build, or delete images, then create a policy to allow these actions and only assign it to certain trusted users such as the CI pipeline's service ID. These privileged users should only add approved images into the registry, thereby limiting the containers that the developers can deploy. The administrators can likewise limit what the CD pipeline can possibly deploy to production by creating a namespace that only includes images approved for production and restricting the CD pipeline to deploy from that namespace.","title":"IBM Cloud Container Registry features"},{"location":"developer-intermediate/image-registry/#image-registry-in-the-pipeline","text":"The CI and CD pipelines currently exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: - Docker images : This Developer Tools Image Registry - Helm charts : A Helm repository in Artifactory In the CI pipeline, the Build image stage creates the Docker image and stores it in the registry. Then the Deploy to DEV env stage specifies the image's path in the Helm chart's values file, which the chart will use to deploy the app. Likewise, the CD pipeline specifies the image's registry path in the values files for the other deployments. When the Helm chart runs, it and Kubernetes read the image from the registry and start the containers.","title":"Image registry in the Pipeline"},{"location":"developer-intermediate/image-registry/#give-it-a-try","text":"Let's take a look at using the registry. If you haven't already, deploy your first app For example, deploy the named Typescript Microservice I deployed my in a project named dev-guide-example-bw In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry It doesn't matter whether you navigate via the Kubernetes panel or the OpenShift panel, they both share the same registry The registry is the IBM Cloud Container Registry for your region In the registry, search for the image named for your project In the CLI, run the corresponding command $ ibmcloud cr image-list | grep dev-guide-example-bw us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0-1 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues Back in the console, click on the image to see its details Under Security Status , click on 5 issues to see the issues that Vulnerability Advisor found in this image What it found were configuration issues Scroll down to see the list of configuration issues","title":"Give it a try"},{"location":"developer-intermediate/image-registry/#conclusion","text":"We've seen that the CI pipeline packages an app as a Docker image and stores it in the registry, and also builds a Helm chart that references the image in the registry so that the chart can deploy containers built from the image. In IBM Cloud, the registry is the IBM Cloud Container Registry. The registry displays all of the namespaces in your account but not those in other tenants' accounts. In addition to basic registry functions, IBM Cloud adds additional features: private, multitenant, highly available, Vulnerability Advisor, Trusted content technology, Container Image Security Enforcement, and User authorization. Using the registry, you can see your list of images, details about the image, and details about any issues that Vulnerability Advisor found.","title":"Conclusion"},{"location":"developer-intermediate/inventory-application/","text":"Develop and deploy an example microservices application Develop an example application with a three-tier microservices architecture and deploy it in IBM Cloud Kubernetes Service or Red Hat OpenShift on IBM Cloud using the IBM Garage Cloud-Native Toolkit Developer Environment including its Code Patterns. Business Need \u00b6 In this guide, imagine you have completed an Enterprise Design Thinking Workshop and the result is an MVP statement that defines the desired business outcomes. Use the steps below to help deliver this MVP quickly while following Garage Method best practices . MVP Statement \u00b6 An MVP is a first hill . Here's the hill statement for the MVP we're going to build: Who : Distribution employees in each of the regional warehouses What : A secure web application that enables easy access to list of product SKU inventory levels and inventory locations Wow : Make the system appealing and easy to use. Deliver it hosted on the IBM Cloud platform. Develop it quickly as a minimum viable product . Use the latest managed container runtimes and DevOps best practices to enable post MVP feature improvements. Simulate a release to a Test environment. Architecture \u00b6 We will build the Micro App using a three-tier microservices architecture. Each tier encapsulates a clean separation of concerns. Each app component will be modelled using microservices and use a number of polyglot programming languages and frameworks. Data will be stored in IBM Cloudant and the Micro App will be secured using IBM App ID. User interface \u00b6 The Micro App's user interface will look like this wireframe : Technical Requirements \u00b6 The Micro App should adhere to the following technical requirements: Microservices Stateless REST APIs Polyglot DevOps with CI/CD (continuous integration and continuous delivery) Monitoring and logging Code analysis App security Deployed to Red Hat OpenShift on IBM Cloud or IBM Cloud Kubernetes Service Follow the Carbon Design System user experience Guide \u00b6 You will approach creating the Micro App bottom up, meaning you will start by creating the backend microservice that manages integration with the data persistence and then build out the digital channel using a backend for frontend pattern. Finally, you will add a web UI to the solution.","title":"Intro"},{"location":"developer-intermediate/inventory-application/#business-need","text":"In this guide, imagine you have completed an Enterprise Design Thinking Workshop and the result is an MVP statement that defines the desired business outcomes. Use the steps below to help deliver this MVP quickly while following Garage Method best practices .","title":"Business Need"},{"location":"developer-intermediate/inventory-application/#mvp-statement","text":"An MVP is a first hill . Here's the hill statement for the MVP we're going to build: Who : Distribution employees in each of the regional warehouses What : A secure web application that enables easy access to list of product SKU inventory levels and inventory locations Wow : Make the system appealing and easy to use. Deliver it hosted on the IBM Cloud platform. Develop it quickly as a minimum viable product . Use the latest managed container runtimes and DevOps best practices to enable post MVP feature improvements. Simulate a release to a Test environment.","title":"MVP Statement"},{"location":"developer-intermediate/inventory-application/#architecture","text":"We will build the Micro App using a three-tier microservices architecture. Each tier encapsulates a clean separation of concerns. Each app component will be modelled using microservices and use a number of polyglot programming languages and frameworks. Data will be stored in IBM Cloudant and the Micro App will be secured using IBM App ID.","title":"Architecture"},{"location":"developer-intermediate/inventory-application/#user-interface","text":"The Micro App's user interface will look like this wireframe :","title":"User interface"},{"location":"developer-intermediate/inventory-application/#technical-requirements","text":"The Micro App should adhere to the following technical requirements: Microservices Stateless REST APIs Polyglot DevOps with CI/CD (continuous integration and continuous delivery) Monitoring and logging Code analysis App security Deployed to Red Hat OpenShift on IBM Cloud or IBM Cloud Kubernetes Service Follow the Carbon Design System user experience","title":"Technical Requirements"},{"location":"developer-intermediate/inventory-application/#guide","text":"You will approach creating the Micro App bottom up, meaning you will start by creating the backend microservice that manages integration with the data persistence and then build out the digital channel using a backend for frontend pattern. Finally, you will add a web UI to the solution.","title":"Guide"},{"location":"developer-intermediate/inventory-bff/","text":"Develop and deploy the BFF component of the inventory application The Inventory BFF's role in the architecture is to act as an orchestrator between the core business services and the specific digital channel it is focused on supporting. This class article will give you more detail about the Backend For Frontend architectural pattern and the benefits. Backend For Frontend pattern Overview - source The Inventory solution will use GraphQL for its BFF layer, which enables the API to be dynamically controlled from the client using API queries. Follow the steps below to get started. Setup \u00b6 [Optional]: Access cloud shell \u00b6 If you don't plan to use your workstation to run this lab, you can use IBM Cloud Shell: Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window. Setup your shell environment \u00b6 We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | sh - Note : If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.6.1/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli The installer updates PATH in the .zshrc or .bashrc file. You will need to source the file to apply the update to the current shell environment: if [[ \" ${ SHELL } \" = ~ zsh ]] ; then source ~/.zshrc else source ~/.bashrc fi You can check the shell was installed correctly by checking the oc version: oc sync --version Log in to OpenShift Cluster \u00b6 Log in to OpenShift Cluster from the cloud console. Go to Resource listStockItems and click on the cluster: Access the OpenShift console from within that console by clicking on the button. In OpenShift Console, click on email address top right, Click on Copy Login Command and get the OpenShift login command, which includes a token. click on Display Token , copy the Login with the token. oc login command will log you in. Run the login command in the cloud shell terminal: $ oc login --token = qvARHflZDlOYfjJZRJUEs53Yfy4F8aa6_L3ezoagQFM --server = https://c103-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" . Setup code base \u00b6 To get the initial BFF project created and registered with a pipeline for automated builds follow these steps. Create a new repository from the Typescript GraphQL template . Warning If you are developing on a shared education cluster, place the repository in the Git Organization listed in your notification email and remember to add your initials as a suffix to the app name. - In order to prevent naming collisions, name the repository inventory-management-bff-{your initials} replacing {your initials} with your actual initials. Clone the new repository to your machine. Run npm install to install all the package dependencies. Go into the repository directory cloned and execute the following: oc sync dev-{your initials} Register the pipeline: oc pipeline --tekton replacing {your initials} with your actual initials Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-<VERSION> . Open the pipeline to see it running, using the link provided in the command output. When the pipeline is completed, run oc endpoints -n dev-{your initials} . You should see an entry for the app we just pushed. Select the entry and hit Enter to launch the browser, if you are working on your desktop/laptop. Otherwise copy the url and paste it in a new browser tab. Create the REST interface \u00b6 The controller provides the REST interface for our BFF. The template uses the typescript-rest package to simplify the tasks required to create a controller. Since we will be developing this microservice following the Test Driven Development approach, we are first going to create the test for our stock-items controller. Start the tests by running the following command in a new terminal that you will keep running while running the lab: npm run tdd Create the controller test: ```typescript title=\"test/controllers/stock-items.controller.spec.ts\" import {Application} from 'express'; import * as request from 'supertest'; import {buildApiServer} from '../helper'; describe('stock-item.controller', () => { let app: Application; beforeEach(async () => { const apiServer = buildApiServer(); app = await apiServer.getApp(); }); test('canary verifies test infrastructure', () => { expect(true).toEqual(true); }); describe('given GET /stock-items', () => { describe('when service is successful', () => { test('then return 200 status', async () => { return request(app).get('/stock-items').expect(200); }); test('then should return an empty array', async () => { return request(app).get('/stock-items').expect([]); }); }); }); }); ``` Notice that tests are now failing for the nex tests. Create the controller component: ```typescript title=\"src/controllers/stock-items.controller.ts\" import {GET, Path} from 'typescript-rest'; @Path('stock-items') export class StockItemsController { @GET async listStockItems(): Promise { return []; } } ``` Add the controller to the controllers index.ts . (Using index.ts is a good way to manage which components are exposed by a component and provide a good way to load the modules that will be injected into other components): typescript title=\"src/controllers/index.ts\" export * from './health.controller'; export * from './stock-items.controller'; Start the service to see it running: npm start Access the running service. This service runs on port 3000 . Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Code Ready Workspaces Click on open link Desktop/Laptop Open a browser to http://localhost:3000/api-docs to see the swagger page Expand our service from the list, click Try it out , then click Execute Push the changes we've made to the repository: git add . git commit -m \"Adds stock items controller\" git push On the openshift console, open the pipeline to see it running . Update the controller to call a service \u00b6 The pattern recommended for the REST controllers is to let it focus on translating REST protocols into javascript and to put the business logic in a separate service component. The pattern recommended for the REST controllers is to let it focus on translating REST protocols into javascript and to put the business logic in a separate service component. Add a StockItem model that contains the values needed for the UI: typescript title=\"src/models/stock-item.model.ts\" export class StockItemModel { id: string; name: string; stock: number; unitPrice: number; manufacturer: string; picture: string; } Register the model with the index.ts file in the models directory. Append this to end of the file: typescript title=\"src/models/index.ts\" ... export * from './stock-item.model'; Define an abstract class to provide the interface for our API: ```typescript title=\"src/services/stock-items.api.ts\" import { StockItemModel } from '../models'; export abstract class StockItemsApi { abstract listStockItems(): Promise ; } ``` Note Why an abstract class and not an interface? TypeScript introduces both abstract classes and interfaces. When TypeScript gets transpiled into JavaScript, abstract classes are generated as classes but interfaces disappear since there isn't an equivalent type in JavaScript. As a result, they cannot be used as a binding type for the typescript-ioc framework. Fortunately, abstract classes can be used and they have the quirky behavior in TypeScript allowing them to either be extended like a class or implemented like an interface. Add the abstract class to the index.ts file in the services directory. Add it to the end of other export statements, do not overwrite the file: typescript title=\"src/services/index.ts\" ... export * from './stock-items.api'; ... Lets create an implementation that will provide mock data for now. Add a stock-items-mock.service to services: ```typescript title=\"src/services/stock-items-mock.service.ts\" import { StockItemsApi } from './stock-items.api'; import { StockItemModel } from '../models'; export class StockItemsMockService implements StockItemsApi { async listStockItems(): Promise { return [ { id: \"1\", name: \"Self-sealing stem bolt\", stock: 10, unitPrice: 10.5, picture: \"https://via.placeholder.com/32.png\", manufacturer: \"Bajor Galactic\" }, { id: \"2\", name: \"Heisenberg compensator\", stock: 20, unitPrice: 20.0, picture: \"https://via.placeholder.com/32.png\", manufacturer: \"Federation Imports\" }, { id: \"3\", name: \"Tooth sharpener\", stock: 30, unitPrice: 5.25, picture: \"https://via.placeholder.com/32.png\", manufacturer: \"Farenginar Exploits\" } ]; } } ``` Add the mock service to the index.ts file in the services directory: typescript title=\"src/services/index.ts\" ... export * from './stock-items-mock.service'; ... Update the controller test to inject the service into the controller and to return the value from the service: ```typescript title=\"test/controllers/stock-items.controller.spec.ts\" import { Application } from 'express'; import request from 'supertest'; import { Container } from 'typescript-ioc'; import { buildApiServer } from '../helper'; import Mock = jest.Mock; import { StockItemsMockService } from '../../src/services'; describe('stock-item.controller', () => { let app: Application; let service_listStockItems: Mock; beforeEach(async () => { service_listStockItems = jest.fn(); Container.bind(StockItemsMockService).factory( () => ({ listStockItems: service_listStockItems }), ); const apiServer = buildApiServer(); app = await apiServer.getApp(); }); test('canary verifies test infrastructure', () => { expect(true).toEqual(true); }); describe('given GET /stock-items', () => { describe('when service is successful', () => { const expectedResult = [{ value: 'val' }]; beforeEach(() => { service_listStockItems.mockResolvedValue(expectedResult); }); test('then return 200 status', async () => { return request(app).get('/stock-items').expect(200); }); test('then should return value from service', async () => { return request(app).get('/stock-items').expect(expectedResult); }); }); describe('when service fails', () => { beforeEach(() => { service_listStockItems.mockRejectedValue(new Error('service failed')); }); test('then return 502 error', async () => { return request(app).get('/stock-items').expect(502); }); }); }); }); ``` Update the controller to inject the service and use it: ```typescript title=\"src/import { Inject } from 'typescript-ioc'; import { GET, Path } from 'typescript-rest'; import { HttpError } from 'typescript-rest/dist/server/model/errors'; import { StockItemModel } from '../models'; import { StockItemsMockService } from '../services'; class BadGateway extends HttpError { constructor(message?: string) { super(\"BadGateway\", message); this.statusCode = 502; } } @Path('stock-items') export class StockItemsController { @Inject service: StockItemsMockService; @GET async listStockItems(): Promise { try { return await this.service.listStockItems(); } catch (err) { throw new BadGateway('There was an error'); } } } ``` Start the service npm start Access the running service. This service runs on port 3000 . Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Code Ready Workspaces Click on open link Desktop/Laptop Open a browser to http://localhost:3000/api-docs to see the swagger page Expand our service from the list, click Try it out , then click Execute Push the changes we've made to the repository: git add . git commit -m \"Adds a mock service implementation\" git push On the openshift console, open the pipeline to see it running . Add a GraphQL implementation of Stock Items \u00b6 The GraphQL template supports both REST and GraphQL APIs for accessing backend services. We have created a REST controller to expose the results from the service and now we will do the same for GraphQL. Create a stock-items GraphQL schema in the schemas directory: ```typescript title=\"src/schemas/stock-item.schema.ts\" import { Field, Float, Int, ObjectType } from 'type-graphql'; import { StockItemModel } from '../models'; @ObjectType() export class StockItem implements StockItemModel { @Field() id: string; @Field() manufacturer: string; @Field() name: string; @Field({ nullable: true }) picture: string; @Field(type => Int) stock: number; @Field(type => Float) unitPrice: number; } ``` Add the stock-items schema to the index.ts in the schemas directory: typescript title=\"src/schemas/index.ts\" export * from './stock-item.schema' Add a stock-item GraphQL resolver in the resolvers directory: ```typescript title=\"src/resolvers/stock-item.resolver.ts\" import { Query, Resolver } from 'type-graphql'; import { Inject } from 'typescript-ioc'; import { resolverManager } from './_resolver-manager'; import { StockItem } from '../schemas'; import { StockItemModel } from '../models'; import { StockItemsService } from '../services'; @Resolver(of => StockItem) export class StockItemResolver { @Inject service: StockItemsService; @Query(returns => [StockItem]) async stockItems(): Promise<StockItemModel[]> { return this.service.listStockItems(); } } resolverManager.registerResolver(StockItemResolver); ``` Note The template includes a resolverManager component that simplifies the steps to make the resolver available. All that is required to use the resolver is to register it, preferably at the bottom of the module where it is defined. Add the stock-items resolver to index.ts in the resolvers directory: typescript title=\"src/resolvers/index.ts\" export * from './stock-item.resolver'; Start the service: npm start Verify that the that the resolver is available using the Graph QL browser provided by the template: Open GraphQL Playground: http://localhost:3000/graphql Run the query query { stockItems { name } } Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Code Ready Workspaces Click on open link Desktop/Laptop Open GraphQL Playground: http://localhost:3000/graphql Run the query query { stockItems { name } } Push the changes we've made to the repository: git add . git commit -m \"Adds a graphql interface\" git push On the openshift console, open the pipeline to see it running . Create a service implementation that calls the microservice \u00b6 Add a stock-item-service.config file in the src/config directory: typescript title=\"src/config/stock-item-service.config.ts\" export class StockItemServiceConfig { baseUrl: string; } Add a stock-item-service.config.provider file in the src/config directory: ```typescript title=\"src/config/stock-item-service.config.provider.ts\" import {ObjectFactory} from 'typescript-ioc'; const baseUrl: string = process.env.SERVICE_URL || 'localhost:9080'; export const stockItemConfigFactory: ObjectFactory = () => ({ baseUrl, }); ``` The config class separates how the config is loaded from how it is used. In this case the config is simply retrieved from an environment variable but in more complex cases the value(s) can be retrieved from external data sources. Add the stock-item-service.config to an index.ts file of the config directory: ```typescript title=\"src/config/index.ts\" import { StockItemServiceConfig } from './stock-item-service.config'; import { stockItemConfigFactory } from './stock-item-service.config.provider'; import { Container } from 'typescript-ioc'; export * from './stock-item-service.config'; Container.bind(StockItemServiceConfig).factory(stockItemConfigFactory); ``` Create a stock-items service in the services directory that uses the config: ```typescript title=\"src/services/stock-items.service.ts\" import { Inject } from 'typescript-ioc'; import { get, Response } from 'superagent'; import { StockItemsApi } from './stock-items.api'; import { StockItemModel } from '../models'; import { StockItemServiceConfig } from '../config'; import { LoggerApi } from '../logger'; class StockItem { 'id'?: string; 'manufacturer'?: string; 'picture'?: string; 'name'?: string; 'price'?: number; 'stock'?: number; } export class StockItemsService implements StockItemsApi { @Inject _logger: LoggerApi; @Inject config: StockItemServiceConfig; get logger(): LoggerApi { return this._logger.child('StockItemsService'); } async listStockItems(): Promise<StockItemModel[]> { return new Promise((resolve, reject) => { get(`${this.config.baseUrl}/stock-items`) .set('Accept', 'application/json') .then(res => { console.error('LOGTAMER', res.body); resolve(this.mapStockItems(res.body)); }) .catch(err => { console.error('LOGTAMER', err); reject(err); }); }); } mapStockItems(data: StockItem[]): StockItemModel[] { return data.map(this.mapStockItem); } mapStockItem(item: StockItem): StockItemModel { return { id: item.id, name: item.name, stock: item.stock, unitPrice: item.price, picture: item.picture ?? 'https://via.placeholder.com/32.png', manufacturer: item.manufacturer, }; } } ``` Add stock-item.service to index.ts in the service directory Replace StockItemsMockService with StockItemsService in the following files: src/resolvers/stock-item.resolver.ts src/controllers/stock-items.controller.ts test/controllers/stock-items.controller.spec.ts Modify connectsTo property to the values.yaml file of the Helm chart. The value of the property should match the Kubernetes service of the microservice. (For template projects, the service name is the same as the name of the application which is that same as the name of the repository.) ```yaml title=\"chart/base/values.yaml\" ... connectsTo: inventory-management-svc-{your initials} ... ``` Info The values.yaml file of the Helm chart defines the variables that can be provided to the template as input. Now that we've added a new variable, we will need to update the appropriate template file to use our new variable. Add a new environment variable named SERVICE_URL to the list of existing environment variables in deployment.yaml. ( SERVICE_URL is the name we gave the environment variable in our stock-item-service.config class as the first step in this section.) The value of this environment variable should come from the connectsTo value we defined. You can add | quote to wrap the value in quotes in case the value is not formatted correctly: yaml title=\"chart/base/templates/deployment.yaml\" ... env: - name: INGRESS_HOST value: \"\" - name: PROTOCOLS value: \"\" - name: LOG_LEVEL value: {{ .Values.logLevel | quote }} - name: SERVICE_URL value: {{ printf \"%s:80\" .Values.connectsTo | quote }} ... Info deployment.yaml is a templatized Kubernetes yaml file that describes the deployment of our component. The deployment will create one or more pods based on the pod template defined in the deployment. Each pod that starts will have the environment variables that we have defined in the env section available for the container image to reference. Commit and push the changes to git: git add . git commit -m \"Adds service implementation\" git push On the openshift console, open the pipeline to see it running .","title":"BFF"},{"location":"developer-intermediate/inventory-bff/#setup","text":"","title":"Setup"},{"location":"developer-intermediate/inventory-bff/#optional-access-cloud-shell","text":"If you don't plan to use your workstation to run this lab, you can use IBM Cloud Shell: Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window.","title":"[Optional]: Access cloud shell"},{"location":"developer-intermediate/inventory-bff/#setup-your-shell-environment","text":"We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | sh - Note : If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.6.1/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli The installer updates PATH in the .zshrc or .bashrc file. You will need to source the file to apply the update to the current shell environment: if [[ \" ${ SHELL } \" = ~ zsh ]] ; then source ~/.zshrc else source ~/.bashrc fi You can check the shell was installed correctly by checking the oc version: oc sync --version","title":"Setup your shell environment"},{"location":"developer-intermediate/inventory-bff/#log-in-to-openshift-cluster","text":"Log in to OpenShift Cluster from the cloud console. Go to Resource listStockItems and click on the cluster: Access the OpenShift console from within that console by clicking on the button. In OpenShift Console, click on email address top right, Click on Copy Login Command and get the OpenShift login command, which includes a token. click on Display Token , copy the Login with the token. oc login command will log you in. Run the login command in the cloud shell terminal: $ oc login --token = qvARHflZDlOYfjJZRJUEs53Yfy4F8aa6_L3ezoagQFM --server = https://c103-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" .","title":"Log in to OpenShift Cluster"},{"location":"developer-intermediate/inventory-bff/#setup-code-base","text":"To get the initial BFF project created and registered with a pipeline for automated builds follow these steps. Create a new repository from the Typescript GraphQL template . Warning If you are developing on a shared education cluster, place the repository in the Git Organization listed in your notification email and remember to add your initials as a suffix to the app name. - In order to prevent naming collisions, name the repository inventory-management-bff-{your initials} replacing {your initials} with your actual initials. Clone the new repository to your machine. Run npm install to install all the package dependencies. Go into the repository directory cloned and execute the following: oc sync dev-{your initials} Register the pipeline: oc pipeline --tekton replacing {your initials} with your actual initials Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-<VERSION> . Open the pipeline to see it running, using the link provided in the command output. When the pipeline is completed, run oc endpoints -n dev-{your initials} . You should see an entry for the app we just pushed. Select the entry and hit Enter to launch the browser, if you are working on your desktop/laptop. Otherwise copy the url and paste it in a new browser tab.","title":"Setup code base"},{"location":"developer-intermediate/inventory-bff/#create-the-rest-interface","text":"The controller provides the REST interface for our BFF. The template uses the typescript-rest package to simplify the tasks required to create a controller. Since we will be developing this microservice following the Test Driven Development approach, we are first going to create the test for our stock-items controller. Start the tests by running the following command in a new terminal that you will keep running while running the lab: npm run tdd Create the controller test: ```typescript title=\"test/controllers/stock-items.controller.spec.ts\" import {Application} from 'express'; import * as request from 'supertest'; import {buildApiServer} from '../helper'; describe('stock-item.controller', () => { let app: Application; beforeEach(async () => { const apiServer = buildApiServer(); app = await apiServer.getApp(); }); test('canary verifies test infrastructure', () => { expect(true).toEqual(true); }); describe('given GET /stock-items', () => { describe('when service is successful', () => { test('then return 200 status', async () => { return request(app).get('/stock-items').expect(200); }); test('then should return an empty array', async () => { return request(app).get('/stock-items').expect([]); }); }); }); }); ``` Notice that tests are now failing for the nex tests. Create the controller component: ```typescript title=\"src/controllers/stock-items.controller.ts\" import {GET, Path} from 'typescript-rest'; @Path('stock-items') export class StockItemsController { @GET async listStockItems(): Promise { return []; } } ``` Add the controller to the controllers index.ts . (Using index.ts is a good way to manage which components are exposed by a component and provide a good way to load the modules that will be injected into other components): typescript title=\"src/controllers/index.ts\" export * from './health.controller'; export * from './stock-items.controller'; Start the service to see it running: npm start Access the running service. This service runs on port 3000 . Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Code Ready Workspaces Click on open link Desktop/Laptop Open a browser to http://localhost:3000/api-docs to see the swagger page Expand our service from the list, click Try it out , then click Execute Push the changes we've made to the repository: git add . git commit -m \"Adds stock items controller\" git push On the openshift console, open the pipeline to see it running .","title":"Create the REST interface"},{"location":"developer-intermediate/inventory-bff/#update-the-controller-to-call-a-service","text":"The pattern recommended for the REST controllers is to let it focus on translating REST protocols into javascript and to put the business logic in a separate service component. The pattern recommended for the REST controllers is to let it focus on translating REST protocols into javascript and to put the business logic in a separate service component. Add a StockItem model that contains the values needed for the UI: typescript title=\"src/models/stock-item.model.ts\" export class StockItemModel { id: string; name: string; stock: number; unitPrice: number; manufacturer: string; picture: string; } Register the model with the index.ts file in the models directory. Append this to end of the file: typescript title=\"src/models/index.ts\" ... export * from './stock-item.model'; Define an abstract class to provide the interface for our API: ```typescript title=\"src/services/stock-items.api.ts\" import { StockItemModel } from '../models'; export abstract class StockItemsApi { abstract listStockItems(): Promise ; } ``` Note Why an abstract class and not an interface? TypeScript introduces both abstract classes and interfaces. When TypeScript gets transpiled into JavaScript, abstract classes are generated as classes but interfaces disappear since there isn't an equivalent type in JavaScript. As a result, they cannot be used as a binding type for the typescript-ioc framework. Fortunately, abstract classes can be used and they have the quirky behavior in TypeScript allowing them to either be extended like a class or implemented like an interface. Add the abstract class to the index.ts file in the services directory. Add it to the end of other export statements, do not overwrite the file: typescript title=\"src/services/index.ts\" ... export * from './stock-items.api'; ... Lets create an implementation that will provide mock data for now. Add a stock-items-mock.service to services: ```typescript title=\"src/services/stock-items-mock.service.ts\" import { StockItemsApi } from './stock-items.api'; import { StockItemModel } from '../models'; export class StockItemsMockService implements StockItemsApi { async listStockItems(): Promise { return [ { id: \"1\", name: \"Self-sealing stem bolt\", stock: 10, unitPrice: 10.5, picture: \"https://via.placeholder.com/32.png\", manufacturer: \"Bajor Galactic\" }, { id: \"2\", name: \"Heisenberg compensator\", stock: 20, unitPrice: 20.0, picture: \"https://via.placeholder.com/32.png\", manufacturer: \"Federation Imports\" }, { id: \"3\", name: \"Tooth sharpener\", stock: 30, unitPrice: 5.25, picture: \"https://via.placeholder.com/32.png\", manufacturer: \"Farenginar Exploits\" } ]; } } ``` Add the mock service to the index.ts file in the services directory: typescript title=\"src/services/index.ts\" ... export * from './stock-items-mock.service'; ... Update the controller test to inject the service into the controller and to return the value from the service: ```typescript title=\"test/controllers/stock-items.controller.spec.ts\" import { Application } from 'express'; import request from 'supertest'; import { Container } from 'typescript-ioc'; import { buildApiServer } from '../helper'; import Mock = jest.Mock; import { StockItemsMockService } from '../../src/services'; describe('stock-item.controller', () => { let app: Application; let service_listStockItems: Mock; beforeEach(async () => { service_listStockItems = jest.fn(); Container.bind(StockItemsMockService).factory( () => ({ listStockItems: service_listStockItems }), ); const apiServer = buildApiServer(); app = await apiServer.getApp(); }); test('canary verifies test infrastructure', () => { expect(true).toEqual(true); }); describe('given GET /stock-items', () => { describe('when service is successful', () => { const expectedResult = [{ value: 'val' }]; beforeEach(() => { service_listStockItems.mockResolvedValue(expectedResult); }); test('then return 200 status', async () => { return request(app).get('/stock-items').expect(200); }); test('then should return value from service', async () => { return request(app).get('/stock-items').expect(expectedResult); }); }); describe('when service fails', () => { beforeEach(() => { service_listStockItems.mockRejectedValue(new Error('service failed')); }); test('then return 502 error', async () => { return request(app).get('/stock-items').expect(502); }); }); }); }); ``` Update the controller to inject the service and use it: ```typescript title=\"src/import { Inject } from 'typescript-ioc'; import { GET, Path } from 'typescript-rest'; import { HttpError } from 'typescript-rest/dist/server/model/errors'; import { StockItemModel } from '../models'; import { StockItemsMockService } from '../services'; class BadGateway extends HttpError { constructor(message?: string) { super(\"BadGateway\", message); this.statusCode = 502; } } @Path('stock-items') export class StockItemsController { @Inject service: StockItemsMockService; @GET async listStockItems(): Promise { try { return await this.service.listStockItems(); } catch (err) { throw new BadGateway('There was an error'); } } } ``` Start the service npm start Access the running service. This service runs on port 3000 . Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Code Ready Workspaces Click on open link Desktop/Laptop Open a browser to http://localhost:3000/api-docs to see the swagger page Expand our service from the list, click Try it out , then click Execute Push the changes we've made to the repository: git add . git commit -m \"Adds a mock service implementation\" git push On the openshift console, open the pipeline to see it running .","title":"Update the controller to call a service"},{"location":"developer-intermediate/inventory-bff/#add-a-graphql-implementation-of-stock-items","text":"The GraphQL template supports both REST and GraphQL APIs for accessing backend services. We have created a REST controller to expose the results from the service and now we will do the same for GraphQL. Create a stock-items GraphQL schema in the schemas directory: ```typescript title=\"src/schemas/stock-item.schema.ts\" import { Field, Float, Int, ObjectType } from 'type-graphql'; import { StockItemModel } from '../models'; @ObjectType() export class StockItem implements StockItemModel { @Field() id: string; @Field() manufacturer: string; @Field() name: string; @Field({ nullable: true }) picture: string; @Field(type => Int) stock: number; @Field(type => Float) unitPrice: number; } ``` Add the stock-items schema to the index.ts in the schemas directory: typescript title=\"src/schemas/index.ts\" export * from './stock-item.schema' Add a stock-item GraphQL resolver in the resolvers directory: ```typescript title=\"src/resolvers/stock-item.resolver.ts\" import { Query, Resolver } from 'type-graphql'; import { Inject } from 'typescript-ioc'; import { resolverManager } from './_resolver-manager'; import { StockItem } from '../schemas'; import { StockItemModel } from '../models'; import { StockItemsService } from '../services'; @Resolver(of => StockItem) export class StockItemResolver { @Inject service: StockItemsService; @Query(returns => [StockItem]) async stockItems(): Promise<StockItemModel[]> { return this.service.listStockItems(); } } resolverManager.registerResolver(StockItemResolver); ``` Note The template includes a resolverManager component that simplifies the steps to make the resolver available. All that is required to use the resolver is to register it, preferably at the bottom of the module where it is defined. Add the stock-items resolver to index.ts in the resolvers directory: typescript title=\"src/resolvers/index.ts\" export * from './stock-item.resolver'; Start the service: npm start Verify that the that the resolver is available using the Graph QL browser provided by the template: Open GraphQL Playground: http://localhost:3000/graphql Run the query query { stockItems { name } } Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Code Ready Workspaces Click on open link Desktop/Laptop Open GraphQL Playground: http://localhost:3000/graphql Run the query query { stockItems { name } } Push the changes we've made to the repository: git add . git commit -m \"Adds a graphql interface\" git push On the openshift console, open the pipeline to see it running .","title":"Add a GraphQL implementation of Stock Items"},{"location":"developer-intermediate/inventory-bff/#create-a-service-implementation-that-calls-the-microservice","text":"Add a stock-item-service.config file in the src/config directory: typescript title=\"src/config/stock-item-service.config.ts\" export class StockItemServiceConfig { baseUrl: string; } Add a stock-item-service.config.provider file in the src/config directory: ```typescript title=\"src/config/stock-item-service.config.provider.ts\" import {ObjectFactory} from 'typescript-ioc'; const baseUrl: string = process.env.SERVICE_URL || 'localhost:9080'; export const stockItemConfigFactory: ObjectFactory = () => ({ baseUrl, }); ``` The config class separates how the config is loaded from how it is used. In this case the config is simply retrieved from an environment variable but in more complex cases the value(s) can be retrieved from external data sources. Add the stock-item-service.config to an index.ts file of the config directory: ```typescript title=\"src/config/index.ts\" import { StockItemServiceConfig } from './stock-item-service.config'; import { stockItemConfigFactory } from './stock-item-service.config.provider'; import { Container } from 'typescript-ioc'; export * from './stock-item-service.config'; Container.bind(StockItemServiceConfig).factory(stockItemConfigFactory); ``` Create a stock-items service in the services directory that uses the config: ```typescript title=\"src/services/stock-items.service.ts\" import { Inject } from 'typescript-ioc'; import { get, Response } from 'superagent'; import { StockItemsApi } from './stock-items.api'; import { StockItemModel } from '../models'; import { StockItemServiceConfig } from '../config'; import { LoggerApi } from '../logger'; class StockItem { 'id'?: string; 'manufacturer'?: string; 'picture'?: string; 'name'?: string; 'price'?: number; 'stock'?: number; } export class StockItemsService implements StockItemsApi { @Inject _logger: LoggerApi; @Inject config: StockItemServiceConfig; get logger(): LoggerApi { return this._logger.child('StockItemsService'); } async listStockItems(): Promise<StockItemModel[]> { return new Promise((resolve, reject) => { get(`${this.config.baseUrl}/stock-items`) .set('Accept', 'application/json') .then(res => { console.error('LOGTAMER', res.body); resolve(this.mapStockItems(res.body)); }) .catch(err => { console.error('LOGTAMER', err); reject(err); }); }); } mapStockItems(data: StockItem[]): StockItemModel[] { return data.map(this.mapStockItem); } mapStockItem(item: StockItem): StockItemModel { return { id: item.id, name: item.name, stock: item.stock, unitPrice: item.price, picture: item.picture ?? 'https://via.placeholder.com/32.png', manufacturer: item.manufacturer, }; } } ``` Add stock-item.service to index.ts in the service directory Replace StockItemsMockService with StockItemsService in the following files: src/resolvers/stock-item.resolver.ts src/controllers/stock-items.controller.ts test/controllers/stock-items.controller.spec.ts Modify connectsTo property to the values.yaml file of the Helm chart. The value of the property should match the Kubernetes service of the microservice. (For template projects, the service name is the same as the name of the application which is that same as the name of the repository.) ```yaml title=\"chart/base/values.yaml\" ... connectsTo: inventory-management-svc-{your initials} ... ``` Info The values.yaml file of the Helm chart defines the variables that can be provided to the template as input. Now that we've added a new variable, we will need to update the appropriate template file to use our new variable. Add a new environment variable named SERVICE_URL to the list of existing environment variables in deployment.yaml. ( SERVICE_URL is the name we gave the environment variable in our stock-item-service.config class as the first step in this section.) The value of this environment variable should come from the connectsTo value we defined. You can add | quote to wrap the value in quotes in case the value is not formatted correctly: yaml title=\"chart/base/templates/deployment.yaml\" ... env: - name: INGRESS_HOST value: \"\" - name: PROTOCOLS value: \"\" - name: LOG_LEVEL value: {{ .Values.logLevel | quote }} - name: SERVICE_URL value: {{ printf \"%s:80\" .Values.connectsTo | quote }} ... Info deployment.yaml is a templatized Kubernetes yaml file that describes the deployment of our component. The deployment will create one or more pods based on the pod template defined in the deployment. Each pod that starts will have the environment variables that we have defined in the env section available for the container image to reference. Commit and push the changes to git: git add . git commit -m \"Adds service implementation\" git push On the openshift console, open the pipeline to see it running .","title":"Create a service implementation that calls the microservice"},{"location":"developer-intermediate/inventory-prebuilt-solution/","text":"Prebuilt Solution \u00b6 If you want to skip the guide and just get the components running, see Deploy the Inventory App solution . Deploy the Inventory App solution \u00b6 If you were unable to get everything working, you can deploy the Inventory App solution. Inventory Service \u00b6 Create a new project from the Inventory Management Service solution template named inventory-management-svc-solution Clone the repository to your local machine Go into the repository directory cloned and execute the following oc sync dev-{your initials} Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Execution of the above command. Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select ibm-java-gradle . Open the pipeline to see it running Inventory BFF \u00b6 Create a new project from the Inventory Management BFF solution template named inventory-management-bff-solution Clone the repository to your local machine Go into the repository directory cloned and execute oc sync command. if you are not executed previously. Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Execution of the above command. Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-v1-2-0 . Open the pipeline to see it running Get the backend service name this value by executing the command `oc get svc -n dev-{initials}. $ oc get svc -n dev-ar NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE inventory-bff-ar ClusterIP 172.21.180.68 <none> 80/TCP 14m inventory-svc-ar ClusterIP 172.21.4.95 <none> 80/TCP 20m Update the connectsTo value in charts/base/values.yaml to point to the kubernetes service of the microservice: connectsTo: inventory-svc-ar Commit and push the changes git add . git commit -m \"Update the connectsTo\" git push Inventory UI \u00b6 Create a new project from the Inventory Management UI solution template named inventory-management-ui-solution Clone the repository to your local machine Go into the repository directory cloned and execute oc sync command. if you are not executed previously. Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Execution of the above command. Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-v1-2-0 . Open the pipeline to see it running . Get the bff service name this value by executing the command `oc get svc -n dev-{initials}. $ oc get svc -n dev-ar NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE inventory-bff-ar ClusterIP 172.21.180.68 <none> 80/TCP 14m inventory-svc-ar ClusterIP 172.21.4.95 <none> 80/TCP 20m Update the connectsTo value in charts/base/values.yaml to point to the kubernetes service of the microservice: connectsTo: inventory-bff-ar Commit and push the changes git add . git commit -m \"Update the connectTo\" git push Verify that the application is running","title":"Solution"},{"location":"developer-intermediate/inventory-prebuilt-solution/#prebuilt-solution","text":"If you want to skip the guide and just get the components running, see Deploy the Inventory App solution .","title":"Prebuilt Solution"},{"location":"developer-intermediate/inventory-prebuilt-solution/#deploy-the-inventory-app-solution","text":"If you were unable to get everything working, you can deploy the Inventory App solution.","title":"Deploy the Inventory App solution"},{"location":"developer-intermediate/inventory-prebuilt-solution/#inventory-service","text":"Create a new project from the Inventory Management Service solution template named inventory-management-svc-solution Clone the repository to your local machine Go into the repository directory cloned and execute the following oc sync dev-{your initials} Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Execution of the above command. Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select ibm-java-gradle . Open the pipeline to see it running","title":"Inventory Service"},{"location":"developer-intermediate/inventory-prebuilt-solution/#inventory-bff","text":"Create a new project from the Inventory Management BFF solution template named inventory-management-bff-solution Clone the repository to your local machine Go into the repository directory cloned and execute oc sync command. if you are not executed previously. Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Execution of the above command. Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-v1-2-0 . Open the pipeline to see it running Get the backend service name this value by executing the command `oc get svc -n dev-{initials}. $ oc get svc -n dev-ar NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE inventory-bff-ar ClusterIP 172.21.180.68 <none> 80/TCP 14m inventory-svc-ar ClusterIP 172.21.4.95 <none> 80/TCP 20m Update the connectsTo value in charts/base/values.yaml to point to the kubernetes service of the microservice: connectsTo: inventory-svc-ar Commit and push the changes git add . git commit -m \"Update the connectsTo\" git push","title":"Inventory BFF"},{"location":"developer-intermediate/inventory-prebuilt-solution/#inventory-ui","text":"Create a new project from the Inventory Management UI solution template named inventory-management-ui-solution Clone the repository to your local machine Go into the repository directory cloned and execute oc sync command. if you are not executed previously. Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Execution of the above command. Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-v1-2-0 . Open the pipeline to see it running . Get the bff service name this value by executing the command `oc get svc -n dev-{initials}. $ oc get svc -n dev-ar NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE inventory-bff-ar ClusterIP 172.21.180.68 <none> 80/TCP 14m inventory-svc-ar ClusterIP 172.21.4.95 <none> 80/TCP 20m Update the connectsTo value in charts/base/values.yaml to point to the kubernetes service of the microservice: connectsTo: inventory-bff-ar Commit and push the changes git add . git commit -m \"Update the connectTo\" git push Verify that the application is running","title":"Inventory UI"},{"location":"developer-intermediate/inventory-service/","text":"Develop and deploy the backend component of the inventory application Setup \u00b6 Setup your cloud shell \u00b6 Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window. We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | sh - Note : If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.6.1/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli The installer updates PATH in the .zshrc or .bashrc file. You will need to source the file to apply the update to the current shell environment: if [[ \" ${ SHELL } \" = ~ zsh ]] ; then source ~/.zshrc else source ~/.bashrc fi You can check the shell was installed correctly by checking the oc version: oc sync --version Log in to OpenShift Cluster from the cloud console.Go to Resource List and click on the cluster: Access the OpenShift console from within that console by clicking on the button. In OpenShift Console, click on email address top right, Click on Copy Login Command and get the OpenShift login command, which includes a token. click on Display Token, copy the Login with the token. oc login command will log you in. Run the login command in the cloud shell terminal: $ oc login --token = qvARHflZDlOYfjJZRJUEs53Yfy4F8aa6_L3ezoagQFM --server = https://c103-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" . Create the initial project and register it with a pipeline for automated builds. Create a new repository from the Spring Boot Microservice template. Make the cloned repository public. You can also access this template on the Code Patterns page in the Developer Dashboard . Warning If you are developing on a shared education cluster, place the repository in the Git Organization listed in your notification email and remember to add your initials as a suffix to the app name. In order to prevent naming collisions, name the repository inventory-management-svc-{your initials} , replacing {your initials} with your actual initials. Clone the new repository to your machine git clone https://github.com/ibm-workshop-team-one/inventory-svc-{your initials}.git Go into the repository directory cloned and execute the following oc sync dev-{your initials} Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select ibm-java-gradle $ oc pipeline --tekton Creating pipeline on openshift cluster in dev-ar namespace Retrieving git parameters Project git repo: https://github.com/aminerachyd/inventory-management-svc-ar.git ? Provide the git username: aminerachyd ? Provide the git password or personal access token: [ hidden ] Branch: main Retrieving available template pipelines from tools Pipeline templates filtered based on detected runtime: openjdk/gradle ? Select the Pipeline to use in the PipelineRun: ibm-java-gradle ? scan-image: Enable the pipeline to scan the image for vulnerabilities? Yes ? health-endpoint: Endpoint to check health after deployment, liberty uses / not /health? /health ? lint-dockerfile: Enable the pipeline to lint the Dockerfile for best practices? Yes Copying tasks from tools.... Copied Pipeline from tools/ibm-java-gradle to dev-ar/inventory-management-svc-ar Creating TriggerTemplate for pipeline: inventory-management-svc-ar Creating TriggerBinding for pipeline: inventory-management-svc-ar Creating/updating TriggerEventListener for pipeline: tekton Waiting for event listener rollout: dev-ar/el-tekton Creating/updating Route for pipeline: tekton Creating PipelineRun for pipeline: inventory-management-svc-ar Creating Github webhook for repo: https://github.com/aminerachyd/inventory-management-svc-ar.git Warning: Webhook already exists for this trigger in this repository. Pipeline run started: inventory-management-svc-ar-181f77c24a4 Open the pipeline to see it running When the pipeline is completed, run oc endpoints -n dev-{your initials} . You should see an entry for the app we just pushed. Select the entry and hit Enter to launch the browser. If you are developing on code ready workspaces/cloud shell, copy the url and paste it in a new browser window. Run the service locally ./gradlew bootRun When the execution output says Server started , the app is running. Access the running service. This service runs on port 9080. Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Code Ready Workspaces Click on yes Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html This will display the Swagger UI page that provides a user interface to exercise the APIs. Create initial components \u00b6 Spring Boot uses annotations to configure the various components that will be injected into and used by the applications. A class with the @SpringBootApplication annotation is the starting point for the rest of the application components to be loaded. Additionally, a @ComponentScan annotation can be added to tell the Spring infrastructure which packages should be scanned for components. We will start by creating the initial application component. Create a class named Application in the com.ibm.inventory_management.app package. Add the @SpringBootApplication and @ComponentScan annotation to the class. The @ComponentScan annotation should include com.ibm.inventory_management.* , com.ibm.cloud_native_toolkit.* , and com.ibm.health packages. ```java title=\"src/main/java/com/ibm/inventory_management/app/Application.java\" package com.ibm.inventory_management.app; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.builder.SpringApplicationBuilder; import org.springframework.boot.web.servlet.support.SpringBootServletInitializer; import org.springframework.context.ApplicationContext; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.core.env.Environment; import springfox.documentation.swagger2.annotations.EnableSwagger2 @SpringBootApplication @EnableSwagger2 @ComponentScan({\"com.ibm.inventory_management. \", \"com.ibm.cloud_native_toolkit. \", \"com.ibm.health\"}) public class Application extends SpringBootServletInitializer { @Autowired Environment environment; public static void main(String[] args) { SpringApplication.run(com.ibm.inventory_management.app.Application.class, args); } @Bean public CommandLineRunner commandLineRunner(ApplicationContext ctx) { return args -> { String port = environment.getProperty(\"local.server.port\"); System.out.println(); System.out.println(\"Server started - http://localhost:\" + port + \"/swagger-ui.html\"); }; } @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(Application.class); } } ``` Delete application.app git rm -r src/main/java/application/ Run the service locally. The swagger page should no longer contain the /hello API endpoint. Commit and push the changes to Git. git add . git commit -m \"Adds Application and Removes default Application class\" git push Add StockItem controller \u00b6 In Spring Boot, the @RestController annotation tells the framework that the class provides a REST interface. Additional annotations like @GetMapping are used to provide the specific configuration for the REST service. Start the tests in tdd mode with npm run tdd (or ./gradlew test --continuous ) Add a StockItemControllerTest.java in com.ibm.inventory_management.controllers under the test folder ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import org.junit.jupiter.api.DisplayName; @DisplayName(\"StockItemController\") public class StockItemControllerTest { } ``` Add the MockMvc infrastructure and create the StockItemController ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import static org.mockito.Mockito.spy; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.DisplayName; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; @DisplayName(\"StockItemController\") public class StockItemControllerTest { StockItemController controller; MockMvc mockMvc; @BeforeEach public void setup() { controller = spy(new StockItemController()); mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); } } java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; public class StockItemController { } ``` Add the tests for the controller behavior and make the corresponding changes to make the tests pass ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import static org.mockito.Mockito.spy; import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.api.Nested; import org.junit.jupiter.api.Test; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; @DisplayName(\"StockItemController\") public class StockItemControllerTest { StockItemController controller; MockMvc mockMvc; @BeforeEach public void setup() { controller = spy(new StockItemController()); mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); } @Nested @DisplayName(\"Given [GET] /stock-items\") public class GivenGetStockItems { @Test @DisplayName(\"When called then it should return a 200 status\") public void when_called_should_return_200_status() throws Exception { mockMvc.perform(get(\"/stock-items\")) .andExpect(status().isOk()); } @Test @DisplayName(\"When called then it should return an empty array\") public void when_called_then_return_an_empty_array() throws Exception { mockMvc.perform(get(\"/stock-items\").accept(\"application/json\")) .andExpect(content().json(\"[]\")); } } } java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.ArrayList; import java.util.List; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class StockItemController { @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List listStockItems() { return new ArrayList(); } } ``` Start the local server ./gradlew bootRun Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". - Selecting \"Open Preview\" opens a window inside gitpod workspace tab. - Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Code Ready Workspaces Click on yes Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop When the server starts, open a browser to http://localhost:9080/swagger-ui.html to view the swagger documentation. You should see the stock item entry in the list Commit and push the changes to Git. git add . git commit -m \"Adds StockItemController\" git push Add a service for providing results \u00b6 An established pattern for REST services in Spring Boot is to keep the REST controller logic simple and focused on translating from REST protocols to Javascript. The business logic for the components should be placed in a component that is given a @Service annotation. Update the controller test to include returning data from the service ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import static org.mockito.Mockito.mock; import static org.mockito.Mockito.spy; import static org.mockito.Mockito.when; import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; import java.util.Arrays; import java.util.List; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.api.Nested; import org.junit.jupiter.api.Test; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @DisplayName(\"StockItemController\") public class StockItemControllerTest { StockItemController controller; StockItemApi service; MockMvc mockMvc; @BeforeEach public void setup() { service = mock(StockItemApi.class); controller = spy(new StockItemController(service)); mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); } @Nested @DisplayName(\"Given [GET] /stock-items\") public class GivenGetStockItems { @Test @DisplayName(\"When called then it should return a 200 status\") public void when_called_should_return_200_status() throws Exception { mockMvc.perform(get(\"/stock-items\")) .andExpect(status().isOk()); } @Test @DisplayName(\"When called then it should return an empty array\") public void when_called_then_return_an_empty_array() throws Exception { mockMvc.perform(get(\"/stock-items\").accept(\"application/json\")) .andExpect(content().json(\"[]\")); } @Test @DisplayName(\"When called then it should return the results of the StockItemService\") public void when_called_then_return_the_results_of_the_stockitemservice() throws Exception { final List<StockItem> expectedResult = Arrays.asList(new StockItem()); when(service.listStockItems()).thenReturn(expectedResult); mockMvc.perform(get(\"/stock-items\").accept(\"application/json\")) .andExpect(content().json(\"[{}]\")); } } } java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import java.io.Serializable; public class StockItem implements Serializable { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } } ``` ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems(); } ```java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.List; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @RestController public class StockItemController { private final StockItemApi service; public StockItemController(StockItemApi service) { this.service = service; } @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List<StockItem> listStockItems() { return this.service.listStockItems(); } } At this points the tests should pass even though we haven't provided an implementation of the service yet since we are creating a mocking the service in the unit test Update the StockItem model to include the remaining fields ```java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import java.io.Serializable; public class StockItem implements Serializable { private String name; private String id = null; private int stock = 0; private double price = 0.0; private String manufacturer = \"\"; public StockItem() { super(); } public StockItem(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } public String getId() { return id; } public void setId(String id) { this.id = id; } public StockItem withId(String id) { this.setId(id); return this; } public int getStock() { return stock; } public void setStock(int stock) { this.stock = stock; } public StockItem withStock(int stock) { this.setStock(stock); return this; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } public StockItem withPrice(double price) { this.setPrice(price); return this; } public String getManufacturer() { return manufacturer; } public void setManufacturer(String manufacturer) { this.manufacturer = manufacturer; } public StockItem withManufacturer(String manufacturer) { this.setManufacturer(manufacturer); return this; } } ``` Provide an implementation of the service that just returns a couple of hard-coded data values, for now. Services are denoted in Spring Boot with the @Service annotation ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import static java.util.Arrays.asList; import java.util.List; import org.springframework.context.annotation.Primary; import org.springframework.stereotype.Service; import com.ibm.inventory_management.models.StockItem; @Service public class StockItemService implements StockItemApi { @Override public List listStockItems() { return asList( new StockItem(\"1\") .withName(\"Item 1\") .withStock(100) .withPrice(10.5) .withManufacturer(\"Sony\"), new StockItem(\"2\") .withName(\"Item 2\") .withStock(150) .withPrice(100.0) .withManufacturer(\"Insignia\"), new StockItem(\"3\") .withName(\"Item 3\") .withStock(10) .withPrice(1000.0) .withManufacturer(\"Panasonic\") ); } } ``` Replace the api() method in the SwaggerDocket class to restrict the swagger page to only show the /stock-items API java title=\"src/main/java/com/ibm/cloud_native_toolkit/swagger/SwaggerDocket.java\" @Bean public Docket api() { return new Docket(DocumentationType.SWAGGER_2) .select() .apis(buildApiRequestHandler()::test) .paths(PathSelectors.regex(\".*stock-item.*\")) .build() .apiInfo(buildApiInfo()); } Verify the service locally and push the changes \u00b6 Start the application ./gradlew bootRun Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Code Ready Workspaces Click on yes Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html Run the service by selecting Try it out then Execute You should see the data we defined in the service in the previous section Commit and push the changes to git git add . git commit -m \"Adds StockItem service implementation\" git push The pipeline should kick off and you will be able to see the running service by running oc endpoints -n dev-{initials} and selecting the route of your service Complete CRUD operations \u00b6 Add POST, PUT and DELETE routes \u00b6 Update the StockItemApi.java interface to support the other CRUD operations ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems(); void updateStockItem(String id); void addStockItem(String id); void deleteStockItem(String id); } - Update the `StockItemService.java` class to implement the methods of the interface java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import static java.util.Arrays.asList; import java.util.ArrayList; import java.util.List; import java.util.stream.Collectors; import org.springframework.stereotype.Service; import com.ibm.inventory_management.models.StockItem; @Service public class StockItemService implements StockItemApi { static int id = 0; static List stockItems = new ArrayList<>(asList( new StockItem(++id+\"\") .withName(\"Item 1\") .withStock(100) .withPrice(10.5) .withManufacturer(\"Sony\"), new StockItem(++id+\"\") .withName(\"Item 2\") .withStock(150) .withPrice(100.5) .withManufacturer(\"Insignia\"), new StockItem(++id+\"\") .withName(\"Item 3\") .withStock(10) .withPrice(1000.0) .withManufacturer(\"Panasonic\") )); @Override public List<StockItem> listStockItems() { return this.stockItems; } @Override public void addStockItem(String name, String manufacturer, double price, int stock) { this.stockItems.add(new StockItem(++id+\"\") .withName(name) .withStock(stock) .withPrice(price) .withManufacturer(manufacturer) ); } @Override public void updateStockItem(String id, String name, String manufacturer, double price, int stock) { StockItem itemToUpdate = this.stockItems.stream().filter(stockItem -> stockItem.getId().equals(id)).findFirst().orElse(null); if(itemToUpdate == null) { System.out.println(\"Item not found\"); return; } itemToUpdate.setName(name !=null ? name : itemToUpdate.getName()); itemToUpdate.setManufacturer(manufacturer != null ? manufacturer : itemToUpdate.getManufacturer()); itemToUpdate.setPrice(Double.valueOf(price) != null ? price : itemToUpdate.getPrice()); itemToUpdate.setStock(Integer.valueOf(stock) != null ? stock : itemToUpdate.getStock()); } @Override public void deleteStockItem(String id) { this.stockItems = this.stockItems.stream().filter((stockItem)-> !stockItem.getId().equals(id)).collect(Collectors.toList()); } } - Update the `StockItemController.java` class to provide the additional routes java title=\"src/main/java/com/ibm/inventory_management/services/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.List; import org.springframework.web.bind.annotation.*; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @RestController public class StockItemController { private final StockItemApi service; public StockItemController(StockItemApi service) { this.service = service; } @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List<StockItem> listStockItems() { return this.service.listStockItems(); } @PostMapping(path = \"/stock-item\") public void addStockItem(@RequestParam String name, @RequestParam String manufacturer, @RequestParam float price, @RequestParam int stock) { this.service.addStockItem(name,manufacturer,price,stock); } @PutMapping(path = \"/stock-item/{id}\") public void updateStockItem(@PathVariable(\"id\") String id, @RequestParam String name, @RequestParam String manufacturer, @RequestParam float price, @RequestParam int stock) { this.service.updateStockItem(id,name,manufacturer,price,stock); } @DeleteMapping(path = \"/stock-item/{id}\") public void deleteStockItem(@PathVariable(\"id\") String id){ this.service.deleteStockItem(id); } } ``` Verify the changes locally and push the changes \u00b6 Start the application ./gradlew bootRun You should see new routes on the Swagger UI. Commit and push the changes to Git to trigger build pipeline on your OpenShift cluster. git add . git commit -m \"Added CRUD operations\" git push","title":"Service"},{"location":"developer-intermediate/inventory-service/#setup","text":"","title":"Setup"},{"location":"developer-intermediate/inventory-service/#setup-your-cloud-shell","text":"Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window. We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | sh - Note : If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.6.1/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli The installer updates PATH in the .zshrc or .bashrc file. You will need to source the file to apply the update to the current shell environment: if [[ \" ${ SHELL } \" = ~ zsh ]] ; then source ~/.zshrc else source ~/.bashrc fi You can check the shell was installed correctly by checking the oc version: oc sync --version Log in to OpenShift Cluster from the cloud console.Go to Resource List and click on the cluster: Access the OpenShift console from within that console by clicking on the button. In OpenShift Console, click on email address top right, Click on Copy Login Command and get the OpenShift login command, which includes a token. click on Display Token, copy the Login with the token. oc login command will log you in. Run the login command in the cloud shell terminal: $ oc login --token = qvARHflZDlOYfjJZRJUEs53Yfy4F8aa6_L3ezoagQFM --server = https://c103-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" . Create the initial project and register it with a pipeline for automated builds. Create a new repository from the Spring Boot Microservice template. Make the cloned repository public. You can also access this template on the Code Patterns page in the Developer Dashboard . Warning If you are developing on a shared education cluster, place the repository in the Git Organization listed in your notification email and remember to add your initials as a suffix to the app name. In order to prevent naming collisions, name the repository inventory-management-svc-{your initials} , replacing {your initials} with your actual initials. Clone the new repository to your machine git clone https://github.com/ibm-workshop-team-one/inventory-svc-{your initials}.git Go into the repository directory cloned and execute the following oc sync dev-{your initials} Register the pipeline register the pipeline oc pipeline --tekton replacing {your initials} with your actual initials Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select ibm-java-gradle $ oc pipeline --tekton Creating pipeline on openshift cluster in dev-ar namespace Retrieving git parameters Project git repo: https://github.com/aminerachyd/inventory-management-svc-ar.git ? Provide the git username: aminerachyd ? Provide the git password or personal access token: [ hidden ] Branch: main Retrieving available template pipelines from tools Pipeline templates filtered based on detected runtime: openjdk/gradle ? Select the Pipeline to use in the PipelineRun: ibm-java-gradle ? scan-image: Enable the pipeline to scan the image for vulnerabilities? Yes ? health-endpoint: Endpoint to check health after deployment, liberty uses / not /health? /health ? lint-dockerfile: Enable the pipeline to lint the Dockerfile for best practices? Yes Copying tasks from tools.... Copied Pipeline from tools/ibm-java-gradle to dev-ar/inventory-management-svc-ar Creating TriggerTemplate for pipeline: inventory-management-svc-ar Creating TriggerBinding for pipeline: inventory-management-svc-ar Creating/updating TriggerEventListener for pipeline: tekton Waiting for event listener rollout: dev-ar/el-tekton Creating/updating Route for pipeline: tekton Creating PipelineRun for pipeline: inventory-management-svc-ar Creating Github webhook for repo: https://github.com/aminerachyd/inventory-management-svc-ar.git Warning: Webhook already exists for this trigger in this repository. Pipeline run started: inventory-management-svc-ar-181f77c24a4 Open the pipeline to see it running When the pipeline is completed, run oc endpoints -n dev-{your initials} . You should see an entry for the app we just pushed. Select the entry and hit Enter to launch the browser. If you are developing on code ready workspaces/cloud shell, copy the url and paste it in a new browser window. Run the service locally ./gradlew bootRun When the execution output says Server started , the app is running. Access the running service. This service runs on port 9080. Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Code Ready Workspaces Click on yes Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html This will display the Swagger UI page that provides a user interface to exercise the APIs.","title":"Setup your cloud shell"},{"location":"developer-intermediate/inventory-service/#create-initial-components","text":"Spring Boot uses annotations to configure the various components that will be injected into and used by the applications. A class with the @SpringBootApplication annotation is the starting point for the rest of the application components to be loaded. Additionally, a @ComponentScan annotation can be added to tell the Spring infrastructure which packages should be scanned for components. We will start by creating the initial application component. Create a class named Application in the com.ibm.inventory_management.app package. Add the @SpringBootApplication and @ComponentScan annotation to the class. The @ComponentScan annotation should include com.ibm.inventory_management.* , com.ibm.cloud_native_toolkit.* , and com.ibm.health packages. ```java title=\"src/main/java/com/ibm/inventory_management/app/Application.java\" package com.ibm.inventory_management.app; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.CommandLineRunner; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.builder.SpringApplicationBuilder; import org.springframework.boot.web.servlet.support.SpringBootServletInitializer; import org.springframework.context.ApplicationContext; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.core.env.Environment; import springfox.documentation.swagger2.annotations.EnableSwagger2 @SpringBootApplication @EnableSwagger2 @ComponentScan({\"com.ibm.inventory_management. \", \"com.ibm.cloud_native_toolkit. \", \"com.ibm.health\"}) public class Application extends SpringBootServletInitializer { @Autowired Environment environment; public static void main(String[] args) { SpringApplication.run(com.ibm.inventory_management.app.Application.class, args); } @Bean public CommandLineRunner commandLineRunner(ApplicationContext ctx) { return args -> { String port = environment.getProperty(\"local.server.port\"); System.out.println(); System.out.println(\"Server started - http://localhost:\" + port + \"/swagger-ui.html\"); }; } @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(Application.class); } } ``` Delete application.app git rm -r src/main/java/application/ Run the service locally. The swagger page should no longer contain the /hello API endpoint. Commit and push the changes to Git. git add . git commit -m \"Adds Application and Removes default Application class\" git push","title":"Create initial components"},{"location":"developer-intermediate/inventory-service/#add-stockitem-controller","text":"In Spring Boot, the @RestController annotation tells the framework that the class provides a REST interface. Additional annotations like @GetMapping are used to provide the specific configuration for the REST service. Start the tests in tdd mode with npm run tdd (or ./gradlew test --continuous ) Add a StockItemControllerTest.java in com.ibm.inventory_management.controllers under the test folder ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import org.junit.jupiter.api.DisplayName; @DisplayName(\"StockItemController\") public class StockItemControllerTest { } ``` Add the MockMvc infrastructure and create the StockItemController ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import static org.mockito.Mockito.spy; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.DisplayName; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; @DisplayName(\"StockItemController\") public class StockItemControllerTest { StockItemController controller; MockMvc mockMvc; @BeforeEach public void setup() { controller = spy(new StockItemController()); mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); } } java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; public class StockItemController { } ``` Add the tests for the controller behavior and make the corresponding changes to make the tests pass ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import static org.mockito.Mockito.spy; import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.api.Nested; import org.junit.jupiter.api.Test; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; @DisplayName(\"StockItemController\") public class StockItemControllerTest { StockItemController controller; MockMvc mockMvc; @BeforeEach public void setup() { controller = spy(new StockItemController()); mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); } @Nested @DisplayName(\"Given [GET] /stock-items\") public class GivenGetStockItems { @Test @DisplayName(\"When called then it should return a 200 status\") public void when_called_should_return_200_status() throws Exception { mockMvc.perform(get(\"/stock-items\")) .andExpect(status().isOk()); } @Test @DisplayName(\"When called then it should return an empty array\") public void when_called_then_return_an_empty_array() throws Exception { mockMvc.perform(get(\"/stock-items\").accept(\"application/json\")) .andExpect(content().json(\"[]\")); } } } java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.ArrayList; import java.util.List; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class StockItemController { @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List listStockItems() { return new ArrayList(); } } ``` Start the local server ./gradlew bootRun Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". - Selecting \"Open Preview\" opens a window inside gitpod workspace tab. - Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Code Ready Workspaces Click on yes Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop When the server starts, open a browser to http://localhost:9080/swagger-ui.html to view the swagger documentation. You should see the stock item entry in the list Commit and push the changes to Git. git add . git commit -m \"Adds StockItemController\" git push","title":"Add StockItem controller"},{"location":"developer-intermediate/inventory-service/#add-a-service-for-providing-results","text":"An established pattern for REST services in Spring Boot is to keep the REST controller logic simple and focused on translating from REST protocols to Javascript. The business logic for the components should be placed in a component that is given a @Service annotation. Update the controller test to include returning data from the service ```java title=\"src/test/java/com/ibm/inventory_management/controllers/StockItemControllerTest.java\" package com.ibm.inventory_management.controllers; import static org.mockito.Mockito.mock; import static org.mockito.Mockito.spy; import static org.mockito.Mockito.when; import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.content; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; import java.util.Arrays; import java.util.List; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.api.Nested; import org.junit.jupiter.api.Test; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @DisplayName(\"StockItemController\") public class StockItemControllerTest { StockItemController controller; StockItemApi service; MockMvc mockMvc; @BeforeEach public void setup() { service = mock(StockItemApi.class); controller = spy(new StockItemController(service)); mockMvc = MockMvcBuilders.standaloneSetup(controller).build(); } @Nested @DisplayName(\"Given [GET] /stock-items\") public class GivenGetStockItems { @Test @DisplayName(\"When called then it should return a 200 status\") public void when_called_should_return_200_status() throws Exception { mockMvc.perform(get(\"/stock-items\")) .andExpect(status().isOk()); } @Test @DisplayName(\"When called then it should return an empty array\") public void when_called_then_return_an_empty_array() throws Exception { mockMvc.perform(get(\"/stock-items\").accept(\"application/json\")) .andExpect(content().json(\"[]\")); } @Test @DisplayName(\"When called then it should return the results of the StockItemService\") public void when_called_then_return_the_results_of_the_stockitemservice() throws Exception { final List<StockItem> expectedResult = Arrays.asList(new StockItem()); when(service.listStockItems()).thenReturn(expectedResult); mockMvc.perform(get(\"/stock-items\").accept(\"application/json\")) .andExpect(content().json(\"[{}]\")); } } } java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import java.io.Serializable; public class StockItem implements Serializable { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } } ``` ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems(); } ```java title=\"src/main/java/com/ibm/inventory_management/controllers/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.List; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @RestController public class StockItemController { private final StockItemApi service; public StockItemController(StockItemApi service) { this.service = service; } @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List<StockItem> listStockItems() { return this.service.listStockItems(); } } At this points the tests should pass even though we haven't provided an implementation of the service yet since we are creating a mocking the service in the unit test Update the StockItem model to include the remaining fields ```java title=\"src/main/java/com/ibm/inventory_management/models/StockItem.java\" package com.ibm.inventory_management.models; import java.io.Serializable; public class StockItem implements Serializable { private String name; private String id = null; private int stock = 0; private double price = 0.0; private String manufacturer = \"\"; public StockItem() { super(); } public StockItem(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public StockItem withName(String name) { this.setName(name); return this; } public String getId() { return id; } public void setId(String id) { this.id = id; } public StockItem withId(String id) { this.setId(id); return this; } public int getStock() { return stock; } public void setStock(int stock) { this.stock = stock; } public StockItem withStock(int stock) { this.setStock(stock); return this; } public double getPrice() { return price; } public void setPrice(double price) { this.price = price; } public StockItem withPrice(double price) { this.setPrice(price); return this; } public String getManufacturer() { return manufacturer; } public void setManufacturer(String manufacturer) { this.manufacturer = manufacturer; } public StockItem withManufacturer(String manufacturer) { this.setManufacturer(manufacturer); return this; } } ``` Provide an implementation of the service that just returns a couple of hard-coded data values, for now. Services are denoted in Spring Boot with the @Service annotation ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import static java.util.Arrays.asList; import java.util.List; import org.springframework.context.annotation.Primary; import org.springframework.stereotype.Service; import com.ibm.inventory_management.models.StockItem; @Service public class StockItemService implements StockItemApi { @Override public List listStockItems() { return asList( new StockItem(\"1\") .withName(\"Item 1\") .withStock(100) .withPrice(10.5) .withManufacturer(\"Sony\"), new StockItem(\"2\") .withName(\"Item 2\") .withStock(150) .withPrice(100.0) .withManufacturer(\"Insignia\"), new StockItem(\"3\") .withName(\"Item 3\") .withStock(10) .withPrice(1000.0) .withManufacturer(\"Panasonic\") ); } } ``` Replace the api() method in the SwaggerDocket class to restrict the swagger page to only show the /stock-items API java title=\"src/main/java/com/ibm/cloud_native_toolkit/swagger/SwaggerDocket.java\" @Bean public Docket api() { return new Docket(DocumentationType.SWAGGER_2) .select() .apis(buildApiRequestHandler()::test) .paths(PathSelectors.regex(\".*stock-item.*\")) .build() .apiInfo(buildApiInfo()); }","title":"Add a service for providing results"},{"location":"developer-intermediate/inventory-service/#verify-the-service-locally-and-push-the-changes","text":"Start the application ./gradlew bootRun Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 9080 this will open a browser tab and display the running app on that port. Gitpod Once you run the application,gitpod gives the option to make the port \"Public\".Once you make the port Public, it gives you the option to \"Open Preview\" or \"Open Browser\". Selecting \"Open Preview\" opens a window inside gitpod workspace tab. Selecting \"Open Browser\" opens a new browser tab for accessing the URL. Code Ready Workspaces Click on yes Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:9080/swagger-ui.html Run the service by selecting Try it out then Execute You should see the data we defined in the service in the previous section Commit and push the changes to git git add . git commit -m \"Adds StockItem service implementation\" git push The pipeline should kick off and you will be able to see the running service by running oc endpoints -n dev-{initials} and selecting the route of your service","title":"Verify the service locally and push the changes"},{"location":"developer-intermediate/inventory-service/#complete-crud-operations","text":"","title":"Complete CRUD operations"},{"location":"developer-intermediate/inventory-service/#add-post-put-and-delete-routes","text":"Update the StockItemApi.java interface to support the other CRUD operations ```java title=\"src/main/java/com/ibm/inventory_management/services/StockItemApi.java\" package com.ibm.inventory_management.services; import java.util.List; import com.ibm.inventory_management.models.StockItem; public interface StockItemApi { List listStockItems(); void updateStockItem(String id); void addStockItem(String id); void deleteStockItem(String id); } - Update the `StockItemService.java` class to implement the methods of the interface java title=\"src/main/java/com/ibm/inventory_management/services/StockItemService.java\" package com.ibm.inventory_management.services; import static java.util.Arrays.asList; import java.util.ArrayList; import java.util.List; import java.util.stream.Collectors; import org.springframework.stereotype.Service; import com.ibm.inventory_management.models.StockItem; @Service public class StockItemService implements StockItemApi { static int id = 0; static List stockItems = new ArrayList<>(asList( new StockItem(++id+\"\") .withName(\"Item 1\") .withStock(100) .withPrice(10.5) .withManufacturer(\"Sony\"), new StockItem(++id+\"\") .withName(\"Item 2\") .withStock(150) .withPrice(100.5) .withManufacturer(\"Insignia\"), new StockItem(++id+\"\") .withName(\"Item 3\") .withStock(10) .withPrice(1000.0) .withManufacturer(\"Panasonic\") )); @Override public List<StockItem> listStockItems() { return this.stockItems; } @Override public void addStockItem(String name, String manufacturer, double price, int stock) { this.stockItems.add(new StockItem(++id+\"\") .withName(name) .withStock(stock) .withPrice(price) .withManufacturer(manufacturer) ); } @Override public void updateStockItem(String id, String name, String manufacturer, double price, int stock) { StockItem itemToUpdate = this.stockItems.stream().filter(stockItem -> stockItem.getId().equals(id)).findFirst().orElse(null); if(itemToUpdate == null) { System.out.println(\"Item not found\"); return; } itemToUpdate.setName(name !=null ? name : itemToUpdate.getName()); itemToUpdate.setManufacturer(manufacturer != null ? manufacturer : itemToUpdate.getManufacturer()); itemToUpdate.setPrice(Double.valueOf(price) != null ? price : itemToUpdate.getPrice()); itemToUpdate.setStock(Integer.valueOf(stock) != null ? stock : itemToUpdate.getStock()); } @Override public void deleteStockItem(String id) { this.stockItems = this.stockItems.stream().filter((stockItem)-> !stockItem.getId().equals(id)).collect(Collectors.toList()); } } - Update the `StockItemController.java` class to provide the additional routes java title=\"src/main/java/com/ibm/inventory_management/services/StockItemController.java\" package com.ibm.inventory_management.controllers; import java.util.List; import org.springframework.web.bind.annotation.*; import com.ibm.inventory_management.models.StockItem; import com.ibm.inventory_management.services.StockItemApi; @RestController public class StockItemController { private final StockItemApi service; public StockItemController(StockItemApi service) { this.service = service; } @GetMapping(path = \"/stock-items\", produces = \"application/json\") public List<StockItem> listStockItems() { return this.service.listStockItems(); } @PostMapping(path = \"/stock-item\") public void addStockItem(@RequestParam String name, @RequestParam String manufacturer, @RequestParam float price, @RequestParam int stock) { this.service.addStockItem(name,manufacturer,price,stock); } @PutMapping(path = \"/stock-item/{id}\") public void updateStockItem(@PathVariable(\"id\") String id, @RequestParam String name, @RequestParam String manufacturer, @RequestParam float price, @RequestParam int stock) { this.service.updateStockItem(id,name,manufacturer,price,stock); } @DeleteMapping(path = \"/stock-item/{id}\") public void deleteStockItem(@PathVariable(\"id\") String id){ this.service.deleteStockItem(id); } } ```","title":"Add POST, PUT and DELETE routes"},{"location":"developer-intermediate/inventory-service/#verify-the-changes-locally-and-push-the-changes","text":"Start the application ./gradlew bootRun You should see new routes on the Swagger UI. Commit and push the changes to Git to trigger build pipeline on your OpenShift cluster. git add . git commit -m \"Added CRUD operations\" git push","title":"Verify the changes locally and push the changes"},{"location":"developer-intermediate/inventory-ui/","text":"Develop and deploy the UI component of the inventory application Setup \u00b6 [Optionnal]: Access cloud shell \u00b6 If you don't plan to use your workstation to run this lab, you can use IBM Cloud Shell: Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window. Setup your shell environment \u00b6 We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | sh - Note : If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.6.1/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli The installer updates PATH in the .zshrc or .bashrc file. You will need to source the file to apply the update to the current shell environment: if [[ \" ${ SHELL } \" = ~ zsh ]] ; then source ~/.zshrc else source ~/.bashrc fi You can check the shell was installed correctly by checking the oc version: oc sync --version Log in to OpenShift Cluster \u00b6 Log in to OpenShift Cluster from the cloud console. Go to Resource listStockItems and click on the cluster: Access the OpenShift console from within that console by clicking on the button. In OpenShift Console, click on email address top right, Click on Copy Login Command and get the OpenShift login command, which includes a token. click on Display Token , copy the Login with the token. oc login command will log you in. Run the login command in the cloud shell terminal: $ oc login --token = qvARHflZDlOYfjJZRJUEs53Yfy4F8aa6_L3ezoagQFM --server = https://c103-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" . Setup code base \u00b6 Get the initial project created and register the pipeline for automated builds. Detailed instructions for each of these steps can be found in the Deploying an App guide. Create a new repository from the Carbon React template into your Git org. Warning If you are developing on a shared education cluster, place the repository in the Git Organization listed in your notification email and remember to add your initials as a suffix to the app name. - In order to prevent naming collisions, name the repository inventory-management-bff-{your initials} replacing {your initials} with your actual initials. Clone the new repository to your machine. Run the following commands to install the project dependencies: npm install -g yarn yarn install Run the application locally: yarn start:dev Go into the repository directory cloned and execute the following: oc sync dev-{your initials} Register the pipeline: oc pipeline --tekton replacing {your initials} with your actual initials Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-<VERSION> . Open the pipeline to see it running, using the link provided in the command output. Create the initial components \u00b6 Based on the requirements of this first use case, we will create a StockItemList component to list stock items. Open a terminal and start the application in development mode to see the initial UI and the changes as we make them: yarn start:dev Access the running service. This service runs by default on port 3000 . Create the StockItemList React component that uses a StructuredList from the Carbon React Components portfolio: ```javascript title=\"src/content/StockItemList.jsx\" import React from \"react\"; import { StructuredListWrapper, StructuredListHead, StructuredListRow, StructuredListCell, StructuredListBody } from '@carbon/react'; const DEFAULT_ITEMS = [ { name: 'Item 1', stock: 10, unitPrice: 51.2, manufacturer: 'Sony' }, { name: 'Item 2', stock: 50, unitPrice: 10, manufacturer: 'LG' }, ] export default function StockItemList() { const items = DEFAULT_ITEMS; return ( <div className=\"stock-items-list\"> <h2>Stock Items</h2> <StructuredListWrapper> <StructuredListHead> <StructuredListRow head> <StructuredListCell head>Name</StructuredListCell> <StructuredListCell head>Stock</StructuredListCell> <StructuredListCell head>Unit Price</StructuredListCell> <StructuredListCell head>Manufacturer</StructuredListCell> </StructuredListRow> </StructuredListHead> <StructuredListBody> {items.map(item => ( <StructuredListRow> <StructuredListCell noWrap>{item.name}</StructuredListCell> <StructuredListCell noWrap>{item.stock}</StructuredListCell> <StructuredListCell noWrap>{item.unitPrice}</StructuredListCell> <StructuredListCell noWrap>{item.manufacturer}</StructuredListCell> </StructuredListRow> ))} </StructuredListBody> </StructuredListWrapper> </div> ); } ``` Now that we have our component to list stock items, let's add it to out app by editing the src/content/UIShell/UIShell.jsx file: Add our new component to the bottom of the imports section: javascript title=\"src/content/UIShell/UIShell.jsx\" ... import StockItemList from \"../StockItemList\"; ... Add a menu to our left navigation panel to link to a new /inventory/items route that we'll use to list stock items: ```javascript title=\"src/content/UIShell/UIShell.jsx\" ... { this.setState({ activeItem: '/' }) }}> Overview { this.setState({ activeItem: '/inventory/items' }) }}> Items Link Link Link Link Link ... - Add a new route for the `/inventory/items` route: ```javascript title=\"src/content/UIShell/UIShell.jsx\" ... <Routes> <Route path=\"/\" element={<LandingPage />} /> <Route path=\"/inventory/items\" element={<StockItemList />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ... Open the application to check that you can now navigate to the Stock Items view: With the application running in the first terminal, open a second terminal in the repository directory and push the changes we've just made: git add . git commit -m \"Initial shell components\" git push On the openshift console, open the pipeline to see it running . Add a service component to get mock Stock Items \u00b6 Now that we've created the initial components, we can start to customize the StockItemList to match the data for our application. So far, we've built a UI that displays a hard-coded set of data in a table. Eventually, we want to display dynamic data provided from a database in the table. As a first step towards that goal, we need to separate the UI logic from the logic that retrieves the data. We will do that with a service component. For this first pass the service component will just return mock data. Create a src/services : mkdir src/services Create a file named stock-item-mock.service.js in the service directory, implementing the service by copying the data array from StockItemList and returning it in the function. You can add a setTimeout() 1s timeout to simulate loading: javascript title=\"src/services/stock-item-mock.service.js\" export class StockItemMockService { async listStockItems() { return new Promise(resolve => { // Wait 1 second before returning data setTimeout(() => { resolve([ { id: 1, name: 'Item 1', stock: 10, unitPrice: 51.2, manufacturer: 'Sony' }, { id: 2, name: 'Item 2', stock: 50, unitPrice: 10, manufacturer: 'LG' }, ]); }, 1000) }); } } Update the components to pass the service in the properties: ```javascript title=\"src/App.test.jsx\" import { render, screen } from '@testing-library/react'; import App from './App'; import {StockItemMockService} from \"./services/stock-item-mock.service\"; describe('App', () => { test('canary verifies test infrastructure', () => { expect(true).toEqual(true); }); test('renders text', () => { Object.defineProperty(window, \"matchMedia\", { writable: true, value: jest.fn().mockImplementation(query => ({ matches: false, media: query, onchange: null, addListener: jest.fn(), // Deprecated removeListener: jest.fn(), // Deprecated addEventListener: jest.fn(), removeEventListener: jest.fn(), dispatchEvent: jest.fn(), })) }); render( ); const linkElement = screen.getByText(/Design & build with Carbon/i); expect(linkElement).toBeInTheDocument(); }); }); ```javascript title=\"src/App.jsx\" import React, { Component } from 'react'; import UIShell from './content/UIShell/UIShell'; import './App.scss'; import { StockItemMockService } from \"./services/stock-item-mock.service\"; class App extends Component { constructor(props) { super(props); this.stockService = props.stockService || new StockItemMockService(); } render() { return ( <div className=\"app\"> <UIShell stockService={this.stockService} /> </div> ); } } export default App; javascript title=\"src/content/UIShell/UIShell.jsx\" ... <Routes> <Route path=\"/\" element={<LandingPage />} /> <Route path=\"/inventory/items\" element={<StockItemList stockService={this.props.stockService} />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ... Update StockItemList.jsx to use the provided service: ```javascript title=\"src/content/StockItemList.jsx\" import React from 'react'; import { useQuery } from '@tanstack/react-query'; import { StructuredListWrapper, StructuredListHead, StructuredListRow, StructuredListCell, StructuredListBody, StructuredListSkeleton } from '@carbon/react'; export default function StockItemList(props) { const { isLoading, error, data } = useQuery(['stock-items'], props.stockService.listStockItems); return ( <div className='stock-items-list'> <h2>Stock Items</h2> {isLoading ? <StructuredListSkeleton /> : error ? 'Error retrieving stock items' : <StructuredListWrapper> <StructuredListHead> <StructuredListRow head> <StructuredListCell head>Name</StructuredListCell> <StructuredListCell head>Stock</StructuredListCell> <StructuredListCell head>Unit Price</StructuredListCell> <StructuredListCell head>Manufacturer</StructuredListCell> </StructuredListRow> </StructuredListHead> <StructuredListBody> {data.map(item => ( <StructuredListRow key={item.id}> <StructuredListCell noWrap>{item.name}</StructuredListCell> <StructuredListCell noWrap>{item.stock}</StructuredListCell> <StructuredListCell noWrap>{item.unitPrice}</StructuredListCell> <StructuredListCell noWrap>{item.manufacturer}</StructuredListCell> </StructuredListRow> ))} </StructuredListBody> </StructuredListWrapper>} </div> ); } ``` Open the app in your browser, if the app isn't started run: yarn start:dev Push the changes we've made to the repository: git add . git commit -m \"Adds a mock service\" git push On the openshift console, open the pipeline to see it running . Add a service that calls the BFF \u00b6 Now that we have a mock service that injects data, we can build an implementation of the service that calls our BFF. For the service, we will use axios to make GraphQL calls to the BFF through an HTTP proxy exposed by the server, using http-proxy-middleware . Install axios and http-proxy-middleware : yarn add http-proxy-middleware axios Update the server to proxy BFF requests (configured in API_HOST environment variable) to /api endpoint: ```javascript title=\"server/server.js\" const express = require('express'); const path = require('path'); const { createProxyMiddleware } = require('http-proxy-middleware'); const app = express(); app.use(express.static(path.join(__dirname, '../build'))); app.use( '/api', createProxyMiddleware({ target: process.env.API_HOST, changeOrigin: true, pathRewrite: { '^/api': '/' }, }) ); app.get('/health', function (req, res) { res.json({ status: 'UP' }); }); app.get('/*', function (req, res) { res.sendFile(path.join(__dirname, '../build', 'index.html')); }); const port = process.env.PORT ?? 3000; app.listen(port, function () { console.info( Server listening on http://localhost:${port} ); }); ``` Add a src/setupProxy.js file to setup the proxy for local development: ```javascript title=\"src/setupProxy.js\" const { createProxyMiddleware } = require('http-proxy-middleware'); module.exports = function(app) { app.use( '/api', createProxyMiddleware({ target: process.env.API_HOST, changeOrigin: true, pathRewrite: { '^/api': '/' }, }) ); }; ``` Create a service implementation in the services directory called stock-item.service.js implementing listStockItems() that calls the BFF through the /api proxy: ```javascript title=\"src/services/stock-item.service.js\" import axios from \"axios\"; export class StockItemService { constructor(baseUrl) { this.baseUrl = baseUrl || '/api'; } async listStockItems() { return axios({ url: '/api/graphql', method: \"POST\", data: { query: ` { stockItems { id manufacturer name picture stock unitPrice } } ` } }).then(response => response.data.data.stockItems); } } ``` Update App.jsx to use the new service instead of the mock service: ```javascript title=\"src/App.jsx\" import React, { Component } from 'react'; import UIShell from './content/UIShell/UIShell'; import './App.scss'; import { StockItemService } from \"./services/stock-item.service\"; class App extends Component { constructor(props) { super(props); this.stockService = props.stockService || new StockItemService(); } render() { return ( ); } } export default App; ``` Open the application to check that your app is now retrieving data from BFF GraphQL endpoint: Modify connectsTo property to the values.yaml file of the Helm chart. The value of the property should match the Kubernetes service of the microservice. (For template projects, the service name is the same as the name of the application which is that same as the name of the repository): yaml title=\"chart/base/values.yaml\" ... connectsTo: inventory-management-bff-{your initials} ... Add a new environment variable named API_HOST to the list of existing environment variables in deployment.yaml. The value of this environment variable should come from the connectsTo value we defined. You can add | quote to wrap the value in quotes in case the value is not formatted correctly: yaml title=\"chart/base/templates/deployment.yaml\" ... env: - name: INGRESS_HOST value: \"\" - name: PROTOCOLS value: \"\" - name: LOG_LEVEL value: {{ .Values.logLevel | quote }} - name: API_HOST value: {{ printf \"%s:80\" .Values.connectsTo | quote } ... Push the changes we've made to the repository: git add . git commit -m \"Updates the StockItemsList view\" git push On the openshift console, open the pipeline to see it running . Summary \u00b6 Congrats! You have now completed the Micro App Guide demonstrating the Inventory solution.","title":"UI"},{"location":"developer-intermediate/inventory-ui/#setup","text":"","title":"Setup"},{"location":"developer-intermediate/inventory-ui/#optionnal-access-cloud-shell","text":"If you don't plan to use your workstation to run this lab, you can use IBM Cloud Shell: Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window.","title":"[Optionnal]: Access cloud shell"},{"location":"developer-intermediate/inventory-ui/#setup-your-shell-environment","text":"We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | sh - Note : If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.6.1/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli The installer updates PATH in the .zshrc or .bashrc file. You will need to source the file to apply the update to the current shell environment: if [[ \" ${ SHELL } \" = ~ zsh ]] ; then source ~/.zshrc else source ~/.bashrc fi You can check the shell was installed correctly by checking the oc version: oc sync --version","title":"Setup your shell environment"},{"location":"developer-intermediate/inventory-ui/#log-in-to-openshift-cluster","text":"Log in to OpenShift Cluster from the cloud console. Go to Resource listStockItems and click on the cluster: Access the OpenShift console from within that console by clicking on the button. In OpenShift Console, click on email address top right, Click on Copy Login Command and get the OpenShift login command, which includes a token. click on Display Token , copy the Login with the token. oc login command will log you in. Run the login command in the cloud shell terminal: $ oc login --token = qvARHflZDlOYfjJZRJUEs53Yfy4F8aa6_L3ezoagQFM --server = https://c103-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" .","title":"Log in to OpenShift Cluster"},{"location":"developer-intermediate/inventory-ui/#setup-code-base","text":"Get the initial project created and register the pipeline for automated builds. Detailed instructions for each of these steps can be found in the Deploying an App guide. Create a new repository from the Carbon React template into your Git org. Warning If you are developing on a shared education cluster, place the repository in the Git Organization listed in your notification email and remember to add your initials as a suffix to the app name. - In order to prevent naming collisions, name the repository inventory-management-bff-{your initials} replacing {your initials} with your actual initials. Clone the new repository to your machine. Run the following commands to install the project dependencies: npm install -g yarn yarn install Run the application locally: yarn start:dev Go into the repository directory cloned and execute the following: oc sync dev-{your initials} Register the pipeline: oc pipeline --tekton replacing {your initials} with your actual initials Give git credentials if prompted, and master as the git branch to use. When prompted for the pipeline, select igc-nodejs-<VERSION> . Open the pipeline to see it running, using the link provided in the command output.","title":"Setup code base"},{"location":"developer-intermediate/inventory-ui/#create-the-initial-components","text":"Based on the requirements of this first use case, we will create a StockItemList component to list stock items. Open a terminal and start the application in development mode to see the initial UI and the changes as we make them: yarn start:dev Access the running service. This service runs by default on port 3000 . Create the StockItemList React component that uses a StructuredList from the Carbon React Components portfolio: ```javascript title=\"src/content/StockItemList.jsx\" import React from \"react\"; import { StructuredListWrapper, StructuredListHead, StructuredListRow, StructuredListCell, StructuredListBody } from '@carbon/react'; const DEFAULT_ITEMS = [ { name: 'Item 1', stock: 10, unitPrice: 51.2, manufacturer: 'Sony' }, { name: 'Item 2', stock: 50, unitPrice: 10, manufacturer: 'LG' }, ] export default function StockItemList() { const items = DEFAULT_ITEMS; return ( <div className=\"stock-items-list\"> <h2>Stock Items</h2> <StructuredListWrapper> <StructuredListHead> <StructuredListRow head> <StructuredListCell head>Name</StructuredListCell> <StructuredListCell head>Stock</StructuredListCell> <StructuredListCell head>Unit Price</StructuredListCell> <StructuredListCell head>Manufacturer</StructuredListCell> </StructuredListRow> </StructuredListHead> <StructuredListBody> {items.map(item => ( <StructuredListRow> <StructuredListCell noWrap>{item.name}</StructuredListCell> <StructuredListCell noWrap>{item.stock}</StructuredListCell> <StructuredListCell noWrap>{item.unitPrice}</StructuredListCell> <StructuredListCell noWrap>{item.manufacturer}</StructuredListCell> </StructuredListRow> ))} </StructuredListBody> </StructuredListWrapper> </div> ); } ``` Now that we have our component to list stock items, let's add it to out app by editing the src/content/UIShell/UIShell.jsx file: Add our new component to the bottom of the imports section: javascript title=\"src/content/UIShell/UIShell.jsx\" ... import StockItemList from \"../StockItemList\"; ... Add a menu to our left navigation panel to link to a new /inventory/items route that we'll use to list stock items: ```javascript title=\"src/content/UIShell/UIShell.jsx\" ... { this.setState({ activeItem: '/' }) }}> Overview { this.setState({ activeItem: '/inventory/items' }) }}> Items Link Link Link Link Link ... - Add a new route for the `/inventory/items` route: ```javascript title=\"src/content/UIShell/UIShell.jsx\" ... <Routes> <Route path=\"/\" element={<LandingPage />} /> <Route path=\"/inventory/items\" element={<StockItemList />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ... Open the application to check that you can now navigate to the Stock Items view: With the application running in the first terminal, open a second terminal in the repository directory and push the changes we've just made: git add . git commit -m \"Initial shell components\" git push On the openshift console, open the pipeline to see it running .","title":"Create the initial components"},{"location":"developer-intermediate/inventory-ui/#add-a-service-component-to-get-mock-stock-items","text":"Now that we've created the initial components, we can start to customize the StockItemList to match the data for our application. So far, we've built a UI that displays a hard-coded set of data in a table. Eventually, we want to display dynamic data provided from a database in the table. As a first step towards that goal, we need to separate the UI logic from the logic that retrieves the data. We will do that with a service component. For this first pass the service component will just return mock data. Create a src/services : mkdir src/services Create a file named stock-item-mock.service.js in the service directory, implementing the service by copying the data array from StockItemList and returning it in the function. You can add a setTimeout() 1s timeout to simulate loading: javascript title=\"src/services/stock-item-mock.service.js\" export class StockItemMockService { async listStockItems() { return new Promise(resolve => { // Wait 1 second before returning data setTimeout(() => { resolve([ { id: 1, name: 'Item 1', stock: 10, unitPrice: 51.2, manufacturer: 'Sony' }, { id: 2, name: 'Item 2', stock: 50, unitPrice: 10, manufacturer: 'LG' }, ]); }, 1000) }); } } Update the components to pass the service in the properties: ```javascript title=\"src/App.test.jsx\" import { render, screen } from '@testing-library/react'; import App from './App'; import {StockItemMockService} from \"./services/stock-item-mock.service\"; describe('App', () => { test('canary verifies test infrastructure', () => { expect(true).toEqual(true); }); test('renders text', () => { Object.defineProperty(window, \"matchMedia\", { writable: true, value: jest.fn().mockImplementation(query => ({ matches: false, media: query, onchange: null, addListener: jest.fn(), // Deprecated removeListener: jest.fn(), // Deprecated addEventListener: jest.fn(), removeEventListener: jest.fn(), dispatchEvent: jest.fn(), })) }); render( ); const linkElement = screen.getByText(/Design & build with Carbon/i); expect(linkElement).toBeInTheDocument(); }); }); ```javascript title=\"src/App.jsx\" import React, { Component } from 'react'; import UIShell from './content/UIShell/UIShell'; import './App.scss'; import { StockItemMockService } from \"./services/stock-item-mock.service\"; class App extends Component { constructor(props) { super(props); this.stockService = props.stockService || new StockItemMockService(); } render() { return ( <div className=\"app\"> <UIShell stockService={this.stockService} /> </div> ); } } export default App; javascript title=\"src/content/UIShell/UIShell.jsx\" ... <Routes> <Route path=\"/\" element={<LandingPage />} /> <Route path=\"/inventory/items\" element={<StockItemList stockService={this.props.stockService} />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ... Update StockItemList.jsx to use the provided service: ```javascript title=\"src/content/StockItemList.jsx\" import React from 'react'; import { useQuery } from '@tanstack/react-query'; import { StructuredListWrapper, StructuredListHead, StructuredListRow, StructuredListCell, StructuredListBody, StructuredListSkeleton } from '@carbon/react'; export default function StockItemList(props) { const { isLoading, error, data } = useQuery(['stock-items'], props.stockService.listStockItems); return ( <div className='stock-items-list'> <h2>Stock Items</h2> {isLoading ? <StructuredListSkeleton /> : error ? 'Error retrieving stock items' : <StructuredListWrapper> <StructuredListHead> <StructuredListRow head> <StructuredListCell head>Name</StructuredListCell> <StructuredListCell head>Stock</StructuredListCell> <StructuredListCell head>Unit Price</StructuredListCell> <StructuredListCell head>Manufacturer</StructuredListCell> </StructuredListRow> </StructuredListHead> <StructuredListBody> {data.map(item => ( <StructuredListRow key={item.id}> <StructuredListCell noWrap>{item.name}</StructuredListCell> <StructuredListCell noWrap>{item.stock}</StructuredListCell> <StructuredListCell noWrap>{item.unitPrice}</StructuredListCell> <StructuredListCell noWrap>{item.manufacturer}</StructuredListCell> </StructuredListRow> ))} </StructuredListBody> </StructuredListWrapper>} </div> ); } ``` Open the app in your browser, if the app isn't started run: yarn start:dev Push the changes we've made to the repository: git add . git commit -m \"Adds a mock service\" git push On the openshift console, open the pipeline to see it running .","title":"Add a service component to get mock Stock Items"},{"location":"developer-intermediate/inventory-ui/#add-a-service-that-calls-the-bff","text":"Now that we have a mock service that injects data, we can build an implementation of the service that calls our BFF. For the service, we will use axios to make GraphQL calls to the BFF through an HTTP proxy exposed by the server, using http-proxy-middleware . Install axios and http-proxy-middleware : yarn add http-proxy-middleware axios Update the server to proxy BFF requests (configured in API_HOST environment variable) to /api endpoint: ```javascript title=\"server/server.js\" const express = require('express'); const path = require('path'); const { createProxyMiddleware } = require('http-proxy-middleware'); const app = express(); app.use(express.static(path.join(__dirname, '../build'))); app.use( '/api', createProxyMiddleware({ target: process.env.API_HOST, changeOrigin: true, pathRewrite: { '^/api': '/' }, }) ); app.get('/health', function (req, res) { res.json({ status: 'UP' }); }); app.get('/*', function (req, res) { res.sendFile(path.join(__dirname, '../build', 'index.html')); }); const port = process.env.PORT ?? 3000; app.listen(port, function () { console.info( Server listening on http://localhost:${port} ); }); ``` Add a src/setupProxy.js file to setup the proxy for local development: ```javascript title=\"src/setupProxy.js\" const { createProxyMiddleware } = require('http-proxy-middleware'); module.exports = function(app) { app.use( '/api', createProxyMiddleware({ target: process.env.API_HOST, changeOrigin: true, pathRewrite: { '^/api': '/' }, }) ); }; ``` Create a service implementation in the services directory called stock-item.service.js implementing listStockItems() that calls the BFF through the /api proxy: ```javascript title=\"src/services/stock-item.service.js\" import axios from \"axios\"; export class StockItemService { constructor(baseUrl) { this.baseUrl = baseUrl || '/api'; } async listStockItems() { return axios({ url: '/api/graphql', method: \"POST\", data: { query: ` { stockItems { id manufacturer name picture stock unitPrice } } ` } }).then(response => response.data.data.stockItems); } } ``` Update App.jsx to use the new service instead of the mock service: ```javascript title=\"src/App.jsx\" import React, { Component } from 'react'; import UIShell from './content/UIShell/UIShell'; import './App.scss'; import { StockItemService } from \"./services/stock-item.service\"; class App extends Component { constructor(props) { super(props); this.stockService = props.stockService || new StockItemService(); } render() { return ( ); } } export default App; ``` Open the application to check that your app is now retrieving data from BFF GraphQL endpoint: Modify connectsTo property to the values.yaml file of the Helm chart. The value of the property should match the Kubernetes service of the microservice. (For template projects, the service name is the same as the name of the application which is that same as the name of the repository): yaml title=\"chart/base/values.yaml\" ... connectsTo: inventory-management-bff-{your initials} ... Add a new environment variable named API_HOST to the list of existing environment variables in deployment.yaml. The value of this environment variable should come from the connectsTo value we defined. You can add | quote to wrap the value in quotes in case the value is not formatted correctly: yaml title=\"chart/base/templates/deployment.yaml\" ... env: - name: INGRESS_HOST value: \"\" - name: PROTOCOLS value: \"\" - name: LOG_LEVEL value: {{ .Values.logLevel | quote }} - name: API_HOST value: {{ printf \"%s:80\" .Values.connectsTo | quote } ... Push the changes we've made to the repository: git add . git commit -m \"Updates the StockItemsList view\" git push On the openshift console, open the pipeline to see it running .","title":"Add a service that calls the BFF"},{"location":"developer-intermediate/inventory-ui/#summary","text":"Congrats! You have now completed the Micro App Guide demonstrating the Inventory solution.","title":"Summary"},{"location":"developer-intermediate/log-management/","text":"Warning Note Only study this section if you are development on an OpenShift Development environment that is managed on IBM Cloud. If you are on AWS or Azure go to the OpenShift Monitoring content In IBM Garage Method, one of the Operate practices is to automate application monitoring , including logging. Imagine your application isn't working right in production even though the environment is fine. What information would you want in your logs to help you figure out what's wrong with your application? Build logging messages for that information into your application. Given that your application is logging, as are lots of other applications and services in your cloud environment, these logs need to be managed and made accessible. LogDNA adds log management capabilities to a Kubernetes cluster and its deployments. The includes an IBM Log Analysis with LogDNA service instance configured with a LogDNA agent installed in the environment's cluster. Simply by deploying your application into the , LogDNA collects the logs. Open the LogDNA web UI by navigating to the OpenShift web console and click on 9 squares icon. It gives a list of the developer tools. Select LogDNA and navigate to the UI as shown below LogDNA log management \u00b6 IBM Log Analysis with LogDNA explains how to configure and use an instance of the LogDNA service, but the has already done this for you. You can skip these steps about provisioning an instance, installing an agent, and user access. LogDNA dashboard \u00b6 Open the LogDNA web UI for your environment's cluster - IBM Log Analysis with LogDNA: Viewing logs explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Logging - Find the logging instance named after your environment's cluster, such as showcase-dev-iks-logdna . To help find it, you can filter by your resource group. - In the logging instance, press the View LogDNA button to open the LogDNA web UI Give it a try \u00b6 Before you begin to look at your application logs, make sure you have deployed an app into your development cluster. Each of the template apps has a logging framework included, this Give it a Try will use template-node-typescript as an example. The LogDNA service is already created, bound and configured to listen to logs from your development cluster. Open the LogDNA instance that is named the same as your development cluster Click on All Apps and enter the name of your app, or, to narrow the app selection, use the suffix that you used to create it Select the app you want to monitor You will now see just the logs for your app Open up the LogDNA console and narrow the logs to just your app Open the app into a browser: Run oc endpoints -n {your namespace} and select your running app Switch to HTTPS and test the /hello/{name} API and pass in some data. You have introduced a DEBUG message and an ERROR into this API logic. Click the Execute button a few times to simulate some API calls. Try the API with your own name, and you will see just the DEBUG message appearing Try the API with ERROR as the name , and this will trigger the error code we put in the API service You will see the errors appearing in LogDNA triggered by your application Try scaling your pods for the app and calling the API again. Watch the logs and see where the API call is being routed and which pod is triggering the error Learn more \u00b6 Learn more about using LogDNA: Conclusion \u00b6 It's important to be able to manage the logs of your deployed applications to help identify issues and quickly understand how to resolve them. The Developer Tools configures LogDNA directly to your development cluster to make it easy to get the log data you need as a developer. Just deploy your application into your and its logs start appearing automatically, open the LogDNA web UI and browse your logs.","title":"OpenShift"},{"location":"developer-intermediate/log-management/#logdna-log-management","text":"IBM Log Analysis with LogDNA explains how to configure and use an instance of the LogDNA service, but the has already done this for you. You can skip these steps about provisioning an instance, installing an agent, and user access.","title":"LogDNA log management"},{"location":"developer-intermediate/log-management/#logdna-dashboard","text":"Open the LogDNA web UI for your environment's cluster - IBM Log Analysis with LogDNA: Viewing logs explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Logging - Find the logging instance named after your environment's cluster, such as showcase-dev-iks-logdna . To help find it, you can filter by your resource group. - In the logging instance, press the View LogDNA button to open the LogDNA web UI","title":"LogDNA dashboard"},{"location":"developer-intermediate/log-management/#give-it-a-try","text":"Before you begin to look at your application logs, make sure you have deployed an app into your development cluster. Each of the template apps has a logging framework included, this Give it a Try will use template-node-typescript as an example. The LogDNA service is already created, bound and configured to listen to logs from your development cluster. Open the LogDNA instance that is named the same as your development cluster Click on All Apps and enter the name of your app, or, to narrow the app selection, use the suffix that you used to create it Select the app you want to monitor You will now see just the logs for your app Open up the LogDNA console and narrow the logs to just your app Open the app into a browser: Run oc endpoints -n {your namespace} and select your running app Switch to HTTPS and test the /hello/{name} API and pass in some data. You have introduced a DEBUG message and an ERROR into this API logic. Click the Execute button a few times to simulate some API calls. Try the API with your own name, and you will see just the DEBUG message appearing Try the API with ERROR as the name , and this will trigger the error code we put in the API service You will see the errors appearing in LogDNA triggered by your application Try scaling your pods for the app and calling the API again. Watch the logs and see where the API call is being routed and which pod is triggering the error","title":"Give it a try"},{"location":"developer-intermediate/log-management/#learn-more","text":"Learn more about using LogDNA:","title":"Learn more"},{"location":"developer-intermediate/log-management/#conclusion","text":"It's important to be able to manage the logs of your deployed applications to help identify issues and quickly understand how to resolve them. The Developer Tools configures LogDNA directly to your development cluster to make it easy to get the log data you need as a developer. Just deploy your application into your and its logs start appearing automatically, open the LogDNA web UI and browse your logs.","title":"Conclusion"},{"location":"developer-intermediate/monitoring/","text":"Warning Note Only study this section if you are development on an OpenShift Development environment that is managed on IBM Cloud. If you are on AWS or Azure go to the OpenShift Monitoring content In IBM Garage Method, one of the Operate practices is to automate application monitoring . Sysdig automates application monitoring, enabling an operator to view stats and collect metrics about a Kubernetes cluster and its deployments. The includes an IBM Cloud Monitoring with Sysdig service instance configured with a Sysdig agent installed in the environment's cluster. Simply by deploying your application into the , Sysdig monitors it. Open the Sysdig web UI by navigating to the OpenShift web console and click on 9 squares icon. It gives a list of the developer tools. Select SysDig and navigate to SysDig UI. Sysdig Monitoring \u00b6 IBM Cloud Monitoring with Sysdig explains how to configure and use an instance of the Sysdig service, but the has already done most of this for you. You can skip steps 1-3 about user access, provisioning an instance, and installing an agent. Sysdig dashboard \u00b6 Open the Sysdig web UI for your environment's cluster - Step 4: Launch the web UI explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Monitoring - Find the monitoring instance named after your environment's cluster, such as showcase-dev-iks-sysdig - In the monitoring instance, press the View Sysdig button to open the Sysdig web UI Explore your application \u00b6 By default, the Sysdig dashboard opens the Explore page on its Deployments and Pods grouping. Select your cluster By default, Sysdig opens its Overview by Process dashboard, which has panels showing stats about CPU, memory, and networking. This is one of Sysdig's Default Dashboards (i.e. built-in dashboards). These are the cumulative totals for all of the pods running in the cluster. Hover over a graph and a key pops up to list the pods and show each one's color. Expand your cluster and namespace, then select your deployment This shows the stats for just the pods in your deployment. On the Dashboard page, you can create your own custom dashboards. Step 5: Monitor your environment of the Getting started tutorial gives some instructions on monitoring, managing, and what to do next. Give it a try \u00b6 Before you begin to monitor your application instances, make sure that you have deployed an app into your development cluster. This Give it a Try uses template-node-typescript as an example. The SysDig service is already created, bound and configured to listen to monitoring metrics and events for your development cluster. You can see this in the HTTP overview. Open the SysDig instance that is named the same as your development cluster. Go to Dashboards > Default Dashboards > Applications > HTTP The dashboard shows stats for all incoming HTTP requests for all apps in the cluster. Browse through these views to get a feel for what they're showing. View your app's metrics \u00b6 Take a look at the metrics for your app. Select the Explore page in the left nav menu On the Explore page, select the Containerized Apps grouping Search for your app, e.g. hello-world-mjp In the list of apps, select yours, e.g. us.icr.io/mooc-team-one/hello-world-mjp:1.0.0-10 With your app selected, select Hosts & Containers > Overview by Container The Overview by Container dashboard shows metrics for the containers in your app. You will now see just the metrics for your app. You can view at different levels--from pod to namespace to node to cluster--giving you a fine grain access to your monitoring requirements. Conclusion \u00b6 It's important to be able to monitor your deployed applications. Here, the uses Sysdig Monitoring, but you never had to install or run Sysdig. Just deploy your application into the and it gets monitored automatically. After deploying your application, open the Sysdig web UI and browse the status, including the status of your cluster as a whole and your deployment in particular. Learn more \u00b6 Learn more about using SysDig Monitoring:","title":"IBM Cloud"},{"location":"developer-intermediate/monitoring/#sysdig-monitoring","text":"IBM Cloud Monitoring with Sysdig explains how to configure and use an instance of the Sysdig service, but the has already done most of this for you. You can skip steps 1-3 about user access, provisioning an instance, and installing an agent.","title":"Sysdig Monitoring"},{"location":"developer-intermediate/monitoring/#sysdig-dashboard","text":"Open the Sysdig web UI for your environment's cluster - Step 4: Launch the web UI explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Monitoring - Find the monitoring instance named after your environment's cluster, such as showcase-dev-iks-sysdig - In the monitoring instance, press the View Sysdig button to open the Sysdig web UI","title":"Sysdig dashboard"},{"location":"developer-intermediate/monitoring/#explore-your-application","text":"By default, the Sysdig dashboard opens the Explore page on its Deployments and Pods grouping. Select your cluster By default, Sysdig opens its Overview by Process dashboard, which has panels showing stats about CPU, memory, and networking. This is one of Sysdig's Default Dashboards (i.e. built-in dashboards). These are the cumulative totals for all of the pods running in the cluster. Hover over a graph and a key pops up to list the pods and show each one's color. Expand your cluster and namespace, then select your deployment This shows the stats for just the pods in your deployment. On the Dashboard page, you can create your own custom dashboards. Step 5: Monitor your environment of the Getting started tutorial gives some instructions on monitoring, managing, and what to do next.","title":"Explore your application"},{"location":"developer-intermediate/monitoring/#give-it-a-try","text":"Before you begin to monitor your application instances, make sure that you have deployed an app into your development cluster. This Give it a Try uses template-node-typescript as an example. The SysDig service is already created, bound and configured to listen to monitoring metrics and events for your development cluster. You can see this in the HTTP overview. Open the SysDig instance that is named the same as your development cluster. Go to Dashboards > Default Dashboards > Applications > HTTP The dashboard shows stats for all incoming HTTP requests for all apps in the cluster. Browse through these views to get a feel for what they're showing.","title":"Give it a try"},{"location":"developer-intermediate/monitoring/#view-your-apps-metrics","text":"Take a look at the metrics for your app. Select the Explore page in the left nav menu On the Explore page, select the Containerized Apps grouping Search for your app, e.g. hello-world-mjp In the list of apps, select yours, e.g. us.icr.io/mooc-team-one/hello-world-mjp:1.0.0-10 With your app selected, select Hosts & Containers > Overview by Container The Overview by Container dashboard shows metrics for the containers in your app. You will now see just the metrics for your app. You can view at different levels--from pod to namespace to node to cluster--giving you a fine grain access to your monitoring requirements.","title":"View your app's metrics"},{"location":"developer-intermediate/monitoring/#conclusion","text":"It's important to be able to monitor your deployed applications. Here, the uses Sysdig Monitoring, but you never had to install or run Sysdig. Just deploy your application into the and it gets monitored automatically. After deploying your application, open the Sysdig web UI and browse the status, including the status of your cluster as a whole and your deployment in particular.","title":"Conclusion"},{"location":"developer-intermediate/monitoring/#learn-more","text":"Learn more about using SysDig Monitoring:","title":"Learn more"},{"location":"developer-intermediate/ocp-log-management/","text":"In IBM Garage Method, one of the Operate practices is to automate application monitoring , including logging. Imagine your application isn't working right in production even though the environment is fine. What information would you want in your logs to help you figure out what's wrong with your application? Build logging messages for that information into your application. Given that your application is logging, as are lots of other applications and services in your cloud environment, these logs need to be managed and made accessible. LogDNA adds log management capabilities to a Kubernetes cluster and its deployments. The includes an IBM Log Analysis with LogDNA service instance configured with a LogDNA agent installed in the environment's cluster. Simply by deploying your application into the , LogDNA collects the logs. Open the LogDNA web UI by navigating to the OpenShift web console and click on 9 squares icon. It gives a list of the developer tools. Select LogDNA and navigate to the UI as shown below OpenShift log management \u00b6 IBM Log Analysis with LogDNA explains how to configure and use an instance of the LogDNA service, but the has already done this for you. You can skip these steps about provisioning an instance, installing an agent, and user access. LogDNA dashboard \u00b6 Open the LogDNA web UI for your environment's cluster - IBM Log Analysis with LogDNA: Viewing logs explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Logging - Find the logging instance named after your environment's cluster, such as showcase-dev-iks-logdna . To help find it, you can filter by your resource group. - In the logging instance, press the View LogDNA button to open the LogDNA web UI Give it a try \u00b6 Before you begin to look at your application logs, make sure you have deployed an app into your development cluster. Each of the template apps has a logging framework included, this Give it a Try will use template-node-typescript as an example. The LogDNA service is already created, bound and configured to listen to logs from your development cluster. Open the LogDNA instance that is named the same as your development cluster Click on All Apps and enter the name of your app, or, to narrow the app selection, use the suffix that you used to create it Select the app you want to monitor You will now see just the logs for your app Open up the LogDNA console and narrow the logs to just your app Open the app into a browser: Run oc endpoints -n {your namespace} and select your running app Switch to HTTPS and test the /hello/{name} API and pass in some data. You have introduced a DEBUG message and an ERROR into this API logic. Click the Execute button a few times to simulate some API calls. Try the API with your own name, and you will see just the DEBUG message appearing Try the API with ERROR as the name , and this will trigger the error code we put in the API service You will see the errors appearing in LogDNA triggered by your application Try scaling your pods for the app and calling the API again. Watch the logs and see where the API call is being routed and which pod is triggering the error Learn more \u00b6 Learn more about using LogDNA: Conclusion \u00b6 It's important to be able to manage the logs of your deployed applications to help identify issues and quickly understand how to resolve them. The Developer Tools configures LogDNA directly to your development cluster to make it easy to get the log data you need as a developer. Just deploy your application into your and its logs start appearing automatically, open the LogDNA web UI and browse your logs.","title":"OpenShift Log Management"},{"location":"developer-intermediate/ocp-log-management/#openshift-log-management","text":"IBM Log Analysis with LogDNA explains how to configure and use an instance of the LogDNA service, but the has already done this for you. You can skip these steps about provisioning an instance, installing an agent, and user access.","title":"OpenShift log management"},{"location":"developer-intermediate/ocp-log-management/#logdna-dashboard","text":"Open the LogDNA web UI for your environment's cluster - IBM Log Analysis with LogDNA: Viewing logs explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Logging - Find the logging instance named after your environment's cluster, such as showcase-dev-iks-logdna . To help find it, you can filter by your resource group. - In the logging instance, press the View LogDNA button to open the LogDNA web UI","title":"LogDNA dashboard"},{"location":"developer-intermediate/ocp-log-management/#give-it-a-try","text":"Before you begin to look at your application logs, make sure you have deployed an app into your development cluster. Each of the template apps has a logging framework included, this Give it a Try will use template-node-typescript as an example. The LogDNA service is already created, bound and configured to listen to logs from your development cluster. Open the LogDNA instance that is named the same as your development cluster Click on All Apps and enter the name of your app, or, to narrow the app selection, use the suffix that you used to create it Select the app you want to monitor You will now see just the logs for your app Open up the LogDNA console and narrow the logs to just your app Open the app into a browser: Run oc endpoints -n {your namespace} and select your running app Switch to HTTPS and test the /hello/{name} API and pass in some data. You have introduced a DEBUG message and an ERROR into this API logic. Click the Execute button a few times to simulate some API calls. Try the API with your own name, and you will see just the DEBUG message appearing Try the API with ERROR as the name , and this will trigger the error code we put in the API service You will see the errors appearing in LogDNA triggered by your application Try scaling your pods for the app and calling the API again. Watch the logs and see where the API call is being routed and which pod is triggering the error","title":"Give it a try"},{"location":"developer-intermediate/ocp-log-management/#learn-more","text":"Learn more about using LogDNA:","title":"Learn more"},{"location":"developer-intermediate/ocp-log-management/#conclusion","text":"It's important to be able to manage the logs of your deployed applications to help identify issues and quickly understand how to resolve them. The Developer Tools configures LogDNA directly to your development cluster to make it easy to get the log data you need as a developer. Just deploy your application into your and its logs start appearing automatically, open the LogDNA web UI and browse your logs.","title":"Conclusion"},{"location":"developer-intermediate/ocp-monitoring/","text":"In IBM Garage Method, one of the Operate practices is to automate application monitoring . Sysdig automates application monitoring, enabling an operator to view stats and collect metrics about a Kubernetes cluster and its deployments. The includes an IBM Cloud Monitoring with Sysdig service instance configured with a Sysdig agent installed in the environment's cluster. Simply by deploying your application into the , Sysdig monitors it. Open the Sysdig web UI by navigating to the OpenShift web console and click on 9 squares icon. It gives a list of the developer tools. Select SysDig and navigate to SysDig UI. Sysdig Monitoring \u00b6 IBM Cloud Monitoring with Sysdig explains how to configure and use an instance of the Sysdig service, but the has already done most of this for you. You can skip steps 1-3 about user access, provisioning an instance, and installing an agent. Sysdig dashboard \u00b6 Open the Sysdig web UI for your environment's cluster - Step 4: Launch the web UI explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Monitoring - Find the monitoring instance named after your environment's cluster, such as showcase-dev-iks-sysdig - In the monitoring instance, press the View Sysdig button to open the Sysdig web UI Explore your application \u00b6 By default, the Sysdig dashboard opens the Explore page on its Deployments and Pods grouping. Select your cluster By default, Sysdig opens its Overview by Process dashboard, which has panels showing stats about CPU, memory, and networking. This is one of Sysdig's Default Dashboards (i.e. built-in dashboards). These are the cumulative totals for all of the pods running in the cluster. Hover over a graph and a key pops up to list the pods and show each one's color. Expand your cluster and namespace, then select your deployment This shows the stats for just the pods in your deployment. On the Dashboard page, you can create your own custom dashboards. Step 5: Monitor your environment of the Getting started tutorial gives some instructions on monitoring, managing, and what to do next. Give it a try \u00b6 Before you begin to monitor your application instances, make sure that you have deployed an app into your development cluster. This Give it a Try uses template-node-typescript as an example. The SysDig service is already created, bound and configured to listen to monitoring metrics and events for your development cluster. You can see this in the HTTP overview. Open the SysDig instance that is named the same as your development cluster. Go to Dashboards > Default Dashboards > Applications > HTTP The dashboard shows stats for all incoming HTTP requests for all apps in the cluster. Browse through these views to get a feel for what they're showing. View your app's metrics \u00b6 Take a look at the metrics for your app. Select the Explore page in the left nav menu On the Explore page, select the Containerized Apps grouping Search for your app, e.g. hello-world-mjp In the list of apps, select yours, e.g. us.icr.io/mooc-team-one/hello-world-mjp:1.0.0-10 With your app selected, select Hosts & Containers > Overview by Container The Overview by Container dashboard shows metrics for the containers in your app. You will now see just the metrics for your app. You can view at different levels--from pod to namespace to node to cluster--giving you a fine grain access to your monitoring requirements. Conclusion \u00b6 It's important to be able to monitor your deployed applications. Here, the uses Sysdig Monitoring, but you never had to install or run Sysdig. Just deploy your application into the and it gets monitored automatically. After deploying your application, open the Sysdig web UI and browse the status, including the status of your cluster as a whole and your deployment in particular. Learn more \u00b6 Learn more about using SysDig Monitoring:","title":"OpenShift"},{"location":"developer-intermediate/ocp-monitoring/#sysdig-monitoring","text":"IBM Cloud Monitoring with Sysdig explains how to configure and use an instance of the Sysdig service, but the has already done most of this for you. You can skip steps 1-3 about user access, provisioning an instance, and installing an agent.","title":"Sysdig Monitoring"},{"location":"developer-intermediate/ocp-monitoring/#sysdig-dashboard","text":"Open the Sysdig web UI for your environment's cluster - Step 4: Launch the web UI explains how to open the web UI - In the IBM Cloud dashboard, navigate to Observability > Monitoring - Find the monitoring instance named after your environment's cluster, such as showcase-dev-iks-sysdig - In the monitoring instance, press the View Sysdig button to open the Sysdig web UI","title":"Sysdig dashboard"},{"location":"developer-intermediate/ocp-monitoring/#explore-your-application","text":"By default, the Sysdig dashboard opens the Explore page on its Deployments and Pods grouping. Select your cluster By default, Sysdig opens its Overview by Process dashboard, which has panels showing stats about CPU, memory, and networking. This is one of Sysdig's Default Dashboards (i.e. built-in dashboards). These are the cumulative totals for all of the pods running in the cluster. Hover over a graph and a key pops up to list the pods and show each one's color. Expand your cluster and namespace, then select your deployment This shows the stats for just the pods in your deployment. On the Dashboard page, you can create your own custom dashboards. Step 5: Monitor your environment of the Getting started tutorial gives some instructions on monitoring, managing, and what to do next.","title":"Explore your application"},{"location":"developer-intermediate/ocp-monitoring/#give-it-a-try","text":"Before you begin to monitor your application instances, make sure that you have deployed an app into your development cluster. This Give it a Try uses template-node-typescript as an example. The SysDig service is already created, bound and configured to listen to monitoring metrics and events for your development cluster. You can see this in the HTTP overview. Open the SysDig instance that is named the same as your development cluster. Go to Dashboards > Default Dashboards > Applications > HTTP The dashboard shows stats for all incoming HTTP requests for all apps in the cluster. Browse through these views to get a feel for what they're showing.","title":"Give it a try"},{"location":"developer-intermediate/ocp-monitoring/#view-your-apps-metrics","text":"Take a look at the metrics for your app. Select the Explore page in the left nav menu On the Explore page, select the Containerized Apps grouping Search for your app, e.g. hello-world-mjp In the list of apps, select yours, e.g. us.icr.io/mooc-team-one/hello-world-mjp:1.0.0-10 With your app selected, select Hosts & Containers > Overview by Container The Overview by Container dashboard shows metrics for the containers in your app. You will now see just the metrics for your app. You can view at different levels--from pod to namespace to node to cluster--giving you a fine grain access to your monitoring requirements.","title":"View your app's metrics"},{"location":"developer-intermediate/ocp-monitoring/#conclusion","text":"It's important to be able to monitor your deployed applications. Here, the uses Sysdig Monitoring, but you never had to install or run Sysdig. Just deploy your application into the and it gets monitored automatically. After deploying your application, open the Sysdig web UI and browse the status, including the status of your cluster as a whole and your deployment in particular.","title":"Conclusion"},{"location":"developer-intermediate/ocp-monitoring/#learn-more","text":"Learn more about using SysDig Monitoring:","title":"Learn more"},{"location":"developer-intermediate/ocp-registry/","text":"In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. When hosted in IBM Cloud, the uses the IBM Cloud Container Registry for storing container images. What is the IBM Cloud Container Registry \u00b6 IBM Cloud Container Registry is a private, multitenant Docker registry built into IBM Cloud for storing OCI images . Each IBM Cloud region hosts its own highly available registry. When deploying an application to a Kubernetes or OpenShift cluster, the cluster creates containers using the images in the registry. To package an application for deployment, the runtime must be built into an image that is stored in the registry. In this standard Docker diagram, the acme.com domain is effectively an IBM Cloud region and the Docker private registry is the instance of IBM Cloud Container Registry in that region. The diagram shows these components and their relationships: - Developer builds the image; ideally it is automated as part of a CI pipeline - Docker private registry stores the image that was built - UAT , Promote , and Production are deployment environments, such as Kubernetes clusters or namespaces, that run the containers based on the image Accessing the registry \u00b6 There are two ways to work with an IBM Cloud registry : - Web UI : In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry - CLI : Use the container-registry CLI plug-in in the IBM Cloud CLI Of the two approaches, the CLI is much more powerful. The console is mainly for looking at your registry. To use the container-registry plug-in, or even to push an image into the registry using your local Docker install, you must first log into IBM Cloud and then log into the region's IBM Cloud Container Registry: $ ibmcloud login $ ibmcloud cr login The logins determine the namespaces you can access . Other accounts for other tenants also have namespaces in the registry, but IBM Cloud doesn't let you see them and prevents you from accessing their images. Registry organization \u00b6 Like the directories and file names in a file system, a Docker registry is a single collection of images that are cataloged with hierarchical names. A Docker registry such as Docker Hub (or, as we'll see, a registry in IBM Cloud) stores an image with a hierarchical name: namespace, repository, and tag. This path is specified when tagging and pushing the image: $ docker tag <image> <namespace>/<repo-name>:<tag> $ docker push <namespace>/<repo-name>:<tag> To tag and push an image to any registry other than Docker Hub, you have to specify its domain as part of the path: $ docker push <domain>/<namespace>/<repo-name>:<tag> Registry organization in an IBM Cloud account \u00b6 IBM's registry organizes images in this same hierarchical structure. Domain : Each region in IBM Cloud (e.g. Dallas, London, Sydney, etc.) has its own multitenant registry instance with its own domain name (such as us.icr.io for Dallas/us-south). IBM provides its public images in a global registry (domain icr.io (no region)). Namespace : A namespace is associated with an IBM Cloud account and groups the account's images. Every user in the account can view and work with all images in the namespace, but users outside of the account cannot access images in the account's namespaces. An account may use multiple namespaces to organize images for groupings such as development vs. production or approved vs. experimental. Each namespace must have a name that is unique within a region for all accounts (not just your account). Repository : A repo is often thought of as the name of the image, but technically the same image can have different names (but the same image ID). Within a registry, different images with the same name stored in the same namespace will be stored in the same repo as long as they have different tags. Tag : Optional; if a command does not specify it, the default tag is latest . Two different tags enable a namespace to store two images with different image IDs but the same repository name. The tag typically specifies a different build of an image with a different image ID. Two builds usually package two different versions of an application's code, but Docker does not enforce that. The two builds could just be the same Dockerfile run twice with the same inputs (and therefore equivalent), or two completely unrelated sets of software. But two builds usually run software that is similar but at least slightly different, such as a new version of an application's code, but could be simply an alternative implementation of the code (such as for A/B testing). Therefore, when adding an image to a registry in IBM Cloud , the push command specifies the image's hierarchical path like this: docker push <region>.icr.io/<my_namespace>/<image_repo>:<tag> You can see this structure when you show the registry using the CLI: $ ibmcloud cr image-list Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0-5 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0-14 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues Notice the columns labeled Repository (which is domain/namespace/repo) and Tag --those comprise the path for finding each image. And Namespace is specified again in its own column, which is useful for filtering searches. You can also see that the first two items in the list are not two separate images but really the same image with two tags: the image ID (a.k.a. digest) is the same, so the two tags are two different ways to look up the same image. Likewise, the last two images are really the same image with two tags. The registry in the IBM Cloud console shows the same images: It shows more clearly that each image has two tags, rather than being two different images. IBM Cloud Container Registry features \u00b6 IBM Cloud Container Registry is not only a Docker registry hosted in IBM Cloud, it also adds several features to the registry service. The registry in each region is private, multitenant, and highly available--properties that a simple registry doesn't have. Here are some other features and capabilities. Vulnerability Advisor scans images in the registry to search for known security issues and generates reports with advice on how to fix your images and better secure your environment. Lists of the vulnerabilities scanned for are available in Vulnerable packages . An administrator can specify exemptions that should not be reported. Keep in mind that running containers are not scanned , only the images in the registry are. In the image lists shown above, the Security status column shows the number of issues found; the report will explain them in greater detail. In the console, click on the number of issues for details. Trusted content technology : IBM Cloud Container Registry supports images signed using Docker Content Trust (DCT) . The signature confirms who built the image, such as the CI tool. The push and pull commands maintain image signatures. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. User authorization : While all users in an account have access to that account's namespaces in the registry, an account administrator can use IAM to manage the access for different users. For example, a common customer concern is: \"How can an administrator control which images can be downloaded from Docker Hub and deployed into production?\" With IAM in the registry, an administrator can disable the ability to push, build, or delete images, then create a policy to allow these actions and only assign it to certain trusted users such as the CI pipeline's service ID. These privileged users should only add approved images into the registry, thereby limiting the containers that the developers can deploy. The administrators can likewise limit what the CD pipeline can possibly deploy to production by creating a namespace that only includes images approved for production and restricting the CD pipeline to deploy from that namespace. Image registry in the Pipeline \u00b6 The CI and CD pipelines currently exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: - Docker images : This Developer Tools Image Registry - Helm charts : A Helm repository in Artifactory In the CI pipeline, the Build image stage creates the Docker image and stores it in the registry. Then the Deploy to DEV env stage specifies the image's path in the Helm chart's values file, which the chart will use to deploy the app. Likewise, the CD pipeline specifies the image's registry path in the values files for the other deployments. When the Helm chart runs, it and Kubernetes read the image from the registry and start the containers. Give it a try \u00b6 Let's take a look at using the registry. If you haven't already, deploy your first app For example, deploy the named Typescript Microservice I deployed my in a project named dev-guide-example-bw In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry It doesn't matter whether you navigate via the Kubernetes panel or the OpenShift panel, they both share the same registry The registry is the IBM Cloud Container Registry for your region In the registry, search for the image named for your project In the CLI, run the corresponding command $ ibmcloud cr image-list | grep dev-guide-example-bw us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0-1 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues Back in the console, click on the image to see its details Under Security Status , click on 5 issues to see the issues that Vulnerability Advisor found in this image What it found were configuration issues Scroll down to see the list of configuration issues Conclusion \u00b6 We've seen that the CI pipeline packages an app as a Docker image and stores it in the registry, and also builds a Helm chart that references the image in the registry so that the chart can deploy containers built from the image. In IBM Cloud, the registry is the IBM Cloud Container Registry. The registry displays all of the namespaces in your account but not those in other tenants' accounts. In addition to basic registry functions, IBM Cloud adds additional features: private, multitenant, highly available, Vulnerability Advisor, Trusted content technology, Container Image Security Enforcement, and User authorization. Using the registry, you can see your list of images, details about the image, and details about any issues that Vulnerability Advisor found.","title":"OpenShift"},{"location":"developer-intermediate/ocp-registry/#what-is-the-ibm-cloud-container-registry","text":"IBM Cloud Container Registry is a private, multitenant Docker registry built into IBM Cloud for storing OCI images . Each IBM Cloud region hosts its own highly available registry. When deploying an application to a Kubernetes or OpenShift cluster, the cluster creates containers using the images in the registry. To package an application for deployment, the runtime must be built into an image that is stored in the registry. In this standard Docker diagram, the acme.com domain is effectively an IBM Cloud region and the Docker private registry is the instance of IBM Cloud Container Registry in that region. The diagram shows these components and their relationships: - Developer builds the image; ideally it is automated as part of a CI pipeline - Docker private registry stores the image that was built - UAT , Promote , and Production are deployment environments, such as Kubernetes clusters or namespaces, that run the containers based on the image","title":"What is the IBM Cloud Container Registry"},{"location":"developer-intermediate/ocp-registry/#accessing-the-registry","text":"There are two ways to work with an IBM Cloud registry : - Web UI : In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry - CLI : Use the container-registry CLI plug-in in the IBM Cloud CLI Of the two approaches, the CLI is much more powerful. The console is mainly for looking at your registry. To use the container-registry plug-in, or even to push an image into the registry using your local Docker install, you must first log into IBM Cloud and then log into the region's IBM Cloud Container Registry: $ ibmcloud login $ ibmcloud cr login The logins determine the namespaces you can access . Other accounts for other tenants also have namespaces in the registry, but IBM Cloud doesn't let you see them and prevents you from accessing their images.","title":"Accessing the registry"},{"location":"developer-intermediate/ocp-registry/#registry-organization","text":"Like the directories and file names in a file system, a Docker registry is a single collection of images that are cataloged with hierarchical names. A Docker registry such as Docker Hub (or, as we'll see, a registry in IBM Cloud) stores an image with a hierarchical name: namespace, repository, and tag. This path is specified when tagging and pushing the image: $ docker tag <image> <namespace>/<repo-name>:<tag> $ docker push <namespace>/<repo-name>:<tag> To tag and push an image to any registry other than Docker Hub, you have to specify its domain as part of the path: $ docker push <domain>/<namespace>/<repo-name>:<tag>","title":"Registry organization"},{"location":"developer-intermediate/ocp-registry/#registry-organization-in-an-ibm-cloud-account","text":"IBM's registry organizes images in this same hierarchical structure. Domain : Each region in IBM Cloud (e.g. Dallas, London, Sydney, etc.) has its own multitenant registry instance with its own domain name (such as us.icr.io for Dallas/us-south). IBM provides its public images in a global registry (domain icr.io (no region)). Namespace : A namespace is associated with an IBM Cloud account and groups the account's images. Every user in the account can view and work with all images in the namespace, but users outside of the account cannot access images in the account's namespaces. An account may use multiple namespaces to organize images for groupings such as development vs. production or approved vs. experimental. Each namespace must have a name that is unique within a region for all accounts (not just your account). Repository : A repo is often thought of as the name of the image, but technically the same image can have different names (but the same image ID). Within a registry, different images with the same name stored in the same namespace will be stored in the same repo as long as they have different tags. Tag : Optional; if a command does not specify it, the default tag is latest . Two different tags enable a namespace to store two images with different image IDs but the same repository name. The tag typically specifies a different build of an image with a different image ID. Two builds usually package two different versions of an application's code, but Docker does not enforce that. The two builds could just be the same Dockerfile run twice with the same inputs (and therefore equivalent), or two completely unrelated sets of software. But two builds usually run software that is similar but at least slightly different, such as a new version of an application's code, but could be simply an alternative implementation of the code (such as for A/B testing). Therefore, when adding an image to a registry in IBM Cloud , the push command specifies the image's hierarchical path like this: docker push <region>.icr.io/<my_namespace>/<image_repo>:<tag> You can see this structure when you show the registry using the CLI: $ ibmcloud cr image-list Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-graphql-typescript 1 .0.0-5 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues us.icr.io/showcase-dev-iks/template-java-spring 1 .0.0-14 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues Notice the columns labeled Repository (which is domain/namespace/repo) and Tag --those comprise the path for finding each image. And Namespace is specified again in its own column, which is useful for filtering searches. You can also see that the first two items in the list are not two separate images but really the same image with two tags: the image ID (a.k.a. digest) is the same, so the two tags are two different ways to look up the same image. Likewise, the last two images are really the same image with two tags. The registry in the IBM Cloud console shows the same images: It shows more clearly that each image has two tags, rather than being two different images.","title":"Registry organization in an IBM Cloud account"},{"location":"developer-intermediate/ocp-registry/#ibm-cloud-container-registry-features","text":"IBM Cloud Container Registry is not only a Docker registry hosted in IBM Cloud, it also adds several features to the registry service. The registry in each region is private, multitenant, and highly available--properties that a simple registry doesn't have. Here are some other features and capabilities. Vulnerability Advisor scans images in the registry to search for known security issues and generates reports with advice on how to fix your images and better secure your environment. Lists of the vulnerabilities scanned for are available in Vulnerable packages . An administrator can specify exemptions that should not be reported. Keep in mind that running containers are not scanned , only the images in the registry are. In the image lists shown above, the Security status column shows the number of issues found; the report will explain them in greater detail. In the console, click on the number of issues for details. Trusted content technology : IBM Cloud Container Registry supports images signed using Docker Content Trust (DCT) . The signature confirms who built the image, such as the CI tool. The push and pull commands maintain image signatures. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. User authorization : While all users in an account have access to that account's namespaces in the registry, an account administrator can use IAM to manage the access for different users. For example, a common customer concern is: \"How can an administrator control which images can be downloaded from Docker Hub and deployed into production?\" With IAM in the registry, an administrator can disable the ability to push, build, or delete images, then create a policy to allow these actions and only assign it to certain trusted users such as the CI pipeline's service ID. These privileged users should only add approved images into the registry, thereby limiting the containers that the developers can deploy. The administrators can likewise limit what the CD pipeline can possibly deploy to production by creating a namespace that only includes images approved for production and restricting the CD pipeline to deploy from that namespace.","title":"IBM Cloud Container Registry features"},{"location":"developer-intermediate/ocp-registry/#image-registry-in-the-pipeline","text":"The CI and CD pipelines currently exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: - Docker images : This Developer Tools Image Registry - Helm charts : A Helm repository in Artifactory In the CI pipeline, the Build image stage creates the Docker image and stores it in the registry. Then the Deploy to DEV env stage specifies the image's path in the Helm chart's values file, which the chart will use to deploy the app. Likewise, the CD pipeline specifies the image's registry path in the values files for the other deployments. When the Helm chart runs, it and Kubernetes read the image from the registry and start the containers.","title":"Image registry in the Pipeline"},{"location":"developer-intermediate/ocp-registry/#give-it-a-try","text":"Let's take a look at using the registry. If you haven't already, deploy your first app For example, deploy the named Typescript Microservice I deployed my in a project named dev-guide-example-bw In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry It doesn't matter whether you navigate via the Kubernetes panel or the OpenShift panel, they both share the same registry The registry is the IBM Cloud Container Registry for your region In the registry, search for the image named for your project In the CLI, run the corresponding command $ ibmcloud cr image-list | grep dev-guide-example-bw us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues us.icr.io/mooc-team-one/dev-guide-example-bw 1 .0.0-1 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues Back in the console, click on the image to see its details Under Security Status , click on 5 issues to see the issues that Vulnerability Advisor found in this image What it found were configuration issues Scroll down to see the list of configuration issues","title":"Give it a try"},{"location":"developer-intermediate/ocp-registry/#conclusion","text":"We've seen that the CI pipeline packages an app as a Docker image and stores it in the registry, and also builds a Helm chart that references the image in the registry so that the chart can deploy containers built from the image. In IBM Cloud, the registry is the IBM Cloud Container Registry. The registry displays all of the namespaces in your account but not those in other tenants' accounts. In addition to basic registry functions, IBM Cloud adds additional features: private, multitenant, highly available, Vulnerability Advisor, Trusted content technology, Container Image Security Enforcement, and User authorization. Using the registry, you can see your list of images, details about the image, and details about any issues that Vulnerability Advisor found.","title":"Conclusion"},{"location":"getting-started/","text":"Cloud-Native Learning Journey \u00b6 The foundation of any modern cloud solution is the cloud-native application. This core concept is formulated around an open and flexible platform and programming models that enable a wide range of solutions to be constructed and delivered to users. IBM Cloud and Red Hat aim to help developers and architects realize the value of cloud computing by delivering a world-class open platform that supports enterprise software and Cloud-Native applications. This learning journey has been designed to take developers through a step by step process to build strong skills in both the programming models IBM is aligned towards and the underlying platform, Red Hat OpenShift, a multi/hybrid cloud Kubernetes environment that provides developers extreme flexibility to build solutions. This Learning Journey targets the following personas : The Cloud-Native Developer . The DevSecOps Engineer . The Ops Engineer and the SRE ( Site Reliability Engineer ) . This Learning Journey focuses on the Cloud Enterprise Sandbox , which provides full access to an IBM Cloud Red Hat OpenShift Cloud Platform development cluster including a set of CNCF open-source software development lifecycle tools which show the IBM Cloud usage for full DevSecOps development. The Learning Journey agenda is broken into Foundation , Intermediate and Advanced developer techniques, and then extends with Application Modernization and Application Integration . With the Cloud-Native Journey you can choose how you want to use the Cloud Enterprise Sandbox . You can use the self-paced agenda or work with the Cloud Ecosystem Team to have a remote workshop model. The self-paced model will enable longer-term learning with the Cloud Enterprise Sandbox . Before you start the Cloud-Native Learning Journey, make sure you have set up the necessary prerequisites, validated your sandbox environment, and have chosen the method you plan to use for your application development.","title":"Overview"},{"location":"getting-started/#cloud-native-learning-journey","text":"The foundation of any modern cloud solution is the cloud-native application. This core concept is formulated around an open and flexible platform and programming models that enable a wide range of solutions to be constructed and delivered to users. IBM Cloud and Red Hat aim to help developers and architects realize the value of cloud computing by delivering a world-class open platform that supports enterprise software and Cloud-Native applications. This learning journey has been designed to take developers through a step by step process to build strong skills in both the programming models IBM is aligned towards and the underlying platform, Red Hat OpenShift, a multi/hybrid cloud Kubernetes environment that provides developers extreme flexibility to build solutions. This Learning Journey targets the following personas : The Cloud-Native Developer . The DevSecOps Engineer . The Ops Engineer and the SRE ( Site Reliability Engineer ) . This Learning Journey focuses on the Cloud Enterprise Sandbox , which provides full access to an IBM Cloud Red Hat OpenShift Cloud Platform development cluster including a set of CNCF open-source software development lifecycle tools which show the IBM Cloud usage for full DevSecOps development. The Learning Journey agenda is broken into Foundation , Intermediate and Advanced developer techniques, and then extends with Application Modernization and Application Integration . With the Cloud-Native Journey you can choose how you want to use the Cloud Enterprise Sandbox . You can use the self-paced agenda or work with the Cloud Ecosystem Team to have a remote workshop model. The self-paced model will enable longer-term learning with the Cloud Enterprise Sandbox . Before you start the Cloud-Native Learning Journey, make sure you have set up the necessary prerequisites, validated your sandbox environment, and have chosen the method you plan to use for your application development.","title":"Cloud-Native Learning Journey"},{"location":"getting-started/checksetup/","text":"Congratulations on becoming part of the IBM Cloud-Native Learning Journey! You should have received a welcome email that provides details of the IBM Cloud Sandbox you've been assigned to and points to this welcome page. By participating in this Cloud-Native Learning Journey, you get access to: The Crafted Education Journey Agenda. A RedHat OpenShift managed service cluster with the IBM Garage Cloud-Native Toolkit and IBM Cloud Pak for Applications pre-installed. The development environment pre-configured in a pay-as-you-go IBM Cloud account which you will use to perform your learning tasks. Notification \u00b6 The welcome email gives you information on how to access the development environment for your team. OpenShift Development Environment Details Platform : AWS, Azure or IBM Cloud Account : this is the IBM Cloud account with a configured sandbox development environment that will enable you to complete the learning agenda. Workshop Team : this is the name of your development team. GitHub Organization : this is the GitHub organization that will be used for storing your code during your learning activities. Resource Group : For IBM Cloud users this will be the resource group where the development cluster and cloud resources have been provisioned. Example of the key information in your email: Registration Details Cloud Platform: AWS Openshift Cluster URL : https://console-openshift-console---------.openshiftapps.com/ Workshop Team : <number> GitHub Organization : https://github.com/cnw-team-<number> You must associate your email address with your public GitHub account or make sure you have a public account that is using an email address that matches the one you linked to your sandbox environment. To set your IBM ID in GitHub: Go to Github (public, not Enterprise). Log in to your account or create a new one. Navigate to your account settings. In Email settings make sure that the email you used as your IBMid is the primary ID for the account. If it is not, then add your IBMid email and verify it. If you are correctly verified and associated, you will receive a second email inviting you to your team's GitHub organization. Warning Warning: If you don't associate your business email with your public GitHub account, you will not receive a Git Organization invitation and you will have issues completing the learning activities. IBM is not mixing personal email addresses with the business email that has been associated with your OpenShift Environment . Environment \u00b6 The OpenShift Environment is designed to make it easy for you to develop and deploy cloud-native applications. It includes an OpenShift cluster with open-source developer tools installed to provide a end to end Software Delivery Life Cycle (SDLC). This combination supports the development of cloud-native microservices architecture applications and CI/CD continuous deployment of these. The OpenShift Environment has been configured for a multi-tenant development team on the AWS, Azure or IBM Cloud platforms. Each development team can contain up to 200+ developers. You will be given access to your team's GitHub organization, and you will be expected to create all of your application source code into this organization. This is how real-world development projects work by enabling you to collaborate with your fellow team members while preventing you from creating duplicate project names. Validating your IBM Cloud access \u00b6 Note Note : If you have any issue with the following steps, please raise an issue on the #gsi-labs-external Slack Channel . All issues should be reported there. Follow the following steps to check you can access your team's development cluster environment. Log in and view the resources: AWS and Azure \u00b6 For AWS and Azure open the Cluster link in your registration email and authenticate with GitHub credentials, you will get direct access to the cluster. IBM Cloud \u00b6 For IBM Cloud Log in to the IBM Cloud console. The registration welcome email will include your account information, team, GitHub organization, and resource group. In the IBM Cloud console, switch to the account specified in the email. Navigate to the Resource List . In the Group filter, select your team's resource group. Click on Expand All (top right) to show all its resources. Under Cluster, you should see the cluster named workshopX-cluster is the name of your team. Some clusters may have a different number if there is more than one of that type. Web Terminal \u00b6 To be able to run CLI commands to drive common operations on the cluster you will first need to validate you can access your web terminal. To validate your terminal click on the >_ icon in the header of the OpenShift Console. You should see a terminal open at the bottom of the console screen. - Validate that you can run oc commands, run the following command oc sync --version - You should see the version number printed Success You have completed this task if you have: - Viewed your team's Red Hat OpenShift cluster. - Accessed the cluster terminal view Etiquette \u00b6 Please remember that you are sharing a development cluster with other colleagues, so be considerate of the following rules: Make sure you use your own namespace/project for your applications: Name your personal namespace/project using the convention dev-{your initials} , i.e. dev-mjp . Any namespaces/projects not following this approach will be deleted by the master builders . Name your applications using the convention {app name}-{your initials} , i.e. inventory-bff-mjp . Using the team's GitHub organization will help avoid creating apps with the same name Use Slack to talk to your fellow team members and to help debug issues. Use the GitHub team discussion feature to share knowledge. Do not delete anything in the cluster that is not yours. Especially secrets and configMaps--deleting these will mess up how the environment works for you and your colleagues, and more importantly, will cause unnecessary repair work for the master builders . Do not delete other people's pipelines or deployed apps. Do not create anything that can cause unnecessary stress to or load on your team's cluster Remember: - Part of what you will learn is how to be team members working on multiple microservices in the same development cluster with other developers - \" My app runs on my computer \" is necessary but not sufficient, it needs to run in the OpenShift clusters as well. - OpenShift is like one big Lego set of software and services. - After this course, you will be on the way to become a master builder. It's a lot of fun!","title":"Validate Environment"},{"location":"getting-started/checksetup/#notification","text":"The welcome email gives you information on how to access the development environment for your team. OpenShift Development Environment Details Platform : AWS, Azure or IBM Cloud Account : this is the IBM Cloud account with a configured sandbox development environment that will enable you to complete the learning agenda. Workshop Team : this is the name of your development team. GitHub Organization : this is the GitHub organization that will be used for storing your code during your learning activities. Resource Group : For IBM Cloud users this will be the resource group where the development cluster and cloud resources have been provisioned. Example of the key information in your email: Registration Details Cloud Platform: AWS Openshift Cluster URL : https://console-openshift-console---------.openshiftapps.com/ Workshop Team : <number> GitHub Organization : https://github.com/cnw-team-<number> You must associate your email address with your public GitHub account or make sure you have a public account that is using an email address that matches the one you linked to your sandbox environment. To set your IBM ID in GitHub: Go to Github (public, not Enterprise). Log in to your account or create a new one. Navigate to your account settings. In Email settings make sure that the email you used as your IBMid is the primary ID for the account. If it is not, then add your IBMid email and verify it. If you are correctly verified and associated, you will receive a second email inviting you to your team's GitHub organization. Warning Warning: If you don't associate your business email with your public GitHub account, you will not receive a Git Organization invitation and you will have issues completing the learning activities. IBM is not mixing personal email addresses with the business email that has been associated with your OpenShift Environment .","title":"Notification"},{"location":"getting-started/checksetup/#environment","text":"The OpenShift Environment is designed to make it easy for you to develop and deploy cloud-native applications. It includes an OpenShift cluster with open-source developer tools installed to provide a end to end Software Delivery Life Cycle (SDLC). This combination supports the development of cloud-native microservices architecture applications and CI/CD continuous deployment of these. The OpenShift Environment has been configured for a multi-tenant development team on the AWS, Azure or IBM Cloud platforms. Each development team can contain up to 200+ developers. You will be given access to your team's GitHub organization, and you will be expected to create all of your application source code into this organization. This is how real-world development projects work by enabling you to collaborate with your fellow team members while preventing you from creating duplicate project names.","title":"Environment"},{"location":"getting-started/checksetup/#validating-your-ibm-cloud-access","text":"Note Note : If you have any issue with the following steps, please raise an issue on the #gsi-labs-external Slack Channel . All issues should be reported there. Follow the following steps to check you can access your team's development cluster environment. Log in and view the resources:","title":"Validating your IBM Cloud access"},{"location":"getting-started/checksetup/#aws-and-azure","text":"For AWS and Azure open the Cluster link in your registration email and authenticate with GitHub credentials, you will get direct access to the cluster.","title":"AWS and Azure"},{"location":"getting-started/checksetup/#ibm-cloud","text":"For IBM Cloud Log in to the IBM Cloud console. The registration welcome email will include your account information, team, GitHub organization, and resource group. In the IBM Cloud console, switch to the account specified in the email. Navigate to the Resource List . In the Group filter, select your team's resource group. Click on Expand All (top right) to show all its resources. Under Cluster, you should see the cluster named workshopX-cluster is the name of your team. Some clusters may have a different number if there is more than one of that type.","title":"IBM Cloud"},{"location":"getting-started/checksetup/#web-terminal","text":"To be able to run CLI commands to drive common operations on the cluster you will first need to validate you can access your web terminal. To validate your terminal click on the >_ icon in the header of the OpenShift Console. You should see a terminal open at the bottom of the console screen. - Validate that you can run oc commands, run the following command oc sync --version - You should see the version number printed Success You have completed this task if you have: - Viewed your team's Red Hat OpenShift cluster. - Accessed the cluster terminal view","title":"Web Terminal"},{"location":"getting-started/checksetup/#etiquette","text":"Please remember that you are sharing a development cluster with other colleagues, so be considerate of the following rules: Make sure you use your own namespace/project for your applications: Name your personal namespace/project using the convention dev-{your initials} , i.e. dev-mjp . Any namespaces/projects not following this approach will be deleted by the master builders . Name your applications using the convention {app name}-{your initials} , i.e. inventory-bff-mjp . Using the team's GitHub organization will help avoid creating apps with the same name Use Slack to talk to your fellow team members and to help debug issues. Use the GitHub team discussion feature to share knowledge. Do not delete anything in the cluster that is not yours. Especially secrets and configMaps--deleting these will mess up how the environment works for you and your colleagues, and more importantly, will cause unnecessary repair work for the master builders . Do not delete other people's pipelines or deployed apps. Do not create anything that can cause unnecessary stress to or load on your team's cluster Remember: - Part of what you will learn is how to be team members working on multiple microservices in the same development cluster with other developers - \" My app runs on my computer \" is necessary but not sufficient, it needs to run in the OpenShift clusters as well. - OpenShift is like one big Lego set of software and services. - After this course, you will be on the way to become a master builder. It's a lot of fun!","title":"Etiquette"},{"location":"getting-started/cli/","text":"Cloud Native Toolkit - Command Line Interface \u00b6 Invoking the CLI \u00b6 When the CLI is installed , it adds an executable named igc to the PATH. Running igc --help will list the available commands. The output text will be similar to the following: Info As of v0.5.1, the Toolkit CLI will now install the commands as plugins to the kubectl and oc CLIs. For example, all the following are equivalent: igc pipeline kubectl pipeline oc pipeline Lets look at what plugins have been installed into the oc CLI in the Web Terminal $ oc plugin list /usr/local/bin/kubectl-console /usr/local/bin/kubectl-credentials /usr/local/bin/kubectl-dashboard /usr/local/bin/kubectl-enable /usr/local/bin/kubectl-endpoints /usr/local/bin/kubectl-git /usr/local/bin/kubectl-gitops /usr/local/bin/kubectl-gitsecret /usr/local/bin/kubectl-igc /usr/local/bin/kubectl-pipeline /usr/local/bin/kubectl-sync /usr/local/bin/kubectl-toolconfig Prerequisite tools \u00b6 Some commands provided by the Toolkit CLI orchestrate interactions between other CLIs. To get started please install the prerequisite tools, these are already installed in the Web Terminal, in particular: The Kubernetes CLI The Red Hat OpenShift CLI The IBM Cloud CLI - used to interact with IBM Cloud vlans (not needed if tools will not run on IBM Cloud) Available commands \u00b6 dashboard \u00b6 Opens the Developer Dashboard in the default browser. If a default browser has not been configured, then the URL to the Dashboard will be printed out. The dashboard displays the Cloud-Native Toolkit tools configured within the cluster along with links to activation content and links to Starter Kits to start a project quickly. This command requires that the login context for the cluster has already been established. Warning This command will only work from a desktop command interface Command flags \u00b6 -n : the namespace where the dashboard has been deployed; the default is tools Usage \u00b6 CLI The command is used in the following way: oc dashboard OpenShift The following commands would have the same result on OpenShift: HOST = $( oc get routes/dashboard -n tools -o jsonpath = '{.spec.host}' ) open \"https:// $HOST \" Kubernetes The following commands would have the same result on Kubernetes: HOST = $( kubectl get ingress/developer-dashboard -n tools -o jsonpath = '{.spec.rules[0].host}' ) open \"https:// $HOST \" Related commands \u00b6 credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command console \u00b6 Opens the OpenShift admin console in the default browser. If a default browser has not been configured, then the URL to the console will be printed out. This command requires that the login context for the cluster has already been established. Warning This command will only work from a desktop command interface Usage \u00b6 CLI The command is used in the following way: oc console OpenShift The following commands would have the same result on OpenShift: open $( oc whoami --show-console ) Kubernetes The following commands would have the same result on Kubernetes: REGION = \"...\" CLUSTER_NAME = \"...\" CLUSTER_ID = $( ibmcloud ks cluster get --cluster ${ CLUSTER_NAME } | grep -E \"^ID\" | sed -E \"s/ID: +([^ ]+)/\\\\1/g\" ) open \"https:// ${ REGION } .containers.cloud.ibm.com/kubeproxy/clusters/ ${ CLUSTER_ID } /service/#/overview?namespace=default\" Related commands \u00b6 credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command git \u00b6 Opens the Git repo in the default browser for the current working directory. If a default browser has not been configured, then the URL to the repo will be printed out. Warning This command will only work from a desktop command interface Usage \u00b6 CLI The command is used in the following way: oc git If you have multiple remotes and would like to open one other than origin : oc git origin-fork Manual The following commands would have the same result with shell commands: alias gh = \"open https://github. $( git config remote.origin.url | cut -f2 -d. | tr ':' / ) \" Related commands \u00b6 credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command credentials \u00b6 Lists the endpoints, usernames, and passwords for the tools configured in the environment. This is the easiest way to get the login credentials for each of the installed tools. Ideally all of the tools would be accessible via SSO at which point this command will be obsolete. The command works by reading information available in the cluster. When each tool is installed by the toolkit, a config map and secret are created to store the url and credential for the tool. That information is used in a number of different ways within the environment: Provide configuration information to the pipelines Populate the tiles on the Developer Dashboard Populate the results of the credentials command This command requires that the login context for the cluster has already been established. Command flags \u00b6 -n : the namespace where the tools have been deployed; the default is tools Usage \u00b6 CLI The command is used in the following way: oc credentials The credential output is JSON format like this Credentials: { argocd: { user: 'admin' , password: '12345678' , url: 'https://argocd-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . dashboard: { url: 'https://dashboard-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . } OpenShift or Kubernetes The following commands have the same result (note the dependency on jq ): # config maps kubectl get configmap -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-config\").data]' # secrets kubectl get secret -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-apikey\").data | with_entries(.value |= @base64d)]' Related commands \u00b6 dashboard : displays the url of the Developer Dashboard and launches the default browser tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command endpoints \u00b6 Lists the ingress and/or route URLs for the applications in a given namespace. An attempt will be made to get the namespace from the current context if one is not provided as an argument. Results of the command are provided in an interactive menu. If one of the endpoints is selected, it will display the URL and launch it in the default browser. Selecting Exit will print the full list of endpoints and exit. This command requires that the login context for the cluster has already been established. Command flags \u00b6 -n : the namespace from which the endpoints will be read; the value will be read from the current context if not provided Usage \u00b6 CLI The command is used in the following way: oc endpoints OpenShift The following commands list the route and ingress endpoints: # routes kubectl get route -n tools # ingress kubectl get ingress -n tools Kubernetes The following commands list the ingress endpoints: kubectl get ingress -n tools sync \u00b6 Creates a Kubernetes namespace or OpenShift project (if it doesn't already exist) and sets it up so that the namespace can be used as a target for application deployments and/or to host the environment. The command synchronize the ConfigMaps and Secrets from a template namespace (ie tools ) to create a \"development\" namespace. After the command has run successfully it will set the provided namespace in the current context (e.g. equivalent to oc project X ) This command copies the relevant ConfigMaps and Secrets into the namespace that are needed for development activities. Managing resources across namespaces (particularly ConfigMaps and Secrets ) is a common challenge in Kubernetes environments. We have provided the command at this time to simplify the steps required to get everything ready. Ultimately, this problem seems like an ideal one for an Operator to solve and when one is available (either from the Toolkit or elsewhere) this command will be retired or transitioned. The command will setup the \"development\" namespace where DevOps pipelines can be run (e.g. myapp-dev) The \"development\" namespace will have the ConfigMaps and Secrets copied over. The command can also add additional privileges to the tekton pipeline service account. These privileges are needed to run the buildah task in OpenShift 4.7 Positionals: namespace The namespace that will be created and/or prepared Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] -p, --tekton flag indicating the tekton pipeline service account should be given privileged scc --verbose flag to produce more verbose logging [boolean] Usage \u00b6 CLI Create a dev namespace for development oc sync dev-myapp OpenShift Create a dev namespace for development oc sync dev-myapp Kubernetes Create a dev namespace for development kubectl sync myapp-dev Manual ConfigMap and Secret setup The following steps will copy the ConfigMaps and Secrets from a template namespace to a target namespace: export TEMPLATE_NAMESPACE = \"tools\" export NAMESPACE = \"NAMESPACE\" kubectl get configmap -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get configmap ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done kubectl get secret -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get secret ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done The -p or --tekton flag performs the same function as command: oc adm policy add-scc-to-user privileged -z pipeline pull-secret \u00b6 Copy pull secrets into the provided project from the template namespace for the IBM Container Registry. Set up a service account in the namespace with the pull secret(s) for the IBM Container Registry that are copied. The pull secret(s) are required in order for pods to pull images that are stored in the IBM Container Registry. When the cluster is created in IBM Cloud, a pull secret is provided in the default namespace. In order for a pod in another namespace to use it, the secret must first be copied into the namespace. After that, the pod either needs to reference the pull secret directly or the service account used by the resource needs to have a reference to the secret. The CLI copies the pull secret over and adds it to the service account so the pod can take either approach. This command should be use to set up \"release\" namespaces where applications can be deployed (e.g. test, staging) Positionals: namespace The namespace into which the pull-secret(s) will be created Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --dev flag to indicate this is a development namespace and that development artifacts should be created[boolean] --verbose flag to produce more verbose logging [boolean] Usage \u00b6 CLI Copy the pull secret from default namespace into myapp-test namespace and add to serviceAccount default oc pull-secret myapp-test -t default -z default Manual pull secret setup The following commands will copy the pull secret(s) from the default namespace and add them to the service account: export NAMESPACE = \"myapp-test\" export SERVICE_ACCOUNT = \"default\" if [[ $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | grep icr | wc -l | xargs ) -eq 0 ]] ; then echo \"*** Copying pull secrets from default namespace to ${ NAMESPACE } namespace\" kubectl get secrets -n default | grep icr | sed \"s/\\([A-Za-z-]*\\) *.*/\\1/g\" | while read default_secret ; do kubectl get secret ${ default_secret } -n default -o yaml --export | sed \"s/name: default-/name: /g\" | kubectl -n ${ NAMESPACE } create -f - done else echo \"*** Pull secrets already exist on ${ NAMESPACE } namespace\" fi EXISTING_SECRETS = $( kubectl get serviceaccount/ ${ SERVICE_ACCOUNT } -n \" ${ NAMESPACE } \" -o json | tr '\\n' ' ' | sed -E \"s/.*imagePullSecrets.: \\[([^]]*)\\].*/\\1/g\" | grep icr | wc -l | xargs ) if [[ ${ EXISTING_SECRETS } -eq 0 ]] ; then echo \"*** Adding secrets to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" PULL_SECRETS = $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ \"{\\\"name\\\": \\\"\"}{ .metadata.name }{ \"\\\"}\\n\" }{ end }' | grep icr | grep -v \" ${ NAMESPACE } \" | paste -sd \",\" - ) kubectl patch -n \" ${ NAMESPACE } \" serviceaccount/ ${ SERVICE_ACCOUNT } -p \"{\\\"imagePullSecrets\\\": [ ${ PULL_SECRETS } ]}\" else echo \"*** Pull secrets already applied to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" fi pipeline \u00b6 Connects a branch in a Git repo to a either a Jenkins or Tekton CI pipeline in the environment and triggers an initial build. A webhook is also created so that when a new commit is added to the branch, the pipeline is triggered to start the process to rebuild and redeploy the app using the new code. Currently, webhook creation is supported for repositories hosted on Gitlab, Github, Github Enterprise, Bitbucket, and Gogs. This command can either be used to register a git repository that has previously been cloned to the local filesystem OR using the remote repo url. Repository location \u00b6 The pipeline command supports registering a CI pipeline for a repository that has been cloned locally or using the remote repository url. Local repository \u00b6 If you are registering a local repository then you must run the command from within the directory of your local clone of the Git repo. When registering a local repository, the pipeline will use the branch that is currently checked out. Remote repository \u00b6 To register a remote repository, pass the repo url as an argument to the pipeline command. For example: oc pipeline \"https://github.com/my-org/my-repo\" You can optionally provide the branch name with the url using a hash ( # ): oc pipeline \"https://github.com/my-org/my-repo#my-branch\" Note When registering a remote git repo, if the branch is not provided then the default branch will be used. Pipeline type \u00b6 The pipeline command supports registering pipelines with either Tekton or Jenkins. The pipeline can be specified from the command-line with either the --tekton or --jenkins flags. If a flag is not provided then you will be prompted to select the pipeline. Git credentials \u00b6 The command will prompt for the username and password/personal access token to access the Git repository, unless those are already stored in a secret in the cluster namespace or provided as command-line parameters. The username and password can be provided with the -u and -p flags. If you want to change the credentials that have already been stored in the cluster namespace, the -g argument an be provided and you will be prompted for the credentials. Tekton template pipeline \u00b6 If a Tekton pipeline will be used, a template pipeline must be selected for the new repository pipeline. The command reads the template pipelines available in the template namespace. The template namespace can be provided with the -t argument and will default to tools if not provided. The command will also filter the list of pipelines based on the runtime determined from the given repository. If there is more than one template pipeline available then you will be prompted to pick one. The template pipeline can also be provided on the command-line using the --pipeline argument. If the name doesn't match an available pipeline then you will be prompted to select one. Pipeline parameters \u00b6 Once the pipeline template is selected, you will be prompted to provide values for the defined pipeline parameters. The values can also be provided from the command-line using the -p argument. The name of the parameter is listed at the beginning of the prompt message. Multiple parameters can be provided by repeating the -p argument. For example: oc pipeline --tekton \"https://github.com/my-org/my-repo\" -p scan-image = false -p edge = false Optional arguments \u00b6 -u : the username for accessing the Git repo -P : the password or personal access token for accessing the Git repo -g : ignore existing git-credentials secret and prompt to update the values -p : provide parameters for the pipeline --jenkins : deploy using a Jenkins pipeline --tekton : deploy using a Tekton pipeline --pipeline : the name of the Tekton pipeline -n : the deployment namespace; if not provided the namespace from the current context will be used -t : the template namespace; if not provided the value will default to tools Usage CLI Create a Jenkins pipeline in the current namespace and prompt for the Git credentials oc pipeline --jenkins Create a Tekton pipeline in the my-dev namespace, using the Git credentials gituser and gitpat oc pipeline -n my-dev -u gituser -P gitpat --tekton Manual Steps for Tekton The following is the list of steps required to manually configure a Tekton pipeline with your development cluster. Set the current namespace/project OpenShift oc project { namespace } Kubernetes kubectl config set-context --current --namespace ={ namespace } Copy the tasks from the tools namespace into the current namespace kubectl get tasks -o json -n tools | \\ jq 'del(.items[].metadata.uid) | del(.items[].metadata.selfLink) | del(.items[].metadata.resourceVersion) | del(.items[].metadata.namespace) | del(.items[].metadata.creationTimestamp) | del(.items[].metadata.generation) | del(.items[].metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' | \\ kubectl apply -f - List the available pipeline templates in the tools namespace and select the one to use for your project. kubectl get pipelines -n tools Clone the selected pipeline from the tools namespace into the current namespace kubectl get pipeline ${ TEMPLATE_NAME } -o json -n tools | \\ jq --arg PIPELINE_NAME ${ PIPELINE_NAME } '.metadata.name = $PIPELINE_NAME | del(.metadata.uid) | del(.metadata.selfLink) | del(.metadata.resourceVersion) | del(.metadata.namespace) | del(.metadata.creationTimestamp) | del(.metadata.generation) | del(.metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' | \\ kubectl apply -f - where: - TEMPLATE_NAME is the name of the pipeline selected in the previous step - PIPELINE_NAME is the name of the pipeline for your project Start the pipeline \u00b6 The Tekton pipeline does not automatically start when it is first created. After the webhook is created in the subsequent steps the pipeline will start when changes are pushed to the repository but before that, we can manually trigger the build to start using the CLI. (The pipeline can also be started through the OpenShift Console.) Kick off the pipeline using the Tekton CLI tkn pipeline start { PIPELINE_NAME } -s pipeline -p git-url ={ GIT_REPO } -p git-revision ={ GIT_BRANCH } To create a new PipelineRun with the same parameters from a previous PipelineRun you can do the following tkn pipeline start { PIPELINE_NAME } --use-pipelinerun { PIPELINE_RUN_NAME } Create a Git Webhook** \u00b6 Create the event listener and triggers \u00b6 In order for a Tekton pipeline to be triggered by a webhook notification, several resources need to be created: TriggerTemplate - defines how to create the PipelineRun and any other required resources when a webhook notification is received. TriggerBinding - provides a mapping for the information available in the webhook payload into the TriggerTemplate EventListener - makes the connection between the Pipeline, TriggerBinding, and TriggerTemplate together that will be created when a webhook is triggered Create a file named tekton-trigger.yaml and paste in the following contents: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : { PIPELINE_NAME } name : { TRIGGER_TEMPLATE_NAME } spec : params : - description : The git revision name : gitrevision - description : The git repository url name : gitrepositoryurl resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : generateName : { PIPELINE_NAME } - spec : params : - name : git-url value : $(params.gitrepositoryurl) - name : git-revision value : $(params.gitrevision) - name : scan-image value : \"false\" pipelineRef : name : { PIPELINE_NAME } --- apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerBinding metadata : labels : app : { PIPELINE_NAME } name : { TRIGGER_BINDING_NAME } spec : params : - name : gitrevision value : $(body.head_commit.id) - name : gitrepositoryurl value : $(body.repository.url) --- apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : { PIPELINE_NAME } name : { EVENT_LISTENER_NAME } spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding name : { TRIGGER_BINDING_NAME } interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/{BRANCH_NAME}' name : { PIPELINE_NAME } template : name : { TRIGGER_TEMPLATE_NAME } Replace the place holder values with the appropriate values: where: - {PIPELINE_NAME} is the name of your Pipeline resource from the previous section. - {TRIGGER_TEMPLATE_NAME} is the name of the TriggerTemplate. This can be the same as the {PIPELINE_NAME} . - {TRIGGER_BINDING_NAME} is the name of the TriggerBinding. This can be the same as the {PIPELINE_NAME} . - {EVENT_LISTENER_NAME} is the name of the EventListener. This can be el-{PIPELINE_NAME} if the EventListeners will be configured one-to-one with the Pipelines or the instance can be shared across the project. - {BRANCH_NAME} is the name of the branch from which webhook events should trigger the build to start Apply the trigger resources to the cluster, in the same namespace where the Pipeline was created kubectl apply -f tekton-trigger.yaml In order for the Git repository to trigger the build with a webhook, an endpoint needs to be available. Expose the EventListener service with a route to provide that endpoint. oc expose service ${ EVENT_LISTENER_NAME } --name = ${ EVENT_LISTENER_NAME } Register the webhook url with your Git repository \u00b6 The particular steps will vary to create the Webhook depending on the flavor of hosted Git you are using (GitHub, GitHub Enterprise, GitLab, BitBucket, etc) but the general flow will remain the same. Get the host name for the route created in the previous step oc get route ${ EVENT_LISTENER_NAME } -o jsonpath = '{.spec.host}' Create a webhook in your hosted Git repository using the https url of the host name from the previous step that is triggered by the desired events (e.g. push, pull request, release) Manual steps for Jenkins on OpenShift Provision Jenkins ephemeral \u00b6 Jenkins ephemeral provides a kubernetes native version of Jenkins that dynamically provisions build agents on-demand. It's ephemeral meaning it doesn't allocate any persistent storage in the cluster. Set the project/namespace oc project { NAMESPACE } where: - {NAMESPACE} is the development namespace where the pipelines will run Run the following command to provision the Jenkins instance in your namespace oc new-app jenkins-ephemeral Open the OpenShift console as described in the login steps above Select Workloads -> Pods from the left-hand menu At the top of the page select your project/namespace from the drop-down list to see the Jenkins instance running Give the jenkins service account privileged access \u00b6 All of the Cloud-Native Toolkit pipelines use buildah to build and push the container image to the registry. Unfortunately, the buildah container must run as root. By default, OpenShift does not allow containers to run as the root user and special permission is required for the pipeline to run. With the Jenkins build engine, all the build processes run as the jenkins service account. In order for the pipeline container to run as root on OpenShift we will need to give the privileged security context constraint (scc) to jenkins service account with the following command: oc project { NAMESPACE } oc adm policy add-scc-to-user privileged -z jenkins where: - {NAMESPACE} should be the name you claimed in the box note prefixed to -dev (e.g. user01-dev) Create a secret with git credentials \u00b6 In order for Jenkins to have access to the git repository, particularly if it is a private repository, a Kubernetes secret needs to be added that contains the git credentials. Create a personal access token (if you don't already have one) using the prereq instructions Copy the following into a file called gitsecret.yaml and update the {Git-Username}, and {Git-PAT} apiVersion : v1 kind : Secret metadata : annotations : build.openshift.io/source-secret-match-uri-1 : https://github.com/* labels : jenkins.io/credentials-type : usernamePassword name : git-credentials type : kubernetes.io/basic-auth stringData : username : { Git-Username } password : { Git-PAT } where: - Git-Username is the username that has access to the git repo - Git-PAT is the personal access token of the git user After logging into the cluster, create the secret by running the following: oc project { NAMESPACE } oc create -f gitsecret.yaml where: - {NAMESPACE} is the development namespace where the pipelines will run Create the build config \u00b6 On OpenShift 4.3, Jenkins is built into the OpenShift console and the build pipelines can be managed using Kubernetes custom resources. The following steps will create one by hand to create the build pipeline for the new application. Copy the following into a file called buildconfig.yaml and update the {Name}, {Secret}, {Git-Repo-URL}, and {Namespace} apiVersion : v1 kind : BuildConfig metadata : name : { Name } spec : triggers : - type : GitHub github : secret : my-secret-value source : git : uri : { Git-Repo-URL } ref : master strategy : jenkinsPipelineStrategy : jenkinsfilePath : Jenkinsfile env : - name : CLOUD_NAME value : openshift - name : NAMESPACE value : { NAMESPACE } where: - Name is in the name of your pipeline - Git-Repo-URL is the https url to the git repository - {NAMESPACE} is the development namespace where the pipelines will run Assuming you are still logged into the cluster, create the buildconfig resource in the cluster oc project { NAMESPACE } oc create -f buildconfig.yaml where: - {NAMESPACE} is the development namespace where the pipelines will run View the pipeline in the OpenShift console \u00b6 Open the OpenShift console for the cluster Select Builds -> Build Config Select your project/namespace (i.e. {NAMESPACE} ) from the top The build pipeline that was created in the previous step should appear Manually trigger the pipeline by selecting Start Build the menu button on the right side of the row Create the webhook \u00b6 Run the following to get the webhook details from the build config oc project { NAMESPACE } oc describe bc { Name } where: - {Name} is the name used in the previous step for the build config - {NAMESPACE} is the development namespace where the pipelines will run The webhook url will have a structure similar to: http://{openshift_api_host:port}/oapi/v1/namespaces/{namespace}/buildconfigs/{name}/webhooks/{secret}/generic In this case {secret} will be my-secret-value Open a browser to the GitHub repo deployed in the previous step in the build config Select Settings then Webhooks . Press Add webhook Paste the webhook url from the previous step into the Payload url Set the content-type to application/json and leave the rest of the values as the defaults Press Add webhook to create the webhook Press the button to test the webhook to ensure that everything was done properly Go back to your project code and push a change to one of the files Go to the Build pipeline page in the OpenShift console to see that the build was triggered Manual steps for Jenkins on Kubernetes TBD enable \u00b6 Adds DevOps artifacts to a Git repo that the environment uses to deploy the app. The command displays a list of available pipelines and applies the one you select to your code repo. The DevOps files added to your repo include (but are not limited to): Helm chart Container file This command DOES NOT require that the terminal is already logged in to an IBM Cloud account nor the cluster. It DOES require that the terminal's current directory is the repository directory for your local copy of the Git repo. The command will add files to the local repo. You should commit these new files and push them to the server repo. Then run oc pipeline to connect your repo to a pipeline in the environment. Command flags \u00b6 --repo : the set of pipelines to choose from; the default is https://github.com/ibm-garage-cloud/garage-pipelines -p : the name of the pipeline that should be installed; if not provided then you will be prompted -b : the branch from which the pipeline should be installed; the default is stable r : the version number of the pipeline that should be installed; the default is latest Usage \u00b6 CLI Before running the command, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Apply the pipeline updates using the CLI command oc enable Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them Manual steps The follow provides the manual steps equivalent to the igc enable command: Before updating the pipelines, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Download the index.yaml file containing the available pipeline versions curl -O https://ibm-garage-cloud.github.io/garage-pipelines/index.yaml Look through the index.yaml file to identify the url for the desired pipeline branch and version With the PIPELINE_URL from the previous step, run the following to download the pipeline tar-ball curl -O ${ PIPELINE_URL } Extract the tar-ball into your repository directory. You will be prompted to overwrite files. Overwrite as appropriate tar xzf ${ PIPELINE_FILE } Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them git-secret \u00b6 Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named {git org}.{git repo} . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated it won't be updated unless the --replace argument is passed. Command flags \u00b6 [positional] : overwrites the name of the config map -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage \u00b6 ===\"CLI\" The following gives an example of using the git-secret command to set up the config map and secret in the dev namespace ```shell oc git-secret -n dev ``` Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap showcase.myrepo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master gitops \u00b6 Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named gitops-repo . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated unless the --replace argument is passed. Command flags \u00b6 -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage \u00b6 CLI The following gives an example of using the gitops command to set up the config map and secret in the dev namespace igc gitops -n dev Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap github-repo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master tool-config \u00b6 Configures a new tool in the environment. After deploying the tool, use this command to add the tool to the list of credentials so that it will be displayed in the dashboard. Command flags \u00b6 The name for the tool -n : the tools namespace; the default is tools --url : the endpoint for accessing the tool, usually its dashboard --username : (optional) the user name for logging into to tool --password : (optional) the password for logging into to tool Usage \u00b6 CLI The following gives an example of using the tool-config command to set up a tool named my-tool with its dashboard's endpoint and credentials oc tool-config my-tool \\ --url https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --username admin \\ --password password Manual install with helm The following gives an example of using helm directly to do the equivalent (using helm 3): helm install my-tool tool-config \\ --repo https://ibm-garage-cloud.github.io/toolkit-charts/ \\ --set url = https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --set username = admin \\ --set password = password","title":"Developer Tools CLI"},{"location":"getting-started/cli/#cloud-native-toolkit-command-line-interface","text":"","title":"Cloud Native Toolkit - Command Line Interface"},{"location":"getting-started/cli/#invoking-the-cli","text":"When the CLI is installed , it adds an executable named igc to the PATH. Running igc --help will list the available commands. The output text will be similar to the following: Info As of v0.5.1, the Toolkit CLI will now install the commands as plugins to the kubectl and oc CLIs. For example, all the following are equivalent: igc pipeline kubectl pipeline oc pipeline Lets look at what plugins have been installed into the oc CLI in the Web Terminal $ oc plugin list /usr/local/bin/kubectl-console /usr/local/bin/kubectl-credentials /usr/local/bin/kubectl-dashboard /usr/local/bin/kubectl-enable /usr/local/bin/kubectl-endpoints /usr/local/bin/kubectl-git /usr/local/bin/kubectl-gitops /usr/local/bin/kubectl-gitsecret /usr/local/bin/kubectl-igc /usr/local/bin/kubectl-pipeline /usr/local/bin/kubectl-sync /usr/local/bin/kubectl-toolconfig","title":"Invoking the CLI"},{"location":"getting-started/cli/#prerequisite-tools","text":"Some commands provided by the Toolkit CLI orchestrate interactions between other CLIs. To get started please install the prerequisite tools, these are already installed in the Web Terminal, in particular: The Kubernetes CLI The Red Hat OpenShift CLI The IBM Cloud CLI - used to interact with IBM Cloud vlans (not needed if tools will not run on IBM Cloud)","title":"Prerequisite tools"},{"location":"getting-started/cli/#available-commands","text":"","title":"Available commands"},{"location":"getting-started/cli/#dashboard","text":"Opens the Developer Dashboard in the default browser. If a default browser has not been configured, then the URL to the Dashboard will be printed out. The dashboard displays the Cloud-Native Toolkit tools configured within the cluster along with links to activation content and links to Starter Kits to start a project quickly. This command requires that the login context for the cluster has already been established. Warning This command will only work from a desktop command interface","title":"dashboard"},{"location":"getting-started/cli/#console","text":"Opens the OpenShift admin console in the default browser. If a default browser has not been configured, then the URL to the console will be printed out. This command requires that the login context for the cluster has already been established. Warning This command will only work from a desktop command interface","title":"console"},{"location":"getting-started/cli/#git","text":"Opens the Git repo in the default browser for the current working directory. If a default browser has not been configured, then the URL to the repo will be printed out. Warning This command will only work from a desktop command interface","title":"git"},{"location":"getting-started/cli/#credentials","text":"Lists the endpoints, usernames, and passwords for the tools configured in the environment. This is the easiest way to get the login credentials for each of the installed tools. Ideally all of the tools would be accessible via SSO at which point this command will be obsolete. The command works by reading information available in the cluster. When each tool is installed by the toolkit, a config map and secret are created to store the url and credential for the tool. That information is used in a number of different ways within the environment: Provide configuration information to the pipelines Populate the tiles on the Developer Dashboard Populate the results of the credentials command This command requires that the login context for the cluster has already been established.","title":"credentials"},{"location":"getting-started/cli/#endpoints","text":"Lists the ingress and/or route URLs for the applications in a given namespace. An attempt will be made to get the namespace from the current context if one is not provided as an argument. Results of the command are provided in an interactive menu. If one of the endpoints is selected, it will display the URL and launch it in the default browser. Selecting Exit will print the full list of endpoints and exit. This command requires that the login context for the cluster has already been established.","title":"endpoints"},{"location":"getting-started/cli/#sync","text":"Creates a Kubernetes namespace or OpenShift project (if it doesn't already exist) and sets it up so that the namespace can be used as a target for application deployments and/or to host the environment. The command synchronize the ConfigMaps and Secrets from a template namespace (ie tools ) to create a \"development\" namespace. After the command has run successfully it will set the provided namespace in the current context (e.g. equivalent to oc project X ) This command copies the relevant ConfigMaps and Secrets into the namespace that are needed for development activities. Managing resources across namespaces (particularly ConfigMaps and Secrets ) is a common challenge in Kubernetes environments. We have provided the command at this time to simplify the steps required to get everything ready. Ultimately, this problem seems like an ideal one for an Operator to solve and when one is available (either from the Toolkit or elsewhere) this command will be retired or transitioned. The command will setup the \"development\" namespace where DevOps pipelines can be run (e.g. myapp-dev) The \"development\" namespace will have the ConfigMaps and Secrets copied over. The command can also add additional privileges to the tekton pipeline service account. These privileges are needed to run the buildah task in OpenShift 4.7 Positionals: namespace The namespace that will be created and/or prepared Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] -p, --tekton flag indicating the tekton pipeline service account should be given privileged scc --verbose flag to produce more verbose logging [boolean]","title":"sync"},{"location":"getting-started/cli/#pull-secret","text":"Copy pull secrets into the provided project from the template namespace for the IBM Container Registry. Set up a service account in the namespace with the pull secret(s) for the IBM Container Registry that are copied. The pull secret(s) are required in order for pods to pull images that are stored in the IBM Container Registry. When the cluster is created in IBM Cloud, a pull secret is provided in the default namespace. In order for a pod in another namespace to use it, the secret must first be copied into the namespace. After that, the pod either needs to reference the pull secret directly or the service account used by the resource needs to have a reference to the secret. The CLI copies the pull secret over and adds it to the service account so the pod can take either approach. This command should be use to set up \"release\" namespaces where applications can be deployed (e.g. test, staging) Positionals: namespace The namespace into which the pull-secret(s) will be created Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --dev flag to indicate this is a development namespace and that development artifacts should be created[boolean] --verbose flag to produce more verbose logging [boolean]","title":"pull-secret"},{"location":"getting-started/cli/#pipeline","text":"Connects a branch in a Git repo to a either a Jenkins or Tekton CI pipeline in the environment and triggers an initial build. A webhook is also created so that when a new commit is added to the branch, the pipeline is triggered to start the process to rebuild and redeploy the app using the new code. Currently, webhook creation is supported for repositories hosted on Gitlab, Github, Github Enterprise, Bitbucket, and Gogs. This command can either be used to register a git repository that has previously been cloned to the local filesystem OR using the remote repo url.","title":"pipeline"},{"location":"getting-started/cli/#enable","text":"Adds DevOps artifacts to a Git repo that the environment uses to deploy the app. The command displays a list of available pipelines and applies the one you select to your code repo. The DevOps files added to your repo include (but are not limited to): Helm chart Container file This command DOES NOT require that the terminal is already logged in to an IBM Cloud account nor the cluster. It DOES require that the terminal's current directory is the repository directory for your local copy of the Git repo. The command will add files to the local repo. You should commit these new files and push them to the server repo. Then run oc pipeline to connect your repo to a pipeline in the environment.","title":"enable"},{"location":"getting-started/cli/#git-secret","text":"Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named {git org}.{git repo} . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated it won't be updated unless the --replace argument is passed.","title":"git-secret"},{"location":"getting-started/cli/#gitops","text":"Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named gitops-repo . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated unless the --replace argument is passed.","title":"gitops"},{"location":"getting-started/cli/#tool-config","text":"Configures a new tool in the environment. After deploying the tool, use this command to add the tool to the list of credentials so that it will be displayed in the dashboard.","title":"tool-config"},{"location":"getting-started/console/","text":"With the release of it is now even easier for developers to integrate the DevSecOps tools into the OpenShift console. The common DevSecOps tools are integrated into the OpenShift console. This enables easy access to all the common tools a developer needs. Tools configured with OpenShift Console \u00b6 When the was configured for your development cluster. The administrator can now configure a set of short cut links to common tools you often use as a developer. Access the tools URLs \u00b6 To list the ingress endpoints for all of the installed tools, use the Developer Tools CLI to run the following command: igc endpoints -n tools This will return the ingress URLs for all of the tools installed in the Developer Tools cluster. You can then select the URL to open the tools' dashboard directly: ```bash ? Endpoints in the 'tools' namespace. Select an endpoint to launch the default browser or 'Exit'. 1) Exit 2) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud 3) argocd-server - https://argocd-tools.gsi-learning-ocp4-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 4) artifactory - https://artifactory-tools.gsi-learning-ocp4-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 5) dashboard - https://dashboard-tools.gsi-learning-ocp4-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 6) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud (Move up and down to reveal more choices) Answer: ``` Credentials \u00b6 In the future, the Dashboard tools will be linked using a single sign-on (SSO) service. Meantime, the CLI includes a command to list the tools' credentials. To list the credentials for all of the installed tools, use the Developer Tools CLI to run the following command: igc credentials The command lists the userid and password for each tool installed. You can use these credentials to log in to each one of the installed tools. More of the tools in will be integrated into the OpenShift console login process. Developer Dashboard \u00b6 The Developer Dashboard is one of the tools running in your . It is designed to help you navigate to the installed tools while providing a simple way to perform common developer tasks, such as: - Dashboard : Navigate to the tools installed in the cluster. - Activation : Links to educational resources to help you learn cloud-native development and deployment using IBM Cloud Kubernetes Service and Red Hat OpenShift on IBM Cloud. - : Links to templates that will help accelerate your project.","title":"Openshift Console"},{"location":"getting-started/console/#tools-configured-with-openshift-console","text":"When the was configured for your development cluster. The administrator can now configure a set of short cut links to common tools you often use as a developer.","title":"Tools configured with OpenShift Console"},{"location":"getting-started/console/#access-the-tools-urls","text":"To list the ingress endpoints for all of the installed tools, use the Developer Tools CLI to run the following command: igc endpoints -n tools This will return the ingress URLs for all of the tools installed in the Developer Tools cluster. You can then select the URL to open the tools' dashboard directly: ```bash ? Endpoints in the 'tools' namespace. Select an endpoint to launch the default browser or 'Exit'. 1) Exit 2) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud 3) argocd-server - https://argocd-tools.gsi-learning-ocp4-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 4) artifactory - https://artifactory-tools.gsi-learning-ocp4-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 5) dashboard - https://dashboard-tools.gsi-learning-ocp4-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 6) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud (Move up and down to reveal more choices) Answer: ```","title":"Access the tools URLs"},{"location":"getting-started/console/#credentials","text":"In the future, the Dashboard tools will be linked using a single sign-on (SSO) service. Meantime, the CLI includes a command to list the tools' credentials. To list the credentials for all of the installed tools, use the Developer Tools CLI to run the following command: igc credentials The command lists the userid and password for each tool installed. You can use these credentials to log in to each one of the installed tools. More of the tools in will be integrated into the OpenShift console login process.","title":"Credentials"},{"location":"getting-started/console/#developer-dashboard","text":"The Developer Dashboard is one of the tools running in your . It is designed to help you navigate to the installed tools while providing a simple way to perform common developer tasks, such as: - Dashboard : Navigate to the tools installed in the cluster. - Activation : Links to educational resources to help you learn cloud-native development and deployment using IBM Cloud Kubernetes Service and Red Hat OpenShift on IBM Cloud. - : Links to templates that will help accelerate your project.","title":"Developer Dashboard"},{"location":"getting-started/devenvsetup/","text":"To enable the best working experience through the hands-on exercises, you need to select the environment you plan to use for development and the required tools for integration with your OpenShift development environment. If you plan to use your desktop/laptop for hands-on exercises, follow the instructions in the Desktop/Laptop tab. You will need a recent macOS or Windows 10 operating system for this option. If you cannot install tools on your desktop/laptop, follow the Web Terminal instructions. Once you have fulfilled these setup requirements you will be ready to start the Developer Intermediate agenda. Web Terminal Open Web Terminal \u00b6 To be able to run CLI commands to drive common operations on the cluster you will first need to open your web terminal instance. - Click on the >_ icon in the header of the OpenShift Console. You should see a terminal open at the bottom of the console screen. - Validate that you can run oc commands, run the following command oc sync --version - You should see the version number printed Desktop/Laptop Tools installation on Desktop/Laptop \u00b6 The following tools are required: Git Client : needs to be installed in your development operating system, it comes as standard for Mac OS. IBM Cloud CLI : required for the management of your IBM Cloud Account, and managed IBM Kubernetes and Red Hat OpenShift clusters Aside from installing the IBM Cloud CLI , you will need to install the IBM Cloud CLI and Developer Tools too: $ curl -sL https://ibm.biz/idt-installer | bash Note: if you log in to the web UI using SSO, you'll need to create an API key to log in to the CLI. You can also use this API key for installing the Developer Tools environment. OpenShift OC CLI : required for Red Hat OpenShift management and development, select 4.3.18 or later version. Place oc and kubectl in your Terminal PATH : MacOS/Linux \u00b6 Once downloaded navigate to the Download folder: $ cd ~/Downloads/openshift-origin-client-tools-v3/ Then copy oc and kubectl to the system-wide available scripts folder (which is already in your PATH). $ cp kubectl /usr/local/bin/kubectl $ cp oc /usr/local/bin/oc Docker Desktop : required for running common tools and Developer Tools Image. Installed and running on your local machine. Node : required for running the IBM Garage for Cloud CLI . Installed on your local machine. Recommended: v12.x LTS . IBM Garage for Cloud CLI : used to help make working with the development tools as easy as possible. $ npm i -g @ibmgaragecloud/cloud-native-toolkit-cli Tekton CLI : used to help control Tekton pipelines from the command line. $ brew tap tektoncd/tools $ brew install tektoncd/tools/tektoncd-cli Visual Studio Code : a popular code editor You will need to edit some files, having a good quality editor is a good practice. Enabling launching VSCode from a terminal . JDK 11 ( optional ): installed on your local machine. Used for SpringBoot content. Log in to IBM Cloud \u00b6 Use the ibmcloud command to log in to the cloud account. Replace the user_id, password and team name with the sandbox ones: $ ibmcloud login -u <user_id> -p <password> -g <workshop-team> -r us-south Select the account number you have been assigned to: $ ibmcloud login -u email@company.com -p XXX -g workshop-team-one -r us-south API endpoint: https://cloud.ibm.com Authenticating... OK Select an account: 1 . GSI Labs - IBM Enter a number> 1 Targeted account GSI Labs Targeted resource group workshop-team-one Targeted region us-south API endpoint: https://cloud.ibm.com Region: us-south User: email@company.com Account: GSI Labs Resource group: workshop-team-one CF API endpoint: Org: Space: Log in to OpenShift Cluster \u00b6 Access the OpenShift console by clicking on the button: Get the OpenShift login command, which includes a token: Run the login command in the terminal: $ oc login --token = <token> --server = https://c103-e.us-south.containers.cloud.ibm.com:<port> Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:<port>\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" . Note Depending on your macOS security settings you may need to allow access to oc and kubectl , go to System Preferences => Security & Privacy and press Allow Anyway . Configure git \u00b6 Execute the following commands replacing the email and short name: $ git config --global user.email \"<email>\" $ git config --global user.name \"<short_name>\"","title":"Developer Tools Setup"},{"location":"getting-started/devenvsetup/#open-web-terminal","text":"To be able to run CLI commands to drive common operations on the cluster you will first need to open your web terminal instance. - Click on the >_ icon in the header of the OpenShift Console. You should see a terminal open at the bottom of the console screen. - Validate that you can run oc commands, run the following command oc sync --version - You should see the version number printed Desktop/Laptop","title":"Open Web Terminal"},{"location":"getting-started/devenvsetup/#tools-installation-on-desktoplaptop","text":"The following tools are required: Git Client : needs to be installed in your development operating system, it comes as standard for Mac OS. IBM Cloud CLI : required for the management of your IBM Cloud Account, and managed IBM Kubernetes and Red Hat OpenShift clusters Aside from installing the IBM Cloud CLI , you will need to install the IBM Cloud CLI and Developer Tools too: $ curl -sL https://ibm.biz/idt-installer | bash Note: if you log in to the web UI using SSO, you'll need to create an API key to log in to the CLI. You can also use this API key for installing the Developer Tools environment. OpenShift OC CLI : required for Red Hat OpenShift management and development, select 4.3.18 or later version. Place oc and kubectl in your Terminal PATH :","title":"Tools installation on Desktop/Laptop"},{"location":"getting-started/devenvsetup/#log-in-to-ibm-cloud","text":"Use the ibmcloud command to log in to the cloud account. Replace the user_id, password and team name with the sandbox ones: $ ibmcloud login -u <user_id> -p <password> -g <workshop-team> -r us-south Select the account number you have been assigned to: $ ibmcloud login -u email@company.com -p XXX -g workshop-team-one -r us-south API endpoint: https://cloud.ibm.com Authenticating... OK Select an account: 1 . GSI Labs - IBM Enter a number> 1 Targeted account GSI Labs Targeted resource group workshop-team-one Targeted region us-south API endpoint: https://cloud.ibm.com Region: us-south User: email@company.com Account: GSI Labs Resource group: workshop-team-one CF API endpoint: Org: Space:","title":"Log in to IBM Cloud"},{"location":"getting-started/devenvsetup/#log-in-to-openshift-cluster","text":"Access the OpenShift console by clicking on the button: Get the OpenShift login command, which includes a token: Run the login command in the terminal: $ oc login --token = <token> --server = https://c103-e.us-south.containers.cloud.ibm.com:<port> Logged into \"https://c103-e.us-south.containers.cloud.ibm.com:<port>\" as \"IAM#email@company\" using the token provided. You have access to 71 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"dev-ab\" . Note Depending on your macOS security settings you may need to allow access to oc and kubectl , go to System Preferences => Security & Privacy and press Allow Anyway .","title":"Log in to OpenShift Cluster"},{"location":"getting-started/devenvsetup/#configure-git","text":"Execute the following commands replacing the email and short name: $ git config --global user.email \"<email>\" $ git config --global user.name \"<short_name>\"","title":"Configure git"},{"location":"getting-started/prereqs/","text":"Create accounts \u00b6 You will need to set up the following accounts to use the OpenShift environment GitHub account (public, not Enterprise): create it if you don't have one yet. If you haven't logged in for a while, check that you can do so. Make sure to set your enterprise email address as the primary address for the account. The OpenShift Environment administrators will send an invitation to your enterprise email address. If are running this learning journey on IBM Cloud : Create one if needed and make sure you can log in. If you are running this journey on AWS and Azure you will login with your GitHub credentials. Configure Github Primary Email address \u00b6 Setup your primary email address to match your enterprise one. Follow these instructions . Configure Github Personal Access Token \u00b6 For your CI pipeline ( Tekton ) to connect to and use your GitHub repo, it will need a GitHub personal access token with public_repo and write:repo_hook scopes. The Personal Access Token only needs to be generated once because it is associated with the GitHub account and can be used to access any organizations and repositories that the account can access. Navigate to Developer Settings and generate a new token. Name it something like \"CI pipeline\". Select the public_repo scope to enable Git clone. Select the write:repo_hook scope so the pipeline can create a webhook . The GitHub UI will not display this token again, so make sure to save it in your password manager or somewhere safe that you can access later on. Once you have created these accounts you can validate you can access your OpenShift cloud-native development environment.","title":"Prerequisites"},{"location":"getting-started/prereqs/#create-accounts","text":"You will need to set up the following accounts to use the OpenShift environment GitHub account (public, not Enterprise): create it if you don't have one yet. If you haven't logged in for a while, check that you can do so. Make sure to set your enterprise email address as the primary address for the account. The OpenShift Environment administrators will send an invitation to your enterprise email address. If are running this learning journey on IBM Cloud : Create one if needed and make sure you can log in. If you are running this journey on AWS and Azure you will login with your GitHub credentials.","title":"Create accounts"},{"location":"getting-started/prereqs/#configure-github-primary-email-address","text":"Setup your primary email address to match your enterprise one. Follow these instructions .","title":"Configure Github Primary Email address"},{"location":"getting-started/prereqs/#configure-github-personal-access-token","text":"For your CI pipeline ( Tekton ) to connect to and use your GitHub repo, it will need a GitHub personal access token with public_repo and write:repo_hook scopes. The Personal Access Token only needs to be generated once because it is associated with the GitHub account and can be used to access any organizations and repositories that the account can access. Navigate to Developer Settings and generate a new token. Name it something like \"CI pipeline\". Select the public_repo scope to enable Git clone. Select the write:repo_hook scope so the pipeline can create a webhook . The GitHub UI will not display this token again, so make sure to save it in your password manager or somewhere safe that you can access later on. Once you have created these accounts you can validate you can access your OpenShift cloud-native development environment.","title":"Configure Github Personal Access Token"},{"location":"guides/","text":"Overview \u00b6 The contains a number of popular and proven open source tools for developing cloud-native applications and deploying them to Kubernetes and Red Hat OpenShift. This section will help you understand what each tool does and how it helps you prepare you application code for production delivery. Tools Configurations \u00b6 The tools have been installed and configured in the cluster that is the foundation of your . This is a popular approach that Red Hat and other vendors are following. There are some advantages and disadvantages. The upside is that you can get started quickly and you are only incurring the cost of the managed cluster. The downside is that you need to monitor them and manage them. The cool thing is that IBM Cloud has an amazing monitoring solution that can be configure to alert you of issues. More about that in the Guides. Note: As the industry moves to full lifecycle operators this will be the case. The tools are installed into a tools namespace and have a number of ConfigMaps and Secrets defined to make it easy for the tools to access and communicate with the underlying IBM Cloud services. Guides \u00b6 The Guides explain how to create and deploy applications using CI pipeline ( Jenkins , Tekton , etc.). They help explain how to: - Integrate code analysis into your applications with SonarQube - Move applications into test, staging, and production using continuous deployment techniques with Artifactory and ArgoCD - And much more","title":"Tools Guides Overview"},{"location":"guides/#overview","text":"The contains a number of popular and proven open source tools for developing cloud-native applications and deploying them to Kubernetes and Red Hat OpenShift. This section will help you understand what each tool does and how it helps you prepare you application code for production delivery.","title":"Overview"},{"location":"guides/#tools-configurations","text":"The tools have been installed and configured in the cluster that is the foundation of your . This is a popular approach that Red Hat and other vendors are following. There are some advantages and disadvantages. The upside is that you can get started quickly and you are only incurring the cost of the managed cluster. The downside is that you need to monitor them and manage them. The cool thing is that IBM Cloud has an amazing monitoring solution that can be configure to alert you of issues. More about that in the Guides. Note: As the industry moves to full lifecycle operators this will be the case. The tools are installed into a tools namespace and have a number of ConfigMaps and Secrets defined to make it easy for the tools to access and communicate with the underlying IBM Cloud services.","title":"Tools Configurations"},{"location":"guides/#guides","text":"The Guides explain how to create and deploy applications using CI pipeline ( Jenkins , Tekton , etc.). They help explain how to: - Integrate code analysis into your applications with SonarQube - Move applications into test, staging, and production using continuous deployment techniques with Artifactory and ArgoCD - And much more","title":"Guides"},{"location":"guides/continuous-delivery/","text":"Use Argo CD to continuously deliver application changes In IBM Garage Method, one of the Develop practices is continuous delivery . The Developer Environment uses an Argo CD pipeline to automate continuous delivery. What is continuous delivery \u00b6 Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: - Continuous delivery deploys an application when a user manually triggers deployment - Continuous deployment deploys an application automatically when it is ready An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. These tests must be automated so that deployment can be automated. Until you have this set of automated tests and trust them sufficiently, stick with continuous delivery. What is GitOps \u00b6 GitOps is the operational pattern of using Git repositories as the source of truth for defining the configuration that makes up the desired state of the application. It uses Git repositories to declaratively represent the desired state of applications in deployment environments. GitOps takes advantage of several Git features: - Git has an audit log of changes - Whole releases can be managed from a pull request - Git enables changes to be rolled back quickly if there is an issue with a new release CI/CD integration \u00b6 For the full end-to-end build and delivery process, both the CI and CD pipelines are required. For this to work, a Git repo is used to convey the configuration values. Within the Developer Environment, we have used certain naming conventions to streamline and simplify the integration between the various components. The naming components are: app repo - The name of the Git repository for the application git org - The name of the GitHub organization for the application's repo tag - The build version chart version - The version of the Helm chart region - The geographic location in IBM Cloud The derived names are: GitHub application path: github.com/{git org}/{app repo} CI Pipeline name: {git org}.{app repo} Docker image's path: {region}.icr.io/{git org}/{app repo}:{tag} in the Image Registry Helm chart's path: generic-local/{git org}/{app repo}-{tag}-{chart version}.tgz in the Helm Repository Dependencies filename: {app repo}/requirements.yaml in the GitOps repo CD Pipeline name: {app repo} What is Argo CD \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a Kubernetes cluster or namespace, which also works for an OpenShift cluster or project. Argo CD models a collection of applications as a project and uses a Git repository to store the project's desired state. Each application is stored as a folder in the repository, and each deployment environment is stored as a branch in the repository. Argo CD supports defining Kubernetes manifests in a number of ways: - helm charts - kustomize - ksonnet - jsonnet - plain directory of yaml/json manifests Argo CD synchronizes the application state with the desired state defined in Git and automates the deployment of Kubernetes resources to ensure they match. Configuring GitOps with Argo CD \u00b6 You must have completed the Argo CD Setup before continuing. Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the Developer Environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution Set up the GitOps repo \u00b6 Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Create a new repo from the Argo CD Code Pattern Clone the project to your machine Create a branch for the environment (e.g. test to configure the values for deployment to the testing environment) git checkout -b test The repository contains a template Helm chart in the app-artifactory folder. Copy that folder and rename it to match one of the application names in your project, i.e. {app repo} . The application name should match the repository name if the CI pipeline is going push changes to the CD pipeline. Update Chart.yaml name - The name of the application, should match the folder from the previous step Update requirements.yaml name - The name of Helm chart and Docker image, should match your Git repo name, i.e. {app repo} version - The version number of the Helm chart, i.e. {chart version} repository - The url to the Helm repository including the folder where the Helm charts are being stored, i.e. http://artifactory.{ingress subdomain}/artifactory/generic-local/ The url of the Helm repository in Artifactory can be determined by following the steps described in Administrator Guide - Argo CD setup . Update values.yaml Replace <app-chart-name> with the name of application Provide configuration values specific to the Helm chart under the <app-chart-name> prefix Note: The Helm values will need to be prefixed with the Helm chart name that was provided in the previous step. For example, assuming the helm chart is message-logger , the values.yaml file would look like the following: message-logger : replicaCount : 3 Note: The specific values that should be configured are dependent on the Helm chart that is referenced in the requirements.yaml . For the Code Patterns, you can see those details and customize the chart by looking in the chart/{chart name} of the repository. Repeat steps 4-7 for each application in the project Register the git repo in Argo CD \u00b6 Now that the repository has been created, we need to tell Argo CD where it is. Get the Argo CD login information from the igc credentials CLI command Note: You need to be logged into the cluster on the command-line for the CLI to access the cluster information. Log in to Argo CD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Press either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the Git repo Create a project in Argo CD (Optional) \u00b6 In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. To create a project: Log into Argo CD Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project name - Provide the name for the project description - A brief description of the project source - Press Add source and pick the Git repository from the list that was added previously destinations Add https://kubernetes.default.svc for the cluster url and test for the namespace Add https://kubernetes.default.svc for the cluster url and staging for the namespace Press Create Note: Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the test and staging namespaces within the current cluster. Add an application in Argo CD for each application component (Helm chart) \u00b6 The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Log into Argo CD Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored revision - The branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed Repeat that step for each application and each environment Hook the CI pipeline to the CD pipeline \u00b6 The last stage in the CI pipeline updates the version number in the requirements.yaml to the version of the helm chart that was just built. Through a couple naming conventions the only thing the pipeline needs in order to interact with the CD process is a Kubernetes secret named gitops-cd-secret that provides the details needed to connect to the git repo to push updates. Fortunately, a CLI command provides a simple way to create a Kubernetes secret that contains git credentials. Create the gitops-cd-secret : Log into the cluster on the command-line. Change the directory to the root of the Argo CD Code Pattern repo that was cloned previously. Run igc git-secret gitops-repo -n dev-{initials} to create the secret. This command will prompt for the username, personal access token, and the branch to put in the secret. What just happened? \u00b6 The git-secret command creates a secret in a Kubernetes namespace containing the url, username, password, and branch information for a git repo. In the command above, we provided gitops-cd-secret for the secret name. (If that value is left off the secret name defaults to {git org}.{git repo} .) You can verify the secret was created by running: kubectl get secrets/gitops-cd-secret -n dev- { initials } -o yaml Note: For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running (e.g. dev-{initials} ). The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. test is the recommended value for the branch field.","title":"Continuous Delivery"},{"location":"guides/continuous-delivery/#what-is-continuous-delivery","text":"Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: - Continuous delivery deploys an application when a user manually triggers deployment - Continuous deployment deploys an application automatically when it is ready An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. These tests must be automated so that deployment can be automated. Until you have this set of automated tests and trust them sufficiently, stick with continuous delivery.","title":"What is continuous delivery"},{"location":"guides/continuous-delivery/#what-is-gitops","text":"GitOps is the operational pattern of using Git repositories as the source of truth for defining the configuration that makes up the desired state of the application. It uses Git repositories to declaratively represent the desired state of applications in deployment environments. GitOps takes advantage of several Git features: - Git has an audit log of changes - Whole releases can be managed from a pull request - Git enables changes to be rolled back quickly if there is an issue with a new release","title":"What is GitOps"},{"location":"guides/continuous-delivery/#cicd-integration","text":"For the full end-to-end build and delivery process, both the CI and CD pipelines are required. For this to work, a Git repo is used to convey the configuration values. Within the Developer Environment, we have used certain naming conventions to streamline and simplify the integration between the various components. The naming components are: app repo - The name of the Git repository for the application git org - The name of the GitHub organization for the application's repo tag - The build version chart version - The version of the Helm chart region - The geographic location in IBM Cloud The derived names are: GitHub application path: github.com/{git org}/{app repo} CI Pipeline name: {git org}.{app repo} Docker image's path: {region}.icr.io/{git org}/{app repo}:{tag} in the Image Registry Helm chart's path: generic-local/{git org}/{app repo}-{tag}-{chart version}.tgz in the Helm Repository Dependencies filename: {app repo}/requirements.yaml in the GitOps repo CD Pipeline name: {app repo}","title":"CI/CD integration"},{"location":"guides/continuous-delivery/#what-is-argo-cd","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a Kubernetes cluster or namespace, which also works for an OpenShift cluster or project. Argo CD models a collection of applications as a project and uses a Git repository to store the project's desired state. Each application is stored as a folder in the repository, and each deployment environment is stored as a branch in the repository. Argo CD supports defining Kubernetes manifests in a number of ways: - helm charts - kustomize - ksonnet - jsonnet - plain directory of yaml/json manifests Argo CD synchronizes the application state with the desired state defined in Git and automates the deployment of Kubernetes resources to ensure they match.","title":"What is Argo CD"},{"location":"guides/continuous-delivery/#configuring-gitops-with-argo-cd","text":"You must have completed the Argo CD Setup before continuing. Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the Developer Environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution","title":"Configuring GitOps with Argo CD"},{"location":"guides/continuous-delivery/#set-up-the-gitops-repo","text":"Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Create a new repo from the Argo CD Code Pattern Clone the project to your machine Create a branch for the environment (e.g. test to configure the values for deployment to the testing environment) git checkout -b test The repository contains a template Helm chart in the app-artifactory folder. Copy that folder and rename it to match one of the application names in your project, i.e. {app repo} . The application name should match the repository name if the CI pipeline is going push changes to the CD pipeline. Update Chart.yaml name - The name of the application, should match the folder from the previous step Update requirements.yaml name - The name of Helm chart and Docker image, should match your Git repo name, i.e. {app repo} version - The version number of the Helm chart, i.e. {chart version} repository - The url to the Helm repository including the folder where the Helm charts are being stored, i.e. http://artifactory.{ingress subdomain}/artifactory/generic-local/ The url of the Helm repository in Artifactory can be determined by following the steps described in Administrator Guide - Argo CD setup . Update values.yaml Replace <app-chart-name> with the name of application Provide configuration values specific to the Helm chart under the <app-chart-name> prefix Note: The Helm values will need to be prefixed with the Helm chart name that was provided in the previous step. For example, assuming the helm chart is message-logger , the values.yaml file would look like the following: message-logger : replicaCount : 3 Note: The specific values that should be configured are dependent on the Helm chart that is referenced in the requirements.yaml . For the Code Patterns, you can see those details and customize the chart by looking in the chart/{chart name} of the repository. Repeat steps 4-7 for each application in the project","title":"Set up the GitOps repo"},{"location":"guides/continuous-delivery/#register-the-git-repo-in-argo-cd","text":"Now that the repository has been created, we need to tell Argo CD where it is. Get the Argo CD login information from the igc credentials CLI command Note: You need to be logged into the cluster on the command-line for the CLI to access the cluster information. Log in to Argo CD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Press either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the Git repo","title":"Register the git repo in Argo CD"},{"location":"guides/continuous-delivery/#create-a-project-in-argo-cd-optional","text":"In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. To create a project: Log into Argo CD Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project name - Provide the name for the project description - A brief description of the project source - Press Add source and pick the Git repository from the list that was added previously destinations Add https://kubernetes.default.svc for the cluster url and test for the namespace Add https://kubernetes.default.svc for the cluster url and staging for the namespace Press Create Note: Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the test and staging namespaces within the current cluster.","title":"Create a project in Argo CD (Optional)"},{"location":"guides/continuous-delivery/#add-an-application-in-argo-cd-for-each-application-component-helm-chart","text":"The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Log into Argo CD Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored revision - The branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed Repeat that step for each application and each environment","title":"Add an application in Argo CD for each application component (Helm chart)"},{"location":"guides/continuous-delivery/#hook-the-ci-pipeline-to-the-cd-pipeline","text":"The last stage in the CI pipeline updates the version number in the requirements.yaml to the version of the helm chart that was just built. Through a couple naming conventions the only thing the pipeline needs in order to interact with the CD process is a Kubernetes secret named gitops-cd-secret that provides the details needed to connect to the git repo to push updates. Fortunately, a CLI command provides a simple way to create a Kubernetes secret that contains git credentials. Create the gitops-cd-secret : Log into the cluster on the command-line. Change the directory to the root of the Argo CD Code Pattern repo that was cloned previously. Run igc git-secret gitops-repo -n dev-{initials} to create the secret. This command will prompt for the username, personal access token, and the branch to put in the secret.","title":"Hook the CI pipeline to the CD pipeline"},{"location":"guides/continuous-integration/","text":"In IBM Garage Method, one of the Develop practices is continuous integration . The uses a Jenkins pipeline to automate continuous integration. What is continuous integration \u00b6 Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. This quote helps explain it: Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly \u2013 Martin Fowler What is Jenkins \u00b6 Jenkins is a self-contained, open source automation server that can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. It is a perfect tool for helping manage continuous integration tasks for a wide range of software components. Jenkins Pipeline (or simply \"Pipeline\") is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins. A continuous delivery pipeline is an automated expression of your process for getting software from version control right through to your users and customers. Jenkins Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code.\" The definition of a Jenkins Pipeline is typically written into a text file (called a Jenkinsfile ) that in turn is checked into a project\u2019s source control repository. Pipelines \u00b6 Pipelines offer a set of stages or steps that can be chained together to allow a level of software automation. This automation can be tailored to the specific project requirements. You can read more information about Jenkins Pipelines here Stages \u00b6 Pipelines are defined in a Jenkinsfile that sits in the root of your application code. It defines a number of stages. Each of the includes a Jenkinsfile that offers a number of stages. The stages have been configured to complete the build, test, package, and deploy of the application code. Each stage can use the defined defined secrets and config maps that were previously configured during the installation of Development cluster setup. Developer Tools Pipeline \u00b6 To enable application compatibility between Kubernetes and OpenShift, the Jenkinsfile is consistent between pipeline registration with both platforms. Also, the Docker images are built from UBI images so that their containers can run on both platforms. These are the stages in the pipeline and a description of what each stage does. The bold stage names indicate the stages that are required; the italics stage names indicate optional stages that can be deleted or will be ignored if the tool supporting the stage is not installed. These stages represent a typical production pipeline flow for a cloud-native application. - Setup : Clones the code into the pipeline - Build : Runs the build commands for the code - Test : Validates the unit tests for the code - Publish pacts : Publishes any pact contracts that have been defined - Sonar scan : Runs a sonar code scan of the source code and publishes the results to SonarQube - Verify environment : Validates the OpenShift or IKS environment configuration is valid - Build image : Builds the code into a Docker images and stores it in the IBM Cloud Image registry - Deploy to DEV env : Deploys the Docker image tagged version to dev namespace using Helm Chart - Health Check : Validates the Health Endpoint of the deployed application - Package Helm Chart : Stores the tagged version of the Helm chart in Artifactory - Trigger CD Pipeline : This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test Registering Pipelines \u00b6 The are a good place to start to see how Jenkinsfile and Dockerfile should be configured for use in a Jenkins CI pipeline. To register your git repo, use the IGC CLI . This command automates a number of manual steps you would have to do with Jenkins, including: managing secrets, webhooks, and pipeline registration in the Jenkins tools. igc pipeline By default, the pipeline will register into the dev namespace and will copy all the configMaps and secrets from the tools namespace to the dev namespace. This means the pipeline can execute, knowing it has access to the key information that enables it to integrate with both the cloud platform and the various development tools. Registering Pipeline in new namespace \u00b6 You can use any namespace you want to register a pipeline. If you add -n or namespace to your igc pipeline command, it will create a new namespace if it doesn't already exist. It will copy the necessary secrets and configMaps into that namespace and configure the build agents pods to run in that namespace. igc pipeline -n team-one This is good if you have various squads, teams, pairs or students working in the same Development Tools environment. Continuous deployment \u00b6 In addition to continuous integration, the also supports continuous delivery using Artifactory and ArgoCD:","title":"Continuous Integration with Jenkins"},{"location":"guides/continuous-integration/#what-is-continuous-integration","text":"Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. This quote helps explain it: Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible. Many teams find that this approach leads to significantly reduced integration problems and allows a team to develop cohesive software more rapidly \u2013 Martin Fowler","title":"What is continuous integration"},{"location":"guides/continuous-integration/#what-is-jenkins","text":"Jenkins is a self-contained, open source automation server that can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. It is a perfect tool for helping manage continuous integration tasks for a wide range of software components. Jenkins Pipeline (or simply \"Pipeline\") is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins. A continuous delivery pipeline is an automated expression of your process for getting software from version control right through to your users and customers. Jenkins Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code.\" The definition of a Jenkins Pipeline is typically written into a text file (called a Jenkinsfile ) that in turn is checked into a project\u2019s source control repository.","title":"What is Jenkins"},{"location":"guides/continuous-integration/#pipelines","text":"Pipelines offer a set of stages or steps that can be chained together to allow a level of software automation. This automation can be tailored to the specific project requirements. You can read more information about Jenkins Pipelines here","title":"Pipelines"},{"location":"guides/continuous-integration/#stages","text":"Pipelines are defined in a Jenkinsfile that sits in the root of your application code. It defines a number of stages. Each of the includes a Jenkinsfile that offers a number of stages. The stages have been configured to complete the build, test, package, and deploy of the application code. Each stage can use the defined defined secrets and config maps that were previously configured during the installation of Development cluster setup.","title":"Stages"},{"location":"guides/continuous-integration/#developer-tools-pipeline","text":"To enable application compatibility between Kubernetes and OpenShift, the Jenkinsfile is consistent between pipeline registration with both platforms. Also, the Docker images are built from UBI images so that their containers can run on both platforms. These are the stages in the pipeline and a description of what each stage does. The bold stage names indicate the stages that are required; the italics stage names indicate optional stages that can be deleted or will be ignored if the tool supporting the stage is not installed. These stages represent a typical production pipeline flow for a cloud-native application. - Setup : Clones the code into the pipeline - Build : Runs the build commands for the code - Test : Validates the unit tests for the code - Publish pacts : Publishes any pact contracts that have been defined - Sonar scan : Runs a sonar code scan of the source code and publishes the results to SonarQube - Verify environment : Validates the OpenShift or IKS environment configuration is valid - Build image : Builds the code into a Docker images and stores it in the IBM Cloud Image registry - Deploy to DEV env : Deploys the Docker image tagged version to dev namespace using Helm Chart - Health Check : Validates the Health Endpoint of the deployed application - Package Helm Chart : Stores the tagged version of the Helm chart in Artifactory - Trigger CD Pipeline : This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test","title":"Developer Tools Pipeline"},{"location":"guides/continuous-integration/#registering-pipelines","text":"The are a good place to start to see how Jenkinsfile and Dockerfile should be configured for use in a Jenkins CI pipeline. To register your git repo, use the IGC CLI . This command automates a number of manual steps you would have to do with Jenkins, including: managing secrets, webhooks, and pipeline registration in the Jenkins tools. igc pipeline By default, the pipeline will register into the dev namespace and will copy all the configMaps and secrets from the tools namespace to the dev namespace. This means the pipeline can execute, knowing it has access to the key information that enables it to integrate with both the cloud platform and the various development tools.","title":"Registering Pipelines"},{"location":"guides/continuous-integration/#registering-pipeline-in-new-namespace","text":"You can use any namespace you want to register a pipeline. If you add -n or namespace to your igc pipeline command, it will create a new namespace if it doesn't already exist. It will copy the necessary secrets and configMaps into that namespace and configure the build agents pods to run in that namespace. igc pipeline -n team-one This is good if you have various squads, teams, pairs or students working in the same Development Tools environment.","title":"Registering Pipeline in new namespace"},{"location":"guides/continuous-integration/#continuous-deployment","text":"In addition to continuous integration, the also supports continuous delivery using Artifactory and ArgoCD:","title":"Continuous deployment"},{"location":"guides/continuous-integration-tekton/","text":"Overview \u00b6 Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. Tekton is a new emerging CI tool that has been built to support Kubernetes and OpenShift from the ground up. What is Tekton \u00b6 Tekton is a powerful yet flexible Kubernetes-native open-source framework for creating continuous integration and delivery (CI/CD) systems. It lets you build, test, and deploy across multiple cloud providers or on-premises systems by abstracting away the underlying implementation details. Tekton 101 \u00b6 Tekton provides open-source components to help you standardize your CI/CD tooling and processes across vendors, languages, and deployment environments. Industry specifications around pipelines, releases, workflows, and other CI/CD components available with Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, and Knative, among others. For more information read up about it Tekton Tutorial For more information read up about it App Build Tutorial with Tekton The IBM Cloud is standardizing on using Tekton in both IBM Cloud DevOps service and IBM Cloud Pak for Applications. OpenShift 4.2 will also natively support it. This guide will focus on using Tekton when the Development tools have been installed in Redhat OpenShift, IBM Kubernetes Managed services and Red Hat Code Ready Containers to give you choice for you Continuous Integration Cloud-Native development toolset. Note Note: This guide will help you set up the with Tekton and requires that you have installed Tekton with Red Hat Code Ready Containers 4.2 or have installed open source Tekton into the The IBM Kubernetes Cluster. Common App Tasks \u00b6 The following gives a description of each Task that is commonly used in a Pipeline . The Optional stages can be deleted or ignored if the tool support it is not installed. These stages represent a typical production pipeline flow for a Cloud-Native application. Setup clones the code into the pipeline Build runs the build commands for the code Test validates the unit tests for the code Publish pacts ( optional ) publishes any pact contracts that have been defined Verify pact ( optional ) verifies the pact contracts Sonar scan ( optional ) runs a sonar code scan of the source code and publishes the results to SonarQube Build image Builds the code into a Docker images and stores it in the IBM Cloud Image registry Deploy to DEV env Deploys the Docker image tagged version to dev namespace using Helm Chart Health Check Validates the Health Endpoint of the deployed application Package Helm Chart ( optional ) Stores the tagged version of the Helm chart into Artifactory Trigger CD Pipeline ( optional ) This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test namespace Install Tekton \u00b6 Tekton can be installed in both RedHat Openshift and IBM Kubernetes manage service and RedHat Code Ready Containers. To install the necessary components follow the steps below. Install IBM Garage for Cloud Developer Tools on your managed OpenShift,CRC or IKS development cluster on the IBM Cloud. This will help configure the tools and secrets and configMap to make working with IBM Cloud so much easier. OpenShift 4.x Install on OpenShift 4.x \u00b6 If you have installed the IBM Garage for Cloud Developer Tools into your cluster this will automatically install the operator for you. Install Tekton on OpenShift 4 including CodeReady Containers (CRC) Install via operator hub Administrator perspective view, click Operator Hub search for OpenShift Pipelines and install operator Kubernetes Install Tekton on IBM Kubernetes Managed Service \u00b6 Install Tekton via Knative add-on (can be found in the Add On tab in the Kubernetes managed service dashboard) , it includes Tekton support by default and the Webhook extension. Install Tekton Dashboard follow the instructions in the README.md Add Ingress endpoint for the Tekton Dashboard add a host name that uses the IKS cluster subdomain ```yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: tekton-dashboard namespace: tekton-pipelines spec: rules: host: tekton-dashboard.showcase-dev-iks-cluster.us-south.containers.appdomain.cloud http: paths: backend: serviceName: tekton-dashboard servicePort: 9097 ``` Install Tekton Webhook Extension Setup Tekton \u00b6 Install Tekton pipelines and tasks into the dev namespace following the instructions in the repository ibm-garage-tekton-tasks Install the Tasks kubectl create -f ibm-garage-tekton-tasks/tasks/ -n dev Install the Pipelines kubectl create -f ibm-garage-tekton-tasks/pipelines/ -n dev Configure namespace for development \u00b6 Install the Tekton CLI tkn https://github.com/tektoncd/cli Create a new namespace (ie dev-{initials} ) and copy all config and secrets igc namespace -n {new-namespace} Set your new-namespace the current namespace context oc project {new-namespace} The template Pipelines provided support for Java or Node.js based apps. You can configure your own custom Tasks for other runtimes. There are a number of default Tasks to get you started they are detailed above. To create an application use one of the provided these templates work seamlessly with the Tasks and Pipelines provided. Register the App with Tekton \u00b6 With Tetkon enabled and your default Tasks and Pipelines installed into the dev namespace. You can now configure your applications to be built, packaged, tested and deployed to your OpenShift or Kubernetes development cluster. Connect to the pipeline. (See the IGC CLI for details about how the pipeline command works.) igc pipeline -n dev- { initials } Verify the pipeline \u00b6 To validate your pipeline have been correctly configured, and has triggered a PipelineRun use the following Tekton dashboards or tkn CLI to validate it ran correctly without errors. OpenShift 4.x Review you Pipeline in the OpenShift 4.x Console Review your Tasks Review your Steps Opensource Tekton Dashboard If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline Tekton CLI If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline tkn pipelinerun list Review Pipeline details tkn pipelinerun describe { pipeline-name } Running Application \u00b6 Once the Tekton pipeline has successfully completed you can validate your app has been successfully deployed. Open the OpenShift Console and select the {new-namespace} project and click on Workloads Get the hostname for the application from ingress kubectl get ingress --all-namespace You can use the the igc command to get the name of the deployed application igc ingress -n { new-namespace } Use the application URL to open it your browser for testing Once you become familiar with deploying code into OpenShift using Tekton , read up about how you can manage code deployment with Continuous Delivery with ArgoCD and Artifactory","title":"Continuous Integration with Tekton"},{"location":"guides/continuous-integration-tekton/#overview","text":"Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. Tekton is a new emerging CI tool that has been built to support Kubernetes and OpenShift from the ground up.","title":"Overview"},{"location":"guides/continuous-integration-tekton/#what-is-tekton","text":"Tekton is a powerful yet flexible Kubernetes-native open-source framework for creating continuous integration and delivery (CI/CD) systems. It lets you build, test, and deploy across multiple cloud providers or on-premises systems by abstracting away the underlying implementation details.","title":"What is Tekton"},{"location":"guides/continuous-integration-tekton/#tekton-101","text":"Tekton provides open-source components to help you standardize your CI/CD tooling and processes across vendors, languages, and deployment environments. Industry specifications around pipelines, releases, workflows, and other CI/CD components available with Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, and Knative, among others. For more information read up about it Tekton Tutorial For more information read up about it App Build Tutorial with Tekton The IBM Cloud is standardizing on using Tekton in both IBM Cloud DevOps service and IBM Cloud Pak for Applications. OpenShift 4.2 will also natively support it. This guide will focus on using Tekton when the Development tools have been installed in Redhat OpenShift, IBM Kubernetes Managed services and Red Hat Code Ready Containers to give you choice for you Continuous Integration Cloud-Native development toolset. Note Note: This guide will help you set up the with Tekton and requires that you have installed Tekton with Red Hat Code Ready Containers 4.2 or have installed open source Tekton into the The IBM Kubernetes Cluster.","title":"Tekton 101"},{"location":"guides/continuous-integration-tekton/#common-app-tasks","text":"The following gives a description of each Task that is commonly used in a Pipeline . The Optional stages can be deleted or ignored if the tool support it is not installed. These stages represent a typical production pipeline flow for a Cloud-Native application. Setup clones the code into the pipeline Build runs the build commands for the code Test validates the unit tests for the code Publish pacts ( optional ) publishes any pact contracts that have been defined Verify pact ( optional ) verifies the pact contracts Sonar scan ( optional ) runs a sonar code scan of the source code and publishes the results to SonarQube Build image Builds the code into a Docker images and stores it in the IBM Cloud Image registry Deploy to DEV env Deploys the Docker image tagged version to dev namespace using Helm Chart Health Check Validates the Health Endpoint of the deployed application Package Helm Chart ( optional ) Stores the tagged version of the Helm chart into Artifactory Trigger CD Pipeline ( optional ) This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test namespace","title":"Common App Tasks"},{"location":"guides/continuous-integration-tekton/#install-tekton","text":"Tekton can be installed in both RedHat Openshift and IBM Kubernetes manage service and RedHat Code Ready Containers. To install the necessary components follow the steps below. Install IBM Garage for Cloud Developer Tools on your managed OpenShift,CRC or IKS development cluster on the IBM Cloud. This will help configure the tools and secrets and configMap to make working with IBM Cloud so much easier. OpenShift 4.x","title":"Install Tekton"},{"location":"guides/continuous-integration-tekton/#install-on-openshift-4x","text":"If you have installed the IBM Garage for Cloud Developer Tools into your cluster this will automatically install the operator for you. Install Tekton on OpenShift 4 including CodeReady Containers (CRC) Install via operator hub Administrator perspective view, click Operator Hub search for OpenShift Pipelines and install operator Kubernetes","title":"Install on OpenShift 4.x"},{"location":"guides/continuous-integration-tekton/#install-tekton-on-ibm-kubernetes-managed-service","text":"Install Tekton via Knative add-on (can be found in the Add On tab in the Kubernetes managed service dashboard) , it includes Tekton support by default and the Webhook extension. Install Tekton Dashboard follow the instructions in the README.md Add Ingress endpoint for the Tekton Dashboard add a host name that uses the IKS cluster subdomain ```yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: tekton-dashboard namespace: tekton-pipelines spec: rules: host: tekton-dashboard.showcase-dev-iks-cluster.us-south.containers.appdomain.cloud http: paths: backend: serviceName: tekton-dashboard servicePort: 9097 ``` Install Tekton Webhook Extension","title":"Install Tekton on IBM Kubernetes Managed Service"},{"location":"guides/continuous-integration-tekton/#setup-tekton","text":"Install Tekton pipelines and tasks into the dev namespace following the instructions in the repository ibm-garage-tekton-tasks Install the Tasks kubectl create -f ibm-garage-tekton-tasks/tasks/ -n dev Install the Pipelines kubectl create -f ibm-garage-tekton-tasks/pipelines/ -n dev","title":"Setup Tekton"},{"location":"guides/continuous-integration-tekton/#configure-namespace-for-development","text":"Install the Tekton CLI tkn https://github.com/tektoncd/cli Create a new namespace (ie dev-{initials} ) and copy all config and secrets igc namespace -n {new-namespace} Set your new-namespace the current namespace context oc project {new-namespace} The template Pipelines provided support for Java or Node.js based apps. You can configure your own custom Tasks for other runtimes. There are a number of default Tasks to get you started they are detailed above. To create an application use one of the provided these templates work seamlessly with the Tasks and Pipelines provided.","title":"Configure namespace for development"},{"location":"guides/continuous-integration-tekton/#register-the-app-with-tekton","text":"With Tetkon enabled and your default Tasks and Pipelines installed into the dev namespace. You can now configure your applications to be built, packaged, tested and deployed to your OpenShift or Kubernetes development cluster. Connect to the pipeline. (See the IGC CLI for details about how the pipeline command works.) igc pipeline -n dev- { initials }","title":"Register the App with Tekton"},{"location":"guides/continuous-integration-tekton/#verify-the-pipeline","text":"To validate your pipeline have been correctly configured, and has triggered a PipelineRun use the following Tekton dashboards or tkn CLI to validate it ran correctly without errors. OpenShift 4.x Review you Pipeline in the OpenShift 4.x Console Review your Tasks Review your Steps Opensource Tekton Dashboard If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline Tekton CLI If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline tkn pipelinerun list Review Pipeline details tkn pipelinerun describe { pipeline-name }","title":"Verify the pipeline"},{"location":"guides/continuous-integration-tekton/#running-application","text":"Once the Tekton pipeline has successfully completed you can validate your app has been successfully deployed. Open the OpenShift Console and select the {new-namespace} project and click on Workloads Get the hostname for the application from ingress kubectl get ingress --all-namespace You can use the the igc command to get the name of the deployed application igc ingress -n { new-namespace } Use the application URL to open it your browser for testing Once you become familiar with deploying code into OpenShift using Tekton , read up about how you can manage code deployment with Continuous Delivery with ArgoCD and Artifactory","title":"Running Application"},{"location":"guides/contract-testing/","text":"In IBM Garage Method, one of the Develop practices is contract-driven testing . Pact automates contract testing and enables it to be added to a continuous integration pipeline. The 's CI pipeline ( Jenkins , Tekton , etc.) includes a Pact stage. Simply by building your app using the CI pipeline, your code's contract gets tested, just open the Pact UI to browse the results. What is contract testing \u00b6 Contract testing is a testing discipline that ensures two applications (a consumer and a provider) have a shared understanding of the interactions or the contract between them. The Pact framework has been selected for the provided tool set. Pact is a consumer-driven contract testing framework. More details can be found here - Pact overview . The framework has been built into the and a Pact Broker instance is provisioned in the cluster along with the other tools. In consumer-driven contract testing it is the consumer who defines the contract in terms of the expected interactions, the data structure, and the expected responses. That contract can then be used on the consumer-side to mock the interactions and validate the consumer behavior. More importantly, the contract can be shared with the provider of the interaction so that the provider's responses can be validated to ensure the consumer's expectations are met. In the Pact framework, the contract is called a pact . A pact consists of one or more interactions . Each interaction has the following structure: Given a *state* of {state} *upon receiving* a {description} request *with request* parameters - HTTP method - path - headers - body *will respond with* values like - status - headers - body where: - {state} is an optional string that describes the initial state. This value can be used by the provider during testing to make sure the preconditions are met - {description} is a unique description of the interaction - the request parameters can contain any values that describe the interaction - the response contains the relevant information for the consumer. The response values can be exact values or using matchers for type, regular expressions, etc Consumer \u00b6 Using the Pact framework libraries in conjunction with the unit testing framework on the consumer, the pact for the interaction between the consumer and provider is generated and validated. As part of the pact test setup, a Pact server is started and configured with the expected interactions. All of the consumer service invocations are directed at the Pact server which provides mock responses based on the interactions defined by the pact . At the end of the test, if all the interactions completed successfully a file containing the pact definition is generated. The following diagram gives an overview of the consumer interactions: An example pact test on a Typescript consumer using the jest testing framework is provided below. It has been broken into several parts. Pact server config \u00b6 At the beginning of the test file, the pact server is configured and started in the beforeAll() block. The afterAll() block finalizes the pacts by writing them out to a file and stopping the pact server. const port = 1234 ; const baseUrl = `http://localhost: ${ port } ` ; let pact : Pact ; beforeAll (() => { pact = new Pact ({ consumer : consumerName , provider : providerName , logLevel : 'error' , port , }); return pact . setup (); }); afterAll (() => { return pact . finalize () . catch ( err => console . error ( 'Error finalizing pact' , err )); }); Setup the service \u00b6 Next, an instance of the component that will be tested is loaded and configured with the pact server host and port as the base url used for the interactions. In this example, the consumer is using the typescript-ioc library to inject the baseUrl config value into the service. let service : SampleApi ; beforeAll (() => { Container . bind ( MyServiceConfig ) . provider ({ get : () => ({ baseUrl })}); service = Container . get ( SampleService ); }); Define and test the interaction \u00b6 For each interaction with the provider, a test similar to the one provided below is created. In it, the Pact framework is used to define the interaction. The addInteraction() publishes the interaction to the Pact server so that it can be used to provide a mock response when the request is made. The mock response is then used to validate the behavior of the component that is being tested. The example below is simple and passes the provider response directly through the service api but in more sophisticated examples the value would be transformed. describe ( 'given createWidget()' , () => { context ( 'when called with valid request' , () => { const widgetRequest = {...}; const widgetResponse = {...}; beforeEach (() => { return pact . addInteraction ({ uponReceiving : 'a valid widget request' , withRequest : { method : 'POST' , path : '/widgets' , headers : { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, body : widgetRequest , }, willRespondWith : { status : 200 , headers : { 'Content-Type' : Matchers . regex ({ generate : 'application/json' , matcher : 'application/json.*' }), }, body : Matchers.like ( widgetResponse ), }, }); }); test ( 'then return 200 status' , async () => { expect ( await service . createWidget ( widgetRequest )). toEqual ( widgetResponse ); }); }); }); Provider \u00b6 The provider uses the Pact framework to verify the running server against the pact , either one started locally as part of the test or another instance running elsewhere. The interactions in the pact are sent to the provider by a mock consumer in the Pact framework and the results are verified against the expected results defined by the pact. As an optional configuration for the verification process, an endpoint can be provided that handles the state information in the pact in order to ensure the preconditions for the test are met. (E.g. state=\"given an empty database\"). In order for these tests to be repeatable, it is often advisable to stand up a clean backend to run the pact tests when the tests start and tear it down when the tests are completed. For example, if a provider interacts with a Cloudant database point the test provider at a new database instance for the tests. The following diagram shows the interactions for the pact provider: Pact Broker \u00b6 One of the underpinning requirements of the pact verification process is the ability to make the pact files generated by the consumer available to the provider. When the pact verification is run in an automated pipeline this is difficult without an intermediary. Within the Pact framework, the Pact Broker provides the facility for consumers and providers to share the pact definitions with minimal dependencies between the systems. Additionally, the Pact Broker provides a place to define webhooks to trigger the provider build process when the pact definition changes and a way record and visualize the results of the verification process. The high-level interaction is shown below: \u00b6 The have been built with the frameworks necessary to generate and publish pacts for api consumers and verify against pacts and publish the results for api providers. The pipelines will do all the publishing and verification against Pact Broker if an instance of Pact Broker has been configured within the cluster. You can review you pact contracts using the Pact Dashboard.app Use the to open the Pact dashboard","title":"Contract Testing"},{"location":"guides/contract-testing/#what-is-contract-testing","text":"Contract testing is a testing discipline that ensures two applications (a consumer and a provider) have a shared understanding of the interactions or the contract between them. The Pact framework has been selected for the provided tool set. Pact is a consumer-driven contract testing framework. More details can be found here - Pact overview . The framework has been built into the and a Pact Broker instance is provisioned in the cluster along with the other tools. In consumer-driven contract testing it is the consumer who defines the contract in terms of the expected interactions, the data structure, and the expected responses. That contract can then be used on the consumer-side to mock the interactions and validate the consumer behavior. More importantly, the contract can be shared with the provider of the interaction so that the provider's responses can be validated to ensure the consumer's expectations are met. In the Pact framework, the contract is called a pact . A pact consists of one or more interactions . Each interaction has the following structure: Given a *state* of {state} *upon receiving* a {description} request *with request* parameters - HTTP method - path - headers - body *will respond with* values like - status - headers - body where: - {state} is an optional string that describes the initial state. This value can be used by the provider during testing to make sure the preconditions are met - {description} is a unique description of the interaction - the request parameters can contain any values that describe the interaction - the response contains the relevant information for the consumer. The response values can be exact values or using matchers for type, regular expressions, etc","title":"What is contract testing"},{"location":"guides/contract-testing/#consumer","text":"Using the Pact framework libraries in conjunction with the unit testing framework on the consumer, the pact for the interaction between the consumer and provider is generated and validated. As part of the pact test setup, a Pact server is started and configured with the expected interactions. All of the consumer service invocations are directed at the Pact server which provides mock responses based on the interactions defined by the pact . At the end of the test, if all the interactions completed successfully a file containing the pact definition is generated. The following diagram gives an overview of the consumer interactions: An example pact test on a Typescript consumer using the jest testing framework is provided below. It has been broken into several parts.","title":"Consumer"},{"location":"guides/contract-testing/#pact-server-config","text":"At the beginning of the test file, the pact server is configured and started in the beforeAll() block. The afterAll() block finalizes the pacts by writing them out to a file and stopping the pact server. const port = 1234 ; const baseUrl = `http://localhost: ${ port } ` ; let pact : Pact ; beforeAll (() => { pact = new Pact ({ consumer : consumerName , provider : providerName , logLevel : 'error' , port , }); return pact . setup (); }); afterAll (() => { return pact . finalize () . catch ( err => console . error ( 'Error finalizing pact' , err )); });","title":"Pact server config"},{"location":"guides/contract-testing/#setup-the-service","text":"Next, an instance of the component that will be tested is loaded and configured with the pact server host and port as the base url used for the interactions. In this example, the consumer is using the typescript-ioc library to inject the baseUrl config value into the service. let service : SampleApi ; beforeAll (() => { Container . bind ( MyServiceConfig ) . provider ({ get : () => ({ baseUrl })}); service = Container . get ( SampleService ); });","title":"Setup the service"},{"location":"guides/contract-testing/#define-and-test-the-interaction","text":"For each interaction with the provider, a test similar to the one provided below is created. In it, the Pact framework is used to define the interaction. The addInteraction() publishes the interaction to the Pact server so that it can be used to provide a mock response when the request is made. The mock response is then used to validate the behavior of the component that is being tested. The example below is simple and passes the provider response directly through the service api but in more sophisticated examples the value would be transformed. describe ( 'given createWidget()' , () => { context ( 'when called with valid request' , () => { const widgetRequest = {...}; const widgetResponse = {...}; beforeEach (() => { return pact . addInteraction ({ uponReceiving : 'a valid widget request' , withRequest : { method : 'POST' , path : '/widgets' , headers : { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, body : widgetRequest , }, willRespondWith : { status : 200 , headers : { 'Content-Type' : Matchers . regex ({ generate : 'application/json' , matcher : 'application/json.*' }), }, body : Matchers.like ( widgetResponse ), }, }); }); test ( 'then return 200 status' , async () => { expect ( await service . createWidget ( widgetRequest )). toEqual ( widgetResponse ); }); }); });","title":"Define and test the interaction"},{"location":"guides/contract-testing/#provider","text":"The provider uses the Pact framework to verify the running server against the pact , either one started locally as part of the test or another instance running elsewhere. The interactions in the pact are sent to the provider by a mock consumer in the Pact framework and the results are verified against the expected results defined by the pact. As an optional configuration for the verification process, an endpoint can be provided that handles the state information in the pact in order to ensure the preconditions for the test are met. (E.g. state=\"given an empty database\"). In order for these tests to be repeatable, it is often advisable to stand up a clean backend to run the pact tests when the tests start and tear it down when the tests are completed. For example, if a provider interacts with a Cloudant database point the test provider at a new database instance for the tests. The following diagram shows the interactions for the pact provider:","title":"Provider"},{"location":"guides/contract-testing/#pact-broker","text":"One of the underpinning requirements of the pact verification process is the ability to make the pact files generated by the consumer available to the provider. When the pact verification is run in an automated pipeline this is difficult without an intermediary. Within the Pact framework, the Pact Broker provides the facility for consumers and providers to share the pact definitions with minimal dependencies between the systems. Additionally, the Pact Broker provides a place to define webhooks to trigger the provider build process when the pact definition changes and a way record and visualize the results of the verification process. The high-level interaction is shown below:","title":"Pact Broker"},{"location":"guides/contract-testing/#_1","text":"The have been built with the frameworks necessary to generate and publish pacts for api consumers and verify against pacts and publish the results for api providers. The pipelines will do all the publishing and verification against Pact Broker if an instance of Pact Broker has been configured within the cluster. You can review you pact contracts using the Pact Dashboard.app Use the to open the Pact dashboard","title":""}]}